
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="崔东林的技术积累">
      
      
        <meta name="author" content="崔东林">
      
      
        <link rel="canonical" href="https://donglin8506.github.io/Blog/object_detection/yolov7/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.1, mkdocs-material-8.5.6">
    
    
      
        <title>Yolov7 - 从零实现</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#abstract" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="从零实现" class="md-header__button md-logo" aria-label="从零实现" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            从零实现
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Yolov7
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/donglin8506/Blog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    donglin8506/Blog
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../Mkdocs/" class="md-tabs__link">
      mkdocs使用
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../deep_learning/transformer/" class="md-tabs__link">
        深度学习基础篇
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../ObjectDetection/" class="md-tabs__link">
        目标检测篇
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../VIT/paper_interpretation.md" class="md-tabs__link">
        目标分类
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../VisionMultiModal/CLIP/paper_interpretation/" class="md-tabs__link">
        视觉多模态
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Introduction.md" class="md-tabs__link">
        系统篇
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="从零实现" class="md-nav__button md-logo" aria-label="从零实现" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    从零实现
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/donglin8506/Blog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    donglin8506/Blog
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../Mkdocs/" class="md-nav__link">
        mkdocs使用
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2">
          深度学习基础篇
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习基础篇" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          深度学习基础篇
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning/transformer/" class="md-nav__link">
        transformer介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning/conv2d/" class="md-nav__link">
        手动实现卷积
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3">
          目标检测篇
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="目标检测篇" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          目标检测篇
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ObjectDetection/" class="md-nav__link">
        yolov5
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4">
          目标分类
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="目标分类" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          目标分类
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4_1" type="checkbox" id="__nav_4_1" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_1">
          ViT
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ViT" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          ViT
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../VIT/paper_interpretation.md" class="md-nav__link">
        论文详解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../VIT/paper_read.md" class="md-nav__link">
        论文通读
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../VIT/paper_code/main.md" class="md-nav__link">
        代码阅读
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          视觉多模态
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="视觉多模态" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          视觉多模态
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5_1" type="checkbox" id="__nav_5_1" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_1">
          CLIP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CLIP" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          CLIP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/CLIP/paper_interpretation/" class="md-nav__link">
        论文详解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/CLIP/paper_read/" class="md-nav__link">
        论文通读
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/CLIP/paper_code/main/" class="md-nav__link">
        代码阅读
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2">
          MAE
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="MAE" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          MAE
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/MAE/paper_interpretation/" class="md-nav__link">
        论文详解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/MAE/paper_read/" class="md-nav__link">
        论文通读
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/MAE/paper_code/main.md" class="md-nav__link">
        代码阅读
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3">
          X-CLIP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="X-CLIP" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          X-CLIP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/X-CLIP/paper_interpretation/" class="md-nav__link">
        论文详解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/X-CLIP/paper_read/" class="md-nav__link">
        论文通读
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VisionMultiModal/X-CLIP/paper_code/main/" class="md-nav__link">
        代码阅读
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" checked>
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6">
          系统篇
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="系统篇" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          系统篇
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Introduction.md" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../cuidonglin/" class="md-nav__link">
        个人简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1 Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-related-work" class="md-nav__link">
    2 Related work
  </a>
  
    <nav class="md-nav" aria-label="2 Related work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-real-time-object-detectors" class="md-nav__link">
    2.1 Real-time object detectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-model-re-parameterization" class="md-nav__link">
    2.2 Model re-parameterization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-model-scaling" class="md-nav__link">
    2.3 Model scaling
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-architecture" class="md-nav__link">
    3. Architecture
  </a>
  
    <nav class="md-nav" aria-label="3. Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-extended-efficient-layer-aggregation-networks" class="md-nav__link">
    3.1 Extended efficient layer aggregation networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-model-scaling-for-concatenation-based-models" class="md-nav__link">
    3.2 Model scaling for concatenation-based models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-trainable-bag-of-freebies" class="md-nav__link">
    4. Trainable bag-of-freebies
  </a>
  
    <nav class="md-nav" aria-label="4. Trainable bag-of-freebies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-planned-re-parameterized-convolution" class="md-nav__link">
    4.1 Planned re-parameterized convolution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-coarse-for-auxiliary-and-fine-for-lead-loss" class="md-nav__link">
    4.2 Coarse for auxiliary(辅助) and fine for lead loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-other-trainable-bag-of-freebies" class="md-nav__link">
    4.3 Other trainable bag-of-freebies
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/donglin8506/Blog/edit/master/docs/object_detection/yolov7.md" title="编辑此页" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


  <h1>Yolov7</h1>

<p>论文名称: YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</p>
<p>readpaper地址: https://readpaper.com/pdf-annotate/note?pdfId=4666972415243337729&amp;noteId=729490235758215168</p>
<p>关键词: </p>
<ul>
<li>
<p>梯度分析 gradient analysis</p>
</li>
<li>
<p>transition layers </p>
</li>
</ul>
<h2 id="abstract">Abstract</h2>
<p>YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy <strong>56.8% AP</strong> among all known real-time object detectors with <strong>30 FPS or higher on GPU V100</strong>. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy.Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.</p>
<h2 id="1-introduction">1 Introduction</h2>
<p>Real-time object detection is a very important topic in computer vision, as it is often a necessary component in computer vision systems. For example, multi-object tracking[94,93], autonomous driving [40,18], robotics [35,58], medical image analysis [34,46], etc. The computing devices that execute real-time object detection is usually some mobile CPU or GPU, as well as various neural processing units (NPU) developed by major manufacturers. For example, the Apple neural engine (Apple), the neural compute stick (Intel), Jetson AI edge devices (Nvidia), the edge TPU (Google), the neural processing engine (Qualcomm), the AI processing unit (MediaTek), and the AI SoCs (Kneron), are all NPUs.Some of the above mentioned edge devices focus on speeding up different operations such as vanilla convolution, depth-wise convolution, or MLP operations.In this paper, the real-time object detector we proposed mainly hopes that it can support both mobile GPU and GPU devices from the edge to the cloud.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[94] FairMOT: On the Fainess of Detection and Re-identification in Multiple Object Tracking</td>
<td align="left">FairMOT</td>
<td align="left">2020-04-04</td>
</tr>
<tr>
<td align="left">[93] ByteTrack: Multi-Object Tracking by Associating Every Detection Box</td>
<td align="left">ByteTrack</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[40] GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</td>
<td align="left">GS3D</td>
<td align="left">2019-03-26</td>
</tr>
<tr>
<td align="left">[18] Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges</td>
<td align="left">-</td>
<td align="left">2021-03-01</td>
</tr>
<tr>
<td align="left">[35] Object Detection Approach for Robot Grasp(抓住) Detection</td>
<td align="left">-</td>
<td align="left">2019-05-20</td>
</tr>
<tr>
<td align="left">[50] Object Detection and Pose Estimation from RGB and Depth Data for Real-Time, Adaptive Robotic Grasping</td>
<td align="left">-</td>
<td align="left">2021-01-18</td>
</tr>
<tr>
<td align="left">[34] Retina U-Net: Embarrassingly Simple Exploitation(简单到令人尴尬的利用) of Segmentation Supervision for Medical Object Detection</td>
<td align="left">-</td>
<td align="left">2020-04-30</td>
</tr>
<tr>
<td align="left">[46] CLU-CNNs: Object detection for medical images</td>
<td align="left">CLU-CNNs</td>
<td align="left">2019-07-20</td>
</tr>
</tbody>
</table>
<p>In recent years, the real-time object detector is still developed for different edge device.For example, the development of MCUNet [49,48] and NanoDet [54] focused on producing low-power single-chip(低功耗单片机) and improving the inference speed on edge CPU. As for methods such as YOLOX [21] and YOLOR [81], they focus on improving the inference speed of various GPUs.More recently, the development of real-time object detector has focused on the design of efficient architecture.As for real-time object detectors that can be used on CPU[54,88,84,83], their design is mostly based on MobileNet [28,66,27], ShuffleNet [92,55], or GhostNet [25].Another mainstream real-time object detectors are developed for GPU [81,21,97], they mostly use ResNet [26], DarkNet [63], or DLA [87], and then use the CSPNet [80] strategy to optimize the architecture.The development direction of the proposed methods in this paper are different from that of the current mainstream real-time object detectors.In addition to architecture optimization, our proposed methods will focus on the optimization of the training process.Our focus will be on some optimized modules and optimization methods which may strengthen the training cost for improving the accuracy of object detection, but without increasing the inference cost. We call the proposed modules and optimization methods trainable bag-of-freebies.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[49] MCUNet: Tiny Deep Learning on IoT Devices</td>
<td align="left">MCUNet</td>
<td align="left">2020-07-20</td>
</tr>
<tr>
<td align="left">[48] Memory-efficient Patch-based Inference for Tiny Deep Learning</td>
<td align="left">-</td>
<td align="left">2021-12-06</td>
</tr>
<tr>
<td align="left">[21] YOLOX: Exceeding(超出) YOLO Series in 2021</td>
<td align="left">YOLOX</td>
<td align="left">2021-07-18</td>
</tr>
<tr>
<td align="left">[81] You Only Learn One Representaion: Unified Network for Multiple Tasks.</td>
<td align="left">YOLOR</td>
<td align="left">2021-05-10</td>
</tr>
<tr>
<td align="left">[88] PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</td>
<td align="left">PP-PicoDet</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[84] MobileDets: Searching for Object Detection Architectures for Mobile Accelerators</td>
<td align="left">MobileDets</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[83] FBNetV5: Neural Architecture Search for Multiple Tasks in One Run</td>
<td align="left">FBNetV5</td>
<td align="left">2021-11-19</td>
</tr>
<tr>
<td align="left">[28] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</td>
<td align="left">MobileNets</td>
<td align="left">2017-04-17</td>
</tr>
<tr>
<td align="left">[66] MobileNetV2: Inverted Residuals and Linear Bottlenecks</td>
<td align="left">MobileNetV2</td>
<td align="left">2018-01-13</td>
</tr>
<tr>
<td align="left">[27] Searching for MobileNetV3</td>
<td align="left">MobileNetV3</td>
<td align="left">2019-05-06</td>
</tr>
<tr>
<td align="left">[92] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</td>
<td align="left">ShuffleNet</td>
<td align="left">2017-07-04</td>
</tr>
<tr>
<td align="left">[55] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</td>
<td align="left">ShuffleNet V2</td>
<td align="left">2018-09-08</td>
</tr>
<tr>
<td align="left">[25] GhostNet: More Features from Cheap Operations</td>
<td align="left">GhostNet</td>
<td align="left">2019-11-27</td>
</tr>
<tr>
<td align="left">[97] Objects as Points</td>
<td align="left">CenterNet</td>
<td align="left">2019-04-16</td>
</tr>
<tr>
<td align="left">[26] Deep Residual Learning for Image Recognition</td>
<td align="left">ResNet</td>
<td align="left">2015-12-10</td>
</tr>
<tr>
<td align="left">[63] YOLOv3: An Incremental Improvement</td>
<td align="left">YOLOv3</td>
<td align="left">2018-04-08</td>
</tr>
<tr>
<td align="left">[87] Deep Layer Aggregation</td>
<td align="left">DLA</td>
<td align="left">2017-07-20</td>
</tr>
<tr>
<td align="left">[80] CSPNet: A New Backbone that can Enhance Learning Capability of CNN</td>
<td align="left">CSPNet</td>
<td align="left">2019-11-27</td>
</tr>
</tbody>
</table>
<p>Recently, model re-parameterization [13,12,29] and dynamic label assignment [20,17,42] have become important topics in network training and object detection.Mainly after the above new concepts are proposed, the training of object detector evolves many new issues. In this paper, we will present some of the new issues we have discovered and devise(设计) effective methods to address them.For model reparameterization, we analyze the model re-parameterization strategies applicable to layers in different networks with the concept of gradient propagation path, and propose planned re-parameterized model.In addition, when we discover that with dynamic label assignment technology, the training of model with multiple output layers will generate new issues. That is: "How to assign dynamic targets for the outputs of different branches?" For this problem, we propose a new label assignment method called coarse-to-fine(从粗到细) lead guided label assignment.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[13] RepVGG: Making VGG-style ConvNets Great Again</td>
<td align="left">RepVGG</td>
<td align="left">2021-01-11</td>
</tr>
<tr>
<td align="left">[12] Diverse Branch Block: Building a Convolution as an Inception-like Unit</td>
<td align="left">-</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[29] Online Convolutional Re-parameterization</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[20] OTA: Optimal Transport Assignment for Object Detection</td>
<td align="left">OTA</td>
<td align="left">2021-03-26</td>
</tr>
<tr>
<td align="left">[17] TOOD: Task-aligned One-stage Object Detection</td>
<td align="left">TOOD</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[42] A Dual Weighting Label Assignment Scheme for Object Detection</td>
<td align="left">-</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>The contributions of this paper are summarized as follows: (1)we design several trainable bag-of-freebies methods, so that real-time object detection can greatly improve the detection accuracy without increasing the inference cost; (2) for the evolution of object detection methods, we found two new issues, namely how re-parameterized module replaces original module, and how dynamic label assignment strategy deals with assignment to different output layers.In addition, we also propose methods to address the difficulties arising from these issues;(3) we propose "extend" and "compound scaling" methods for the real-time object detector that can effectively utilize parameters and computation; and (4) the method we proposed can effectively reduce about 40% parameters and 50% computation of state-of-the-art real-time object detector, and has faster inference speed and higher detection accuracy.</p>
<h2 id="2-related-work">2 Related work</h2>
<h4 id="21-real-time-object-detectors">2.1 Real-time object detectors</h4>
<p>Currently state-of-the-art real-time object detectors are mainly based on YOLO [61, 62, 63] and FCOS [76, 77], which are [3, 79, 81, 21, 54, 85, 23].Being able to become a state-of-the-art real-time object detector usually requires the following characteristics:(1) a faster and stronger network architecture;(2) a more effective feature integration method [22, 97, 37, 74, 59, 30, 9, 45];(3) a more accurate detection method [76, 77, 69]; (4) a more robust loss function [96, 64, 6, 56, 95, 57]; (5) a more efficient label assignment method [99, 20, 17, 82, 42];and (6) a more efficient training method.In this paper, we do not intend to explore self-supervised learning or knowledge distillation methods that require additional data or large model.Instead, we will design new trainable bag-of-freebies method for the issues derived from the state-of-the-art methods associated with (4), (5), and (6) mentioned above.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[61] You Only Look Once: Unified, Real-Time Object Detection</td>
<td align="left">YOLO</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[62] YOLO9000: Better, Faster, Stronger</td>
<td align="left">YOLO9000</td>
<td align="left">2017-07-21</td>
</tr>
<tr>
<td align="left">[63] YOLOv3: An Incremental Improvement</td>
<td align="left">YOLOv3</td>
<td align="left">2018-04-08</td>
</tr>
<tr>
<td align="left">[76] FCOS: Fully Convolutional One-Stage Object Detection</td>
<td align="left">FCOS</td>
<td align="left">2019-04-02</td>
</tr>
<tr>
<td align="left">[77] FCOS: A simple and strong anchor-free object detector</td>
<td align="left">FCOS</td>
<td align="left">2020-06-14</td>
</tr>
<tr>
<td align="left">[3] YOLOv4: Optimal Speed and Accuracy of Object Detection</td>
<td align="left">YOLOv4</td>
<td align="left">2020-04-23</td>
</tr>
<tr>
<td align="left">[79] Scaled-YOLOv4: Scaling Cross Stage Partial Network</td>
<td align="left">Scaled-YOLOv4</td>
<td align="left">2020-11-16</td>
</tr>
<tr>
<td align="left">[81] You Only Learn One Representaion: Unified Network for Multiple Tasks.</td>
<td align="left">YOLOR</td>
<td align="left">2021-05-10</td>
</tr>
<tr>
<td align="left">[21] YOLOX: Exceeding(超出) YOLO Series in 2021</td>
<td align="left">YOLOX</td>
<td align="left">2021-07-18</td>
</tr>
<tr>
<td align="left">[85] PP-YOLOE: An evolved version of YOLO</td>
<td align="left">PP-YOLOE</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[22] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection</td>
<td align="left">NAS-FPN</td>
<td align="left">2019-04-16</td>
</tr>
<tr>
<td align="left">[97] Objects as Points</td>
<td align="left">CenterNet</td>
<td align="left">2019-04-16</td>
</tr>
<tr>
<td align="left">[37] Panoptic Feature Pyramid Networks</td>
<td align="left">-</td>
<td align="left">2019-01-08</td>
</tr>
<tr>
<td align="left">[74] EfficientDet: Scalable and Efficient Object Detection</td>
<td align="left">EfficientDet</td>
<td align="left">2019-11-20</td>
</tr>
<tr>
<td align="left">[59] DetectoRS: Detecting Objects with Recursive Feature Pyramid and Swithchable Atrous Convolution</td>
<td align="left">DetectoRS</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[30] A2-FPN: Attention Aggregation based Feature Pyramid Network for Instance Segmentation</td>
<td align="left">A2-FPN</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[9] Dynamic Head: Unifying Object Detection Heads with Attentions</td>
<td align="left">-</td>
<td align="left">2021-06-15</td>
</tr>
<tr>
<td align="left">[45] Exploring Plain Vision Transformer Backbones for Object Detection</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[69] Sparse R-CNN: End-to-End Object Detection with Learnable Proposals</td>
<td align="left">-</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[96] IoU Loss for 2D/3D Object Detection</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[64] Generalized Intersection over Union: A Metric and A Loss fro Bounding Box Regression</td>
<td align="left">-</td>
<td align="left">2019-02-25</td>
</tr>
<tr>
<td align="left">[6] AP-Loss for Accurate One-Stage Object Detection</td>
<td align="left">-</td>
<td align="left">2021-11-01</td>
</tr>
<tr>
<td align="left">[56] A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</td>
<td align="left">-</td>
<td align="left">2020-09-28</td>
</tr>
<tr>
<td align="left">[95] Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</td>
<td align="left">-</td>
<td align="left">2019-11-19</td>
</tr>
<tr>
<td align="left">[57] Rank &amp; Sort Loss for Object Detection and Instance Segmentation</td>
<td align="left">-</td>
<td align="left">2021-07-24</td>
</tr>
<tr>
<td align="left">[99] AutoAssign: Differentiable Label Assignment for Dense Object Detection</td>
<td align="left">AutoAssign</td>
<td align="left">2020-07-07</td>
</tr>
<tr>
<td align="left">[20] OTA: Optimal Transport Assignment for Object Detection</td>
<td align="left">OTA</td>
<td align="left">2021-03-26</td>
</tr>
<tr>
<td align="left">[17] TOOD: Task-aligned One-stage Object Detection</td>
<td align="left">TOOD</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[42] A Dual(双重的) Weighting Label Assignment Scheme for Object Detection</td>
<td align="left">-</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">[82] End-to-End Object Detection with Fully Convolutional Network</td>
<td align="left">-</td>
<td align="left">2020-12-07</td>
</tr>
</tbody>
</table>
<h4 id="22-model-re-parameterization">2.2 Model re-parameterization</h4>
<p>Model re-parametrization techniques [71, 31, 75, 19, 33, 11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple computational modules into one at inference stage.The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble.There are two common practices for model-level reparameterization to obtain the final inference model.One is to train multiple identical models with different training data, and then average the weights of multiple trained models.The other is to perform a weighted average of the weights of models at different iteration number.Modulelevel re-parameterization is a more popular research issue recently. This type of method splits a module into multiple identical or different module branches during training and integrates multiple branched modules into a completely equivalent module during inference.However, not all proposed re-parameterized module can be perfectly applied to different architectures. With this in mind, we have developed new re-parameterization module and designed related application strategies for various architectures.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[71] Rethinking the Inception Architecture for Computer Vision</td>
<td align="left">-</td>
<td align="left">2015-12-02</td>
</tr>
<tr>
<td align="left">[31] Snapshot Ensembles: Train 1, Get M for Free</td>
<td align="left">-</td>
<td align="left">2017-04-01</td>
</tr>
<tr>
<td align="left">[75] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</td>
<td align="left">-</td>
<td align="left">2017-01-01</td>
</tr>
<tr>
<td align="left">[19] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</td>
<td align="left">-</td>
<td align="left">2018-02-27</td>
</tr>
<tr>
<td align="left">[33] Averaging Weights Leads to Wider Optima and Better Generalization</td>
<td align="left">-</td>
<td align="left">2018-03-14</td>
</tr>
<tr>
<td align="left">[11] ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</td>
<td align="left">ACNet</td>
<td align="left">2019-08-11</td>
</tr>
<tr>
<td align="left">[4] Ensemble deep learning in bioinformatics</td>
<td align="left">-</td>
<td align="left">2020-08-17</td>
</tr>
<tr>
<td align="left">[24] ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks</td>
<td align="left">ExpandNets</td>
<td align="left">2018-11-26</td>
</tr>
<tr>
<td align="left">[13] RepVGG: Making VGG-style ConvNets Great Again</td>
<td align="left">RepVGG</td>
<td align="left">2021-01-11</td>
</tr>
<tr>
<td align="left">[12] Diverse Branch Block: Building a Convolution as an Inception-like Unit</td>
<td align="left">-</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[10] Re-parameterizing Your Optimizers rather than Architectures</td>
<td align="left">-</td>
<td align="left">2022-05-30</td>
</tr>
<tr>
<td align="left">[29] Online Convolutional Re-parameterization</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[14] Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[78] An Improved One millisecond Mobile Backbone</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
</tbody>
</table>
<h4 id="23-model-scaling">2.3 Model scaling</h4>
<p>Model scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way to scale up or down an already designed model and make it fit in different computing devices.The model scaling method usually uses different scaling factors, such as resolution (size of input image), depth (number of layer), width (number of channel), and stage (number of feature pyramid), so as to achieve a good trade-off for the amount of network parameters, computation, inference speed, and accuracy.Network architecture search (NAS) is one of the commonly used model scaling methods. NAS can automatically search for suitable scaling factors from search space without defining too complicated rules.The disadvantage of NAS is that it requires very expensive computation to complete the search for model scaling factors. In [15], the researcher analyzes the relationship between scaling factors and the amount of parameters and operations, trying to directly estimate some rules, and thereby obtain the scaling factors required by model scaling. Checking the literature, we found that almost all model scaling methods analyze individual scaling factor independently, and even the methods in the compound scaling category also optimized scaling factor independently.The reason for this is because most popular NAS architectures deal with scaling factors that are not very correlated.We observed that all concatenationbased models, such as DenseNet [32] or VoVNet [39], will change the input width of some layers when the depth of such models is scaled. Since the proposed architecture is concatenation-based, we have to design a new compound scaling method for this model.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[72] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</td>
<td align="left">EfficientNet</td>
<td align="left">2019-05-24</td>
</tr>
<tr>
<td align="left">[60] Designing Network Design Spaces</td>
<td align="left">-</td>
<td align="left">2020-03-30</td>
</tr>
<tr>
<td align="left">[74] EfficientDet: Scalable and Efficient Object Detection</td>
<td align="left">EfficientDet</td>
<td align="left">2019-11-20</td>
</tr>
<tr>
<td align="left">[73] EfficientNetV2: Smaller Models and Faster Training</td>
<td align="left">EfficientNetV2</td>
<td align="left">2021-04-01</td>
</tr>
<tr>
<td align="left">[15] Fast and Accurate Model Scaling</td>
<td align="left">-</td>
<td align="left">2021-03-11</td>
</tr>
<tr>
<td align="left">[16] Simple Training Strategies and Model Scaling for Object Detection</td>
<td align="left">-</td>
<td align="left">2021-06-30</td>
</tr>
<tr>
<td align="left">[2] Revisiting ResNets: Improved Training and Scaling Strategies</td>
<td align="left">-</td>
<td align="left">2021-12-06</td>
</tr>
<tr>
<td align="left">[51] Swin Transformer V2: Scaling up Capacity and Resolution</td>
<td align="left">SwinTransformerV2</td>
<td align="left">2021-11-18</td>
</tr>
<tr>
<td align="left">[32] Densely Connected Convolutional Networks</td>
<td align="left">DenseNet</td>
<td align="left">2016-08-25</td>
</tr>
<tr>
<td align="left">[39] An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</td>
<td align="left">VoVNet</td>
<td align="left">2019-04-22</td>
</tr>
</tbody>
</table>
<h2 id="3-architecture">3. Architecture</h2>
<h4 id="31-extended-efficient-layer-aggregation-networks">3.1 Extended efficient layer aggregation networks</h4>
<p>In most of the literature on designing the efficient architectures, the main considerations are no more than(不外乎是) the number of parameters, the amount of computation, and the computational density.Starting from the characteristics of memory access cost(内存访问成本), Ma et al. [55] also analyzed the influence of the input/output channel ratio, the number of branches of the architecture, and the element-wise operation on the network inference speed.Doll ́ar et al. [15] additionally considered activation when performing model scaling, that is, to put more consideration on the number of elements in the output tensors of convolutional layers.The design of CSPVoVNet [79] in Figure 2 (b) is a variation of VoVNet [39]. In addition to considering the aforementioned(前述的) basic designing concerns, the architecture of CSPVoVNet [79] also analyzes the gradient path, in order to enable the weights of different layers to learn more diverse features.The gradient analysis approach described above makes inferences faster and more accurate.ELAN [1] in Figure 2 (c) considers the following design strategy – "How to design an efficient network?." They came out with a conclusion: By controlling the shortest longest gradient path, a deeper network can learn and converge effectively. In this paper, we propose Extended-ELAN (E-ELAN) based on ELAN and its main architecture is shown in Figure 2 (d).</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[55] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</td>
<td align="left">ShuffleNet V2</td>
<td align="left">2018-09-08</td>
</tr>
<tr>
<td align="left">[15] Fast and Accurate Model Scaling</td>
<td align="left">-</td>
<td align="left">2021-03-11</td>
</tr>
<tr>
<td align="left">[79] Scaled-YOLOv4: Scaling Cross Stage Partial Network</td>
<td align="left">Scaled-YOLOv4</td>
<td align="left">2020-11-16</td>
</tr>
</tbody>
</table>
<p>Regardless of(无论) the gradient path length and the stacking number of computational blocks in large-scale ELAN, it has reached a stable state.If more computational blocks are stacked unlimitedly, this stable state may be destroyed, and the parameter utilization rate will decrease.The proposed E-ELAN uses expand, shuffle, merge cardinality(基数) to achieve the ability to continuously enhance the learning ability of the network without destroying the original gradient path.In terms of architecture, E-ELAN only changes the architecture in computational block, while the architecture of transition layer is completely unchanged.Our strategy is to use group convolution to expand the channel and cardinality of computational blocks.We will apply the same group parameter and channel multiplier to all the computational blocks of a computational layer.Then, the feature map calculated by each computational block will be shuffled into g groups according to the set group parameter g, and then concatenate them together.</p>
<h4 id="32-model-scaling-for-concatenation-based-models">3.2 Model scaling for concatenation-based models</h4>
<p>The main purpose of model scaling is to adjust some attributes of the model and generate models of different scales to meet the needs of different inference speeds.For example the scaling model of EfficientNet [72] considers the width, depth, and resolution.As for the scaled-YOLOv4 [79], its scaling model is to adjust the number of stages. In [15], Doll ́ar et al. analyzed the influence of vanilla convolution and group convolution on the amount of parameter and computation when performing width and depth scaling, and used this to design the corresponding model scaling method.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[72] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</td>
<td align="left">EfficientNet</td>
<td align="left">2019-05-24</td>
</tr>
<tr>
<td align="left">[79] Scaled-YOLOv4: Scaling Cross Stage Partial Network</td>
<td align="left">Scaled-YOLOv4</td>
<td align="left">2020-11-16</td>
</tr>
<tr>
<td align="left">[15] Fast and Accurate Model Scaling</td>
<td align="left">-</td>
<td align="left">2021-03-11</td>
</tr>
</tbody>
</table>
<p>The above methods are mainly used in architectures such as PlainNet or ResNet.When these architectures are in executing scaling up or scaling down, the in-degree and out-degree of each layer will not change, so we can independently analyze the impact of each scaling factor on the amount of parameters and computation.However, if these methods are applied to the concatenation-based architecture, we will find that when scaling up or scaling down is performed on depth, the in-degree of a translation layer which is immediately after a concatenation-based computational block will decrease or increase, as shown in Figure 3 (a) and (b).</p>
<p>It can be inferred from the above phenomenon that(从以上现象不难推理出) we cannot analyze different scaling factors separately for a concatenation-based model but must be considered together.Take scaling-up depth as an example, such an action will cause a ratio change between the input channel and output channel of a transition layer, which may lead to a decrease in the hardware usage of the model.Therefore, we must propose the corresponding compound model scaling method for a concatenation-based model.When we scale the depth factor of a computational block, we must also calculate the change of the output channel of that block.Then, we will perform width factor scaling with the same amount of change on the transition layers, and the result is shown in Figure 3 (c).Our proposed compound scaling method can maintain the properties that the model had at the initial design and maintains the optimal structure.</p>
<h2 id="4-trainable-bag-of-freebies">4. Trainable bag-of-freebies</h2>
<h4 id="41-planned-re-parameterized-convolution">4.1 Planned re-parameterized convolution</h4>
<p>Although RepConv [13] has achieved excellent performance on the VGG [68], when we directly apply it to ResNet [26] and DenseNet [32] and other architectures, its accuracy will be significantly reduced(减少).We use gradient flow propagation paths(梯度流传播路径) to analyze how re-parameterized convolution should be combined with different network.We also designed planned re-parameterized convolution accordingly.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[13] RepVGG: Making VGG-style ConvNets Great Again</td>
<td align="left">RepVGG</td>
<td align="left">2021-01-11</td>
</tr>
<tr>
<td align="left">[68] Very deep convolutional networks for large-scale image recognition</td>
<td align="left">VGG</td>
<td align="left">2015-01-01</td>
</tr>
<tr>
<td align="left">[26] Deep Residual Learning for Image Recognition</td>
<td align="left">ResNet</td>
<td align="left">2015-12-10</td>
</tr>
<tr>
<td align="left">[32] Densely Connected Convolutional Networks</td>
<td align="left">DenseNet</td>
<td align="left">2016-08-25</td>
</tr>
</tbody>
</table>
<p>RepConv actually combines 3 × 3 convolution, 1 × 1convolution, and identity connection in one convolutional layer.RepConv actually combines 3 × 3 convolution, 1 × 1convolution, and identity connection in one convolutional layer. After analyzing the combination and corresponding performance of RepConv and different architectures, we find that the identity connection in RepConv destroys the residual in ResNet and the concatenation in DenseNet, which provides more diversity of gradients for different feature maps. For the above reasons, we use RepConv without identity connection (RepConvN) to design the architecture of planned re-parameterized convolution.In our thinking, when a convolutional layer with residual or concatenation is replaced by re-parameterized convolution, there should be no identity connection.Figure 4 shows an example of our designed "planned re-parameterized convolution" used in PlainNet and ResNet. As for the complete planned re-parameterized convolution experiment in residual-based model and concatenation-based model, it will be presented in the ablation study session.</p>
<h4 id="42-coarse-for-auxiliary-and-fine-for-lead-loss">4.2 Coarse for auxiliary(辅助) and fine for lead loss</h4>
<p>Deep supervision [38] is a technique that is often used in training deep networks. Its main concept is to add extra auxiliary head in the middle layers of the network, and the shallow network weights with assistant loss as the guide.Even for architectures such as ResNet [26] and DenseNet [32] which usually converge well, deep supervision [70, 98, 67, 47, 82, 65, 86, 50] can still significantly improve the performance of the model on many tasks.Figure 5 (a) and (b) show, respectively, the object detector architecture "without" and "with" deep supervision.In this paper, we call the head responsible for the final output as the lead head, and the head used to assist（助攻） training is called auxiliary(辅助) head.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[38] Deeply-Supervised Nets</td>
<td align="left">-</td>
<td align="left">2014-09-18</td>
</tr>
<tr>
<td align="left">[26] Deep Residual Learning for Image Recognition</td>
<td align="left">ResNet</td>
<td align="left">2015-12-10</td>
</tr>
<tr>
<td align="left">[32] Densely Connected Convolutional Networks</td>
<td align="left">DenseNet</td>
<td align="left">2016-08-25</td>
</tr>
<tr>
<td align="left">[70] Going Deeper with Convolutions</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[98] UNet++: A Nested U-Net Architecture for Medical image Segmentation</td>
<td align="left">UNet++</td>
<td align="left">2018-07-18</td>
</tr>
<tr>
<td align="left">[67] Object Detection from Scratch with Deep Supervision</td>
<td align="left">DSOD</td>
<td align="left">2018-09-25</td>
</tr>
<tr>
<td align="left">[47] CBNetV2: A Composite Backbone Network Architecture for Object Detection</td>
<td align="left">CBNetV2</td>
<td align="left">2021-07-01</td>
</tr>
<tr>
<td align="left">[82] End-to-End Object Detection with Fully Convolutional Network</td>
<td align="left">POTO</td>
<td align="left">2020-12-07</td>
</tr>
<tr>
<td align="left">[65] Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity</td>
<td align="left">Sparse DETR</td>
<td align="left">2021-11-29</td>
</tr>
<tr>
<td align="left">[86] 3D-MAN: 3D Multi-frame Attention Network for Object Detection</td>
<td align="left">3D-MAN</td>
<td align="left">2021-06-01</td>
</tr>
<tr>
<td align="left">[50] YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection</td>
<td align="left">YOLOStereo3D</td>
<td align="left">2021-05-30</td>
</tr>
</tbody>
</table>
<p>Next we want to discuss the issue of label assignment. In the past, in the training of deep network, label assignment usually refers directly to the ground truth and generate hard label according to the given rules. However, in recent years, if we take object detection as an example, researchers often use the quality and distribution of prediction output by the network, and then consider together with the ground truth to use some calculation and optimization methods to generate a reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].For example, YOLO [61] use IoU of prediction of bounding box regression and ground truth as the soft label of objectness.In this paper, we call the mechanism that considers the network prediction results together with the ground truth and then assigns soft labels as "label assigner."</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[61] You Only Look Once: Unified, Real-Time Object Detection</td>
<td align="left">YOLO</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[8] Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving</td>
<td align="left">Gaussian YOLOv3</td>
<td align="left">2019-04-09</td>
</tr>
<tr>
<td align="left">[36] Probabilistic Anchor Assignment with IoU Prediction for Object Detection</td>
<td align="left">-</td>
<td align="left">2020-07-16</td>
</tr>
<tr>
<td align="left">[99] AutoAssign: Differentiable Label Assignment for Dense Object Detection</td>
<td align="left">AutoAssign</td>
<td align="left">2020-07-07</td>
</tr>
<tr>
<td align="left">[91] Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</td>
<td align="left">-</td>
<td align="left">2019-12-05</td>
</tr>
<tr>
<td align="left">[44] Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</td>
<td align="left">-</td>
<td align="left">2020-06-08</td>
</tr>
<tr>
<td align="left">[43] Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</td>
<td align="left">-</td>
<td align="left">2020-11-25</td>
</tr>
<tr>
<td align="left">[90] VarifocalNet: An IoU-aware Dense Object Detector</td>
<td align="left">VarifocalNet</td>
<td align="left">2020-08-31</td>
</tr>
<tr>
<td align="left">[20] OTA: Optimal Transport Assignment for Object Detection</td>
<td align="left">OTA</td>
<td align="left">2021-03-26</td>
</tr>
<tr>
<td align="left">[17] TOOD: Task-aligned One-stage Object Detection</td>
<td align="left">TOOD</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">[42] A Dual(双重的) Weighting Label Assignment Scheme for Object Detection</td>
<td align="left">-</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>Deep supervision needs to be trained on the target objectives regardless of the circumstances of auxiliary head or lead head.During the development of soft label assigner related techniques, we accidentally discovered a new derivative issue, i.e., "How to assign soft label to auxiliary head and lead head ?" To the best of our knowledge, the relevant literature has not explored this issue so far. The results of the most popular method at present is as shown in Figure 5 (c), which is to separate auxiliary head and lead head, and then use their own prediction results and the ground truth to execute label assignment.The method proposed in this paper is a new label assignment method that guides both auxiliary head and lead head by the lead head prediction.In other words, we use lead head prediction as guidance to generate coarse-to-fine hierarchical labels, which are used for auxiliary head and lead head learning, respectively. The two proposed deep supervision label assignment strategies are shown in Figure 5 (d) and (e), respectively.</p>
<p><strong>Lead head guided label assigner</strong> is mainly calculated based on the prediction result of the lead head and the ground truth, and generate soft label through the optimization process.This set of soft labels will be used as the target training model for both auxiliary head and lead head.The reason to do this is because lead head has a relatively strong learning capability, so the soft label generated from it should be more representative of the distribution and correlation between the source data and the target.Furthermore, we can view such learning as a kind of generalized residual learning. By letting the shallower auxiliary head directly learn the information that lead head has learned, lead head will be more able to focus on learning residual information that has not yet been learned.</p>
<p><strong>Coarse-to-fine lead head guided label assigner</strong> also used the predicted result of the lead head and the ground truth to generate soft label.However, in the process we generate two different sets of soft label, i.e., coarse label and fine label, where fine label is the same as the soft label generated by lead head guided label assigner, and coarse label is generated by allowing more grids to be treated as positive target by relaxing the constraints of the positive sample assignment process. The reason for this is that the learning ability of an auxiliary head is not as strong as that of a lead head, and in order to avoid losing the information that needs to be learned, we will focus on optimizing the recall of auxiliary head in the object detection task. As for the output of lead head, we can filter the high precision results from the high recall results as the final output.However, we must note that if the additional weight of coarse label is close to that of fine label, it may produce bad prior at final prediction.Therefore, in order to make those extra coarse positive grids have less impact, we put restrictions in the decoder, so that the extra coarse positive grids cannot produce soft label perfectly. The mechanism mentioned above allows the importance of fine label and coarse label to be dynamically adjusted during the learning process, and makes the optimizable upper bound of fine label always higher than coarse label.</p>
<h4 id="43-other-trainable-bag-of-freebies">4.3 Other trainable bag-of-freebies</h4>
<p>In this section we will list some trainable bag-offreebies. These freebies are some of the tricks we used in training, but the original concepts were not proposed by us.The training details of these freebies will be elaborated in the Appendix, including (1) Batch normalization in conv-bn-activation topology: This part mainly connects batch normalization layer directly to convolutional layer. The purpose of this is to integrate the mean and variance of batch normalization into the bias and weight of convolutional layer at the inference stage.(2) Implicit knowledge in YOLOR [81] combined with convolution feature map in addition and multiplication manner: Implicit knowledge in YOLOR can be simplified to a vector by pre-computing at the inference stage. This vector can be combined with the bias and weight of the previous or subsequent convolutional layer. (3) EMA model: EMA is a technique used in mean teacher [75], and in our system we use EMA model purely as the final inference model.</p>
<table>
<thead>
<tr>
<th align="left">论文名称</th>
<th align="left">论文别名</th>
<th align="left">论文时间</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[81] You Only Learn One Representaion: Unified Network for Multiple Tasks.</td>
<td align="left">YOLOR</td>
<td align="left">2021-05-10</td>
</tr>
<tr>
<td align="left">[75] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</td>
<td align="left">-</td>
<td align="left">2017-01-01</td>
</tr>
</tbody>
</table>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 Donglin Cui
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://github.com/donglin8506" target="_blank" rel="noopener" title="GitHub | Donglin-Cui" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.top", "navigation.indexes", "navigation.expand", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\s\\-\uff0c\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.078830c0.min.js"></script>
      
    
  </body>
</html>