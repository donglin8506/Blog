{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"Mkdocs/","text":"\u4ecb\u7ecd \u5f85\u89e3\u51b3\u7684\u95ee\u9898 \u76ee\u5f55\u5982\u4f55\u5206\u7ea7 markdown\u5982\u4f55\u63d2\u5165\u56fe\u7247 \u53c2\u8003\u6587\u6863 Mkdocs \u914d\u7f6e\u548c\u4f7f\u7528 https://github.com/cyent/markdown-with-mkdocs-material https://cyent.github.io/markdown-with-mkdocs-material/syntax/math_usage/ https://github.com/Yang-Xijie/yang-xijie.github.io/blob/main/mkdocs.yml https://github.com/squidfunk/mkdocs-material/blob/master/mkdocs.yml","title":"mkdocs\u4f7f\u7528"},{"location":"Mkdocs/#_1","text":"","title":"\u4ecb\u7ecd"},{"location":"Mkdocs/#_2","text":"\u76ee\u5f55\u5982\u4f55\u5206\u7ea7 markdown\u5982\u4f55\u63d2\u5165\u56fe\u7247","title":"\u5f85\u89e3\u51b3\u7684\u95ee\u9898"},{"location":"Mkdocs/#_3","text":"Mkdocs \u914d\u7f6e\u548c\u4f7f\u7528 https://github.com/cyent/markdown-with-mkdocs-material https://cyent.github.io/markdown-with-mkdocs-material/syntax/math_usage/ https://github.com/Yang-Xijie/yang-xijie.github.io/blob/main/mkdocs.yml https://github.com/squidfunk/mkdocs-material/blob/master/mkdocs.yml","title":"\u53c2\u8003\u6587\u6863"},{"location":"cuidonglin/","text":"\u4e2a\u4eba\u5c65\u5386 2020.06-\u81f3\u4eca \u5de5\u4f5c\u4e8e\u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 2019.07-2020.05 \u5de5\u4f5c\u4e8e\u8fbe\u95fc\u79d1\u6280 2016.09-2019.07 \u7855\u58eb\u5c31\u8bfb\u4e8e\u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66\uff08\u5317\u4eac\uff09 2012.09-2016.07 \u672c\u79d1\u5c31\u8bfb\u4e8e\u592a\u539f\u7406\u5de5\u5927\u5b66 \u4e13\u5229 \u5e8f\u53f7 \u540d\u79f0 \u7533\u8bf7\u53f7 \u4e13\u5229\u72b6\u6001 \u4e13\u5229\u7c7b\u578b \u53d1\u660e\u4eba\u6392\u5e8f \u7533\u8bf7\u4eba 12 \u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\u3001\u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\u3001\u88c5\u7f6e\u53ca\u7535\u5b50\u8bbe\u5907 202210817804.5 \u516c\u5f00 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 11 \u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\u3001\u8bbe\u5907\u548c\u4ecb\u8d28 202110119275.7 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 10 \u6c34\u5370\u68c0\u6d4b\u65b9\u6cd5\u53ca\u88c5\u7f6e\u3001\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u53ca\u88c5\u7f6e 202210745781.1 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 9 \u56fe\u50cf\u8bc6\u522b\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\u3001\u88c5\u7f6e 202110931644.2 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 8 \u6570\u636e\u5904\u7406\u65b9\u6cd5\u3001\u88c5\u7f6e\u3001\u7535\u5b50\u8bbe\u5907\u548c\u5b58\u50a8\u4ecb\u8d28 202210572911.6 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 7 \u6837\u672c\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3001\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u3001\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u548c\u88c5\u7f6e 202210976849.7 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 6 \u4e00\u79cd\u906e\u6321\u56fe\u6848\u8bc6\u522b\u65b9\u6cd5\u3001\u88c5\u7f6e\u3001\u8bbe\u5907\u548c\u4ecb\u8d28 202111592421.4 \u5b9e\u5ba1 \u53d1\u660e \u7b2c5 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 5 \u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u77ff\u4e95\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\u65b9\u6cd5 201811582240.1 \u6709\u6743 \u53d1\u660e \u7b2c2 \u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66(\u5317\u4eac) 4 \u4e00\u79cd\u77ff\u4e95\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u8bc6\u522b\u65b9\u6cd5 201711477567.8 \u6709\u6743 \u53d1\u660e \u7b2c2 \u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66(\u5317\u4eac) 3 \u4e00\u79cd\u63d0\u9ad8\u4e95\u4e0b\u52a0\u70ed\u7164\u5c42\u74e6\u65af\u62bd\u91c7\u91cf\u7684\u65b9\u6cd5 201510249993.0 \u6709\u6743 \u53d1\u660e \u7b2c7 \u592a\u539f\u7406\u5de5\u5927\u5b66 2 \u4e00\u79cd\u7528\u4e8e\u4e95\u4e0b\u7164\u5c42\u5df7\u9053\u5feb\u901f\u6398\u8fdb\u7684\u80c0\u88c2\u589e\u900f\u65b9\u6cd5 201510249945.1 \u6709\u6743 \u53d1\u660e \u7b2c2 \u592a\u539f\u7406\u5de5\u5927\u5b66 1 \u4e95\u4e0b\u7164\u5c42\u5df7\u9053\u74e6\u65af\u62bd\u91c7\u667a\u80fd\u63a7\u5236\u7cfb\u7edf\u53ca\u5176\u63a7\u5236\u65b9\u6cd5 201510363510.X \u6709\u6743 \u53d1\u660e \u7b2c5 \u592a\u539f\u7406\u5de5\u5927\u5b66","title":"\u4e2a\u4eba\u7b80\u4ecb"},{"location":"cuidonglin/#_1","text":"2020.06-\u81f3\u4eca \u5de5\u4f5c\u4e8e\u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 2019.07-2020.05 \u5de5\u4f5c\u4e8e\u8fbe\u95fc\u79d1\u6280 2016.09-2019.07 \u7855\u58eb\u5c31\u8bfb\u4e8e\u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66\uff08\u5317\u4eac\uff09 2012.09-2016.07 \u672c\u79d1\u5c31\u8bfb\u4e8e\u592a\u539f\u7406\u5de5\u5927\u5b66","title":"\u4e2a\u4eba\u5c65\u5386"},{"location":"cuidonglin/#_2","text":"\u5e8f\u53f7 \u540d\u79f0 \u7533\u8bf7\u53f7 \u4e13\u5229\u72b6\u6001 \u4e13\u5229\u7c7b\u578b \u53d1\u660e\u4eba\u6392\u5e8f \u7533\u8bf7\u4eba 12 \u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\u3001\u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\u3001\u88c5\u7f6e\u53ca\u7535\u5b50\u8bbe\u5907 202210817804.5 \u516c\u5f00 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 11 \u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\u3001\u8bbe\u5907\u548c\u4ecb\u8d28 202110119275.7 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 10 \u6c34\u5370\u68c0\u6d4b\u65b9\u6cd5\u53ca\u88c5\u7f6e\u3001\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u53ca\u88c5\u7f6e 202210745781.1 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 9 \u56fe\u50cf\u8bc6\u522b\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\u3001\u88c5\u7f6e 202110931644.2 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 8 \u6570\u636e\u5904\u7406\u65b9\u6cd5\u3001\u88c5\u7f6e\u3001\u7535\u5b50\u8bbe\u5907\u548c\u5b58\u50a8\u4ecb\u8d28 202210572911.6 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 7 \u6837\u672c\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3001\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u3001\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u548c\u88c5\u7f6e 202210976849.7 \u5b9e\u5ba1 \u53d1\u660e \u7b2c1 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 6 \u4e00\u79cd\u906e\u6321\u56fe\u6848\u8bc6\u522b\u65b9\u6cd5\u3001\u88c5\u7f6e\u3001\u8bbe\u5907\u548c\u4ecb\u8d28 202111592421.4 \u5b9e\u5ba1 \u53d1\u660e \u7b2c5 \u5317\u4eac\u767e\u5ea6\u7f51\u8baf\u79d1\u6280\u6709\u9650\u516c\u53f8 5 \u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u77ff\u4e95\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\u65b9\u6cd5 201811582240.1 \u6709\u6743 \u53d1\u660e \u7b2c2 \u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66(\u5317\u4eac) 4 \u4e00\u79cd\u77ff\u4e95\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u8bc6\u522b\u65b9\u6cd5 201711477567.8 \u6709\u6743 \u53d1\u660e \u7b2c2 \u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66(\u5317\u4eac) 3 \u4e00\u79cd\u63d0\u9ad8\u4e95\u4e0b\u52a0\u70ed\u7164\u5c42\u74e6\u65af\u62bd\u91c7\u91cf\u7684\u65b9\u6cd5 201510249993.0 \u6709\u6743 \u53d1\u660e \u7b2c7 \u592a\u539f\u7406\u5de5\u5927\u5b66 2 \u4e00\u79cd\u7528\u4e8e\u4e95\u4e0b\u7164\u5c42\u5df7\u9053\u5feb\u901f\u6398\u8fdb\u7684\u80c0\u88c2\u589e\u900f\u65b9\u6cd5 201510249945.1 \u6709\u6743 \u53d1\u660e \u7b2c2 \u592a\u539f\u7406\u5de5\u5927\u5b66 1 \u4e95\u4e0b\u7164\u5c42\u5df7\u9053\u74e6\u65af\u62bd\u91c7\u667a\u80fd\u63a7\u5236\u7cfb\u7edf\u53ca\u5176\u63a7\u5236\u65b9\u6cd5 201510363510.X \u6709\u6743 \u53d1\u660e \u7b2c5 \u592a\u539f\u7406\u5de5\u5927\u5b66","title":"\u4e13\u5229"},{"location":"direction/","text":"\u80cc\u666f \u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u7684\u4e13\u4e1a\u65b9\u5411\u6574\u7406\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u5b66\u4e60\u8d44\u6599\u3001\u8bba\u6587\u3001\u4e2a\u4eba\u601d\u8003\u7b49 \u4e13\u4e1a\u65b9\u5411 \u5305\u542b\u5b50\u65b9\u5411 \u5e38\u89c1\u7b97\u6cd5 \u7b80\u4ecb \u7ed3\u6784\u5316 \u7f51\u9875\u7ed3\u6784\u5316\u3001\u56fe\u7247\u7ed3\u6784\u5316\u3001OCR\u7ed3\u6784\u5316 StrucTexT(\u767e\u5ea6)\u3001LayoutLM\uff08\u5fae\u8f6f\uff09 \u81ea\u76d1\u7763\u8868\u5f81\u9884\u8bad\u7ec3\u65b9\u5411 - Deit / MoCo v3/DINO/BEiT/MAE/CAE \u5f02\u6784\u6a21\u578b\u84b8\u998f \u5c06transformer\u5927\u6a21\u578b\u5b66\u5230\u7684\u5185\u5bb9\u84b8\u998f\u5230CNN\u6a21\u578b\u4e0a \u9762\u5411\u82af\u7247\u7684\u7f51\u7edc\u7ed3\u6784\u641c\u7d22 - GP-NAS/OFA \u9488\u5bf9\u82af\u7247\u5b9a\u5236\u5316\uff0c\u9ad8\u6548\u4ea7\u51fa\u6700\u4f18\u7684\u6a21\u578b\u7ed3\u6784 \u4eba\u8138\u6d3b\u4f53\u3001OCR\u3001","title":"Direction"},{"location":"direction/#_1","text":"\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u7684\u4e13\u4e1a\u65b9\u5411\u6574\u7406\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u5b66\u4e60\u8d44\u6599\u3001\u8bba\u6587\u3001\u4e2a\u4eba\u601d\u8003\u7b49 \u4e13\u4e1a\u65b9\u5411 \u5305\u542b\u5b50\u65b9\u5411 \u5e38\u89c1\u7b97\u6cd5 \u7b80\u4ecb \u7ed3\u6784\u5316 \u7f51\u9875\u7ed3\u6784\u5316\u3001\u56fe\u7247\u7ed3\u6784\u5316\u3001OCR\u7ed3\u6784\u5316 StrucTexT(\u767e\u5ea6)\u3001LayoutLM\uff08\u5fae\u8f6f\uff09 \u81ea\u76d1\u7763\u8868\u5f81\u9884\u8bad\u7ec3\u65b9\u5411 - Deit / MoCo v3/DINO/BEiT/MAE/CAE \u5f02\u6784\u6a21\u578b\u84b8\u998f \u5c06transformer\u5927\u6a21\u578b\u5b66\u5230\u7684\u5185\u5bb9\u84b8\u998f\u5230CNN\u6a21\u578b\u4e0a \u9762\u5411\u82af\u7247\u7684\u7f51\u7edc\u7ed3\u6784\u641c\u7d22 - GP-NAS/OFA \u9488\u5bf9\u82af\u7247\u5b9a\u5236\u5316\uff0c\u9ad8\u6548\u4ea7\u51fa\u6700\u4f18\u7684\u6a21\u578b\u7ed3\u6784 \u4eba\u8138\u6d3b\u4f53\u3001OCR\u3001","title":"\u80cc\u666f"},{"location":"NLP/paper/transformer/","text":"\u8bba\u6587\u540d\u79f0: Attention Is All You Need readpaper\u5730\u5740: https://readpaper.com/pdf-annotate/note?pdfId=602045685808148480&noteId=727322466937372672 \u4f5c\u8005\u4ecb\u7ecd \u4f5c\u8005\u987a\u5e8f\u662f\u968f\u673a\u7684\uff0c\u4f5c\u8005\u7684\u8d21\u732e\u662f\u7b49\u540c\u7684\u3002 \u4f5c\u8005 \u516c\u53f8 \u8d21\u732e Ashish Vaswani Google Brain designed and implemented the first Transformer models and has been crucially(\u81f3\u5173\u91cd\u8981) involved in every aspect of this work. Noam Shazeer Google Brain proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki Parmar Google Research designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Jakob Uszkoreit Google Research proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Llion Jones Google Research also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Aidan N. Gomez University of Toronto spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. Lukasz Kaiser Google Brain \u540c Aidan N. Gomez Illia Polosukhin Google Research \u540c Ashish Vaswani \u6458\u8981 The dominant(\u4e3b\u5bfc\u7684) sequence transduction(\u8f6c\u5f55) models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing(\u4e22\u5f03) with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u5e8f\u5217\u8f6c\u5f55\u6a21\u578b\uff1a\u8f93\u5165\u4e00\u4e2a\u5e8f\u5217\uff0c\u8f93\u51fa\u4e00\u4e2a\u5e8f\u5217 \u5f15\u8a00 Recurrent neural networks, long shot-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [13] Long short-term memory LSTM 1997-11-01 [7] Empirical evaluation of gated recurrent neural networks on sequence modeling GRN 2014-12-11 [35] Sequence to Sequence Learning with Neural Networks - 2014-12-08 [2] Neural Machine Translation by Jointly Learning to Align and Translate - 2014-01-01 [5] Learning Phrase Representations using RNN Encoder -- Decoder for Statistical Machine Translation - 2014-01-01 [38] Google's Neural Machine Translation System: Bridgingthe the Gap between Human and Machine Translation - 2016-09-26 [24] Effective Approaches to Attention-based Neural Machine Translation - 2015-08-17 [15] Exploring the limits of language modeling - 2016-02-07 Recurrent models typically(\u901a\u5e38) factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.(\u5e8f\u5217\u5f88\u957f\u7684\u65f6\u5019\uff0c\u4e3a\u4e86\u4f7f\u5f53\u524d\u65f6\u523b\u80fd\u4f9d\u7136\u80fd\u83b7\u53d6\u5f88\u65e9\u7684\u9690\u542b\u7279\u5f81\uff0c\u5219\u9700\u8981\u5c06\u90a3\u4e48\u65f6\u523b\u7684\u9690\u542b\u7279\u5f81\u90fd\u5b58\u50a8\u4e0b\u6765\uff0c\u5219\u6bd4\u8f83\u8017\u5185\u5b58) Recent work has achieved significant improvements in computational efficiency through factorization tricks[21] and conditional computation[32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Factorization tricks for LSTM networks - 2017-02-17 Attention mechanisms have become an integral(\u4e0d\u53ef\u7f3a\u5c11\u7684) part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences[2,19]. In all but a few cases(\u9664\u5c11\u6570\u60c5\u51b5\u5916)[27], however, such attention mechanisms are used in conjunction with a recurrent network. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [2] Neural Machine Translation by Jointly Learning to Align and Translate - 2014-01-01 [19] Structured Attention Networks - 2017-02-03 In this work we propose the Transformer, a model architecture eschewing(\u907f\u514d) recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. \u80cc\u666f The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positioins. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant(\u9065\u8fdc\u7684) positions[12]. In the Transformer this is reduced to a constant number of operations, albeit(\u5c3d\u7ba1) at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2 \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [16] Can active memory replace attention? - 2016 [18] Neural Machine Translation in Linear Time - 2016-10-31 [9] Convolutional Sequence to Sequence Learning - 2017-05-08 [12] Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies - 2001-01-01 Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations[4,27,28,22]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [4] Long Short-Term Memory-Networks for Machine Reading - 2016-01-25 [28] A Deep Reinforced Model for Abstractive Summarization - 2017-05-11 [22] A Structured Self-Attention Sentence Embedding - - End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.[34] \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [34] End-To-End Memory Networks - - To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17,18] and [9]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [17] Neural GPUs Learn Algorithms - 2015-11-25 [18] Neural Machine Translation in Linear Time - 2016-10-31 [9] Convolutional Sequence to Sequence Learning - 2017-05-08 \u6a21\u578b\u7ed3\u6784 Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations ( $x_1,..,x_n$ ) to a sequence of continuous representations ( $z=(z_1, ..., z_n)$ ). Given $z$, the decoder then generates an output sequence ( $y_1, ..., y_m$ ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [5] Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation - 2014-01-01 [2] Neural Machine Translation by Jointy Learning to Align and Translate - 2014-01-01 [35] Sequence to Sequence Learning with Neural Networks - 2014-12-08 [10] Generating Sequences With Recurrent Neural Networks - 2013-08-04 The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network(\u4e00\u4e2a\u7b80\u5355\u7684MLP\u5c42). We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm($x$ + Sublayer($x$)), where Sublayer($x$) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model}=512$. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [11] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [1] Layer Normalization - 2016-07-21 \u4ec0\u4e48\u662fLN\u5c42\uff1f Decoder: The decoder is also composed of a stack of $N=6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequence positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously(\u540c\u65f6), packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as: $$ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ The two most commonly used attention functions are additive attention[2], and dot-product (multi-plicative \u4e58\u6cd5\u7684) attention. Dot-production attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Neural Machine Translation by Jointly Learning Align and Translate - 2014-01-01 While for small values of $d_k$ the two mechanisms perform similarily, additive attention outperforms dot product attention without scaling for larger values of $d_k$[3]. We suspect that for large values of $d_K$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [3] Massive Exploration of Neural Machine Translation Architectures - 2017-03-11 3.2.2 Multi-Head Attention Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to $d_q$, $d_k$, $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averagin inhibits this. $$ MultiHead(Q,K,V)=Concat(head_1, ..., head_h)W^O $$ $$ head_i = Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V}) $$ Where the projections are parameter matrices $W_{i}^{Q} \\in R^{d_{model} \\times d_k}$, $W_{i}^{K} \\in R^{d_{model} \\times d_k}$, $W_{i}^{V} \\in R^{d_{model} \\times d_v}$ and $W^O \\in R^{hd_v \\times d_{model}}$. In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{model}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with fully dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attenion\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics(\u6a21\u4eff) the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [38] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 [2] Neural Machine Translation by Jointly Learning to Align and Translate - 2014-01-01 [9] Convolutional sequence to sequence learning. - 2017 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. $$ FFN(x) = max(0, xW_1+b_1)W_2+b_2 $$ While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{model}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. \u6bcf\u4e2aMLP\u5355\u72ec\u4f5c\u7528\u5230\u4e00\u4e2a\u5411\u91cf\u4e0a\uff1b\u4e0d\u540c\u5411\u91cf\u4e0a\u7684MLP\u5171\u4eab\u53c2\u6570\uff1b \u7531\u4e8e\u5411\u91cf\u5728\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u5df2\u7ecf\u878d\u5408\u4e86\u6240\u6709\u8f93\u5165\u5411\u91cf\u7684\u7279\u5f81\uff0c\u5305\u62ec\u5176\u4e2d\u7684\u65f6\u5e8f\u4fe1\u606f\uff0c\u6240\u4ee5\u53ef\u4ee5\u8fdb\u884c\u5355\u72ec\u7684MLP\u64cd\u4f5c\u3002 \u5c42\u4e0e\u5c42\u4e4b\u95f4\u7684MLP\u662f\u4e0d\u540c\u7684\uff0c\u540c\u4e00\u5c42\u4e0d\u540c\u4f4d\u7f6e\u7684MLP\u662f\u76f8\u540c\u7684\u3002 MLP\u4e2d\u6709\u4e00\u4e2a\u9690\u85cf\u5c42\uff0c\u7ef4\u5ea6\u662f2048\uff0c\u8f93\u5165\u662f512\u7ef4\u5ea6\uff0c\u8f93\u51fa\u662f512\u7ef4\u5ea6\u3002 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probablities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$. 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject(\u6ce8\u5165) some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed[9]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [9] Convolutional Sequence to Sequence Learning - 2017-05-08 In this work, we use sine and cosine functions of different frequencies: $$ PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}}) $$ $$ PE_{pos, 2i+1}=cos(pos/10000^{2i/d_{model}}) $$ where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression(\u4e00\u4e2a\u51e0\u4f55\u7ea7\u6570) from $2\\pi$ to $1000 * 2\\pi$. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed $k$,$PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. We also experimented with using learned positional embeddings[9] instead, and found that the two versions produced nearly identical results(see Table 3 row E). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [9] Convolutional Sequence to Sequence Learning - 2017-05-08 \u4e3a\u4ec0\u4e48\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236 In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations ($x_1, ..., x_n$) to another sequence of equal length ($z_1, ..., z_n$), with $x_i, z_i \\in R^d$, such as a hidden layer in a typical sequence transduction encoder to encoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorted these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies[12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [12] Gradient FLow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies - 2001-01-01 As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as wrod-piece[38] and byte-pair[31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective(\u5404\u81ea\u7684) output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [38] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 [31] Neural Machine Translation of Rare Words with Subword Units - 2015-08-31 A single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_k(n))$ in the case of dilated convolutions[18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions[6], however, decrease the complexity considerably, to $O(k n d+n*d^2)$. Even with $k=n$, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [18] Neural Machine Translation In Linear Time - 2016-10-31 Xception: Deep Learning with Depthwise Separable Convolutions - 2017-07-21 As side benefit (\u4f5c\u4e3a\u9644\u5e26\u5229\u76ca), self-attention could yield more interpretable(\u53ef\u89e3\u91ca\u7684) models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic(\u53e5\u6cd5) and semantic structure of the sentences. Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. $n$ is the sequence length, $d$ is the representation dimension, $k$ is the kernel size of convolutions and $r$ is the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequentail Operations Maximum Path Length Self-Attention $O(n^2 * d)$ $O(1)$ $O(1)$ Recurrent $O(n * d^2)$ $O(n)$ $O(n)$ Convolutional $O(k n d^2)$ $O(1)$ $O(log_k(n))$ Sel-Attention (restricted) $O(r * n * d)$ $O(1)$ $O(n/r)$ \u8bad\u7ec3 This section describes the training regime for our models. 5.1 Traning Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38].Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. byte-pair: \u540c\u4e00\u4e2a\u82f1\u8bed\u8bcd\u53ef\u80fd\u6709\u591a\u79cd\u5f62\u5f0f\uff0c\u4f8b\u5982 like/liking\u7b49\uff0c\u63d0\u53d6\u76f8\u540c\u82f1\u8bed\u8bcd\u7684\u8bcd\u6839 \u5f04\u5230\u8bcd\u5178\u91cc\uff0c\u8fd9\u6837\u8bcd\u5178\u4e2d\u7684\u8bcd\u53ef\u4ee5\u6bd4\u8f83\u5c11\u3002\u8fd9\u91cc \u82f1\u8bed\u548c\u5fb7\u8bed\u4f7f\u7528\u76f8\u540c\u7684\u8bcd\u5e93\uff0c\u4e5f\u5c31\u6709\u76f8\u540c\u7684\u8bcd\u5411\u91cf\u3002 \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [3] Massive Exploration of Neural Machine Translation Architectures - 2017-03-11 [38] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer[20] with $\\beta_1=0.9$, $\\beta_2=0.98$, and $\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula: $$ lrate = d_{model}^{-0.5} \\cdot min(step_num^{-0.5}, step_num \\cdot warmup_steps^{-1.5}) $$ This corresponds to increasing the learning rate linearly for the first $warmup_steps$ and decreasing it thereafter(\u6b64\u540e) proportionally(\u6210\u6bd4\u4f8b) to the inverse(\u5012\u6570) square root of the step number. We used $warmup_steps=4000$. 5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}=0.1$. Label Smoothing During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [33] Dropout: a simple way to prevent neural networks from overfiting Dropout 2014-01-01 [36] Rethinking the Inception Architecture for Computer Vision - 2015-12-02 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than $1/4$ the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate $P_{drop}=0.1$, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints. We used beam search(\u675f\u641c\u7d22) with a beam size of 4 and length penalty $\\alpha=0.6$[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2.While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size $d_k$ hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to(\u53d7\u5236\u4e8e) strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes(\u65b9\u6848).[37] \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [37] Grammer as a Foreign Language - 2014-12-23 We trained a 4-layer transformer with $d_{model}=1024$ on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and $\\alpha=0.3$ for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [25] Building a large annotated corpus of English: the penn treebank [37] Grammer as a Foreign Language - 2014-12-23 [8] Recurrent Neural Network Grammars - 2016-02-25 [29] Learning Accurate, Compact, and interpretable Tree Annotation - 2006-07-17 \u7ed3\u8bba In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly(\u663e\u8457\u5730) faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential(\u4f7f\u751f\u6210\u4e0d\u90a3\u4e48\u65f6\u5e8f\u5316) is another research goals of ours.","title":"Transformer"},{"location":"NLP/paper/transformer/#_1","text":"The dominant(\u4e3b\u5bfc\u7684) sequence transduction(\u8f6c\u5f55) models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing(\u4e22\u5f03) with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u5e8f\u5217\u8f6c\u5f55\u6a21\u578b\uff1a\u8f93\u5165\u4e00\u4e2a\u5e8f\u5217\uff0c\u8f93\u51fa\u4e00\u4e2a\u5e8f\u5217","title":"\u6458\u8981"},{"location":"NLP/paper/transformer/#_2","text":"Recurrent neural networks, long shot-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [13] Long short-term memory LSTM 1997-11-01 [7] Empirical evaluation of gated recurrent neural networks on sequence modeling GRN 2014-12-11 [35] Sequence to Sequence Learning with Neural Networks - 2014-12-08 [2] Neural Machine Translation by Jointly Learning to Align and Translate - 2014-01-01 [5] Learning Phrase Representations using RNN Encoder -- Decoder for Statistical Machine Translation - 2014-01-01 [38] Google's Neural Machine Translation System: Bridgingthe the Gap between Human and Machine Translation - 2016-09-26 [24] Effective Approaches to Attention-based Neural Machine Translation - 2015-08-17 [15] Exploring the limits of language modeling - 2016-02-07 Recurrent models typically(\u901a\u5e38) factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.(\u5e8f\u5217\u5f88\u957f\u7684\u65f6\u5019\uff0c\u4e3a\u4e86\u4f7f\u5f53\u524d\u65f6\u523b\u80fd\u4f9d\u7136\u80fd\u83b7\u53d6\u5f88\u65e9\u7684\u9690\u542b\u7279\u5f81\uff0c\u5219\u9700\u8981\u5c06\u90a3\u4e48\u65f6\u523b\u7684\u9690\u542b\u7279\u5f81\u90fd\u5b58\u50a8\u4e0b\u6765\uff0c\u5219\u6bd4\u8f83\u8017\u5185\u5b58) Recent work has achieved significant improvements in computational efficiency through factorization tricks[21] and conditional computation[32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Factorization tricks for LSTM networks - 2017-02-17 Attention mechanisms have become an integral(\u4e0d\u53ef\u7f3a\u5c11\u7684) part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences[2,19]. In all but a few cases(\u9664\u5c11\u6570\u60c5\u51b5\u5916)[27], however, such attention mechanisms are used in conjunction with a recurrent network. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [2] Neural Machine Translation by Jointly Learning to Align and Translate - 2014-01-01 [19] Structured Attention Networks - 2017-02-03 In this work we propose the Transformer, a model architecture eschewing(\u907f\u514d) recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.","title":"\u5f15\u8a00"},{"location":"NLP/paper/transformer/#_3","text":"The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positioins. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant(\u9065\u8fdc\u7684) positions[12]. In the Transformer this is reduced to a constant number of operations, albeit(\u5c3d\u7ba1) at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2 \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [16] Can active memory replace attention? - 2016 [18] Neural Machine Translation in Linear Time - 2016-10-31 [9] Convolutional Sequence to Sequence Learning - 2017-05-08 [12] Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies - 2001-01-01 Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations[4,27,28,22]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [4] Long Short-Term Memory-Networks for Machine Reading - 2016-01-25 [28] A Deep Reinforced Model for Abstractive Summarization - 2017-05-11 [22] A Structured Self-Attention Sentence Embedding - - End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.[34] \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [34] End-To-End Memory Networks - - To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17,18] and [9]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [17] Neural GPUs Learn Algorithms - 2015-11-25 [18] Neural Machine Translation in Linear Time - 2016-10-31 [9] Convolutional Sequence to Sequence Learning - 2017-05-08","title":"\u80cc\u666f"},{"location":"NLP/paper/transformer/#_4","text":"Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations ( $x_1,..,x_n$ ) to a sequence of continuous representations ( $z=(z_1, ..., z_n)$ ). Given $z$, the decoder then generates an output sequence ( $y_1, ..., y_m$ ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [5] Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation - 2014-01-01 [2] Neural Machine Translation by Jointy Learning to Align and Translate - 2014-01-01 [35] Sequence to Sequence Learning with Neural Networks - 2014-12-08 [10] Generating Sequences With Recurrent Neural Networks - 2013-08-04 The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.","title":"\u6a21\u578b\u7ed3\u6784"},{"location":"NLP/paper/transformer/#31-encoder-and-decoder-stacks","text":"Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network(\u4e00\u4e2a\u7b80\u5355\u7684MLP\u5c42). We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm($x$ + Sublayer($x$)), where Sublayer($x$) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model}=512$. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [11] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [1] Layer Normalization - 2016-07-21 \u4ec0\u4e48\u662fLN\u5c42\uff1f Decoder: The decoder is also composed of a stack of $N=6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequence positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.","title":"3.1 Encoder and Decoder Stacks"},{"location":"NLP/paper/transformer/#32-attention","text":"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.","title":"3.2 Attention"},{"location":"NLP/paper/transformer/#321-scaled-dot-product-attention","text":"We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously(\u540c\u65f6), packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as: $$ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ The two most commonly used attention functions are additive attention[2], and dot-product (multi-plicative \u4e58\u6cd5\u7684) attention. Dot-production attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Neural Machine Translation by Jointly Learning Align and Translate - 2014-01-01 While for small values of $d_k$ the two mechanisms perform similarily, additive attention outperforms dot product attention without scaling for larger values of $d_k$[3]. We suspect that for large values of $d_K$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [3] Massive Exploration of Neural Machine Translation Architectures - 2017-03-11","title":"3.2.1 Scaled Dot-Product Attention"},{"location":"NLP/paper/transformer/#322-multi-head-attention","text":"Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to $d_q$, $d_k$, $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averagin inhibits this. $$ MultiHead(Q,K,V)=Concat(head_1, ..., head_h)W^O $$ $$ head_i = Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V}) $$ Where the projections are parameter matrices $W_{i}^{Q} \\in R^{d_{model} \\times d_k}$, $W_{i}^{K} \\in R^{d_{model} \\times d_k}$, $W_{i}^{V} \\in R^{d_{model} \\times d_v}$ and $W^O \\in R^{hd_v \\times d_{model}}$. In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{model}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with fully dimensionality.","title":"3.2.2 Multi-Head Attention"},{"location":"NLP/paper/transformer/#323-applications-of-attention-in-our-model","text":"The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attenion\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics(\u6a21\u4eff) the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [38] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 [2] Neural Machine Translation by Jointly Learning to Align and Translate - 2014-01-01 [9] Convolutional sequence to sequence learning. - 2017","title":"3.2.3 Applications of Attention in our Model"},{"location":"NLP/paper/transformer/#33-position-wise-feed-forward-networks","text":"In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. $$ FFN(x) = max(0, xW_1+b_1)W_2+b_2 $$ While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{model}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. \u6bcf\u4e2aMLP\u5355\u72ec\u4f5c\u7528\u5230\u4e00\u4e2a\u5411\u91cf\u4e0a\uff1b\u4e0d\u540c\u5411\u91cf\u4e0a\u7684MLP\u5171\u4eab\u53c2\u6570\uff1b \u7531\u4e8e\u5411\u91cf\u5728\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u5df2\u7ecf\u878d\u5408\u4e86\u6240\u6709\u8f93\u5165\u5411\u91cf\u7684\u7279\u5f81\uff0c\u5305\u62ec\u5176\u4e2d\u7684\u65f6\u5e8f\u4fe1\u606f\uff0c\u6240\u4ee5\u53ef\u4ee5\u8fdb\u884c\u5355\u72ec\u7684MLP\u64cd\u4f5c\u3002 \u5c42\u4e0e\u5c42\u4e4b\u95f4\u7684MLP\u662f\u4e0d\u540c\u7684\uff0c\u540c\u4e00\u5c42\u4e0d\u540c\u4f4d\u7f6e\u7684MLP\u662f\u76f8\u540c\u7684\u3002 MLP\u4e2d\u6709\u4e00\u4e2a\u9690\u85cf\u5c42\uff0c\u7ef4\u5ea6\u662f2048\uff0c\u8f93\u5165\u662f512\u7ef4\u5ea6\uff0c\u8f93\u51fa\u662f512\u7ef4\u5ea6\u3002","title":"3.3 Position-wise Feed-Forward Networks"},{"location":"NLP/paper/transformer/#34-embeddings-and-softmax","text":"Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probablities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$.","title":"3.4 Embeddings and Softmax"},{"location":"NLP/paper/transformer/#35-positional-encoding","text":"Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject(\u6ce8\u5165) some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed[9]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [9] Convolutional Sequence to Sequence Learning - 2017-05-08 In this work, we use sine and cosine functions of different frequencies: $$ PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}}) $$ $$ PE_{pos, 2i+1}=cos(pos/10000^{2i/d_{model}}) $$ where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression(\u4e00\u4e2a\u51e0\u4f55\u7ea7\u6570) from $2\\pi$ to $1000 * 2\\pi$. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed $k$,$PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. We also experimented with using learned positional embeddings[9] instead, and found that the two versions produced nearly identical results(see Table 3 row E). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [9] Convolutional Sequence to Sequence Learning - 2017-05-08","title":"3.5 Positional Encoding"},{"location":"NLP/paper/transformer/#_5","text":"In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations ($x_1, ..., x_n$) to another sequence of equal length ($z_1, ..., z_n$), with $x_i, z_i \\in R^d$, such as a hidden layer in a typical sequence transduction encoder to encoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorted these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies[12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [12] Gradient FLow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies - 2001-01-01 As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as wrod-piece[38] and byte-pair[31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective(\u5404\u81ea\u7684) output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [38] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 [31] Neural Machine Translation of Rare Words with Subword Units - 2015-08-31 A single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_k(n))$ in the case of dilated convolutions[18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions[6], however, decrease the complexity considerably, to $O(k n d+n*d^2)$. Even with $k=n$, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [18] Neural Machine Translation In Linear Time - 2016-10-31 Xception: Deep Learning with Depthwise Separable Convolutions - 2017-07-21 As side benefit (\u4f5c\u4e3a\u9644\u5e26\u5229\u76ca), self-attention could yield more interpretable(\u53ef\u89e3\u91ca\u7684) models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic(\u53e5\u6cd5) and semantic structure of the sentences. Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. $n$ is the sequence length, $d$ is the representation dimension, $k$ is the kernel size of convolutions and $r$ is the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequentail Operations Maximum Path Length Self-Attention $O(n^2 * d)$ $O(1)$ $O(1)$ Recurrent $O(n * d^2)$ $O(n)$ $O(n)$ Convolutional $O(k n d^2)$ $O(1)$ $O(log_k(n))$ Sel-Attention (restricted) $O(r * n * d)$ $O(1)$ $O(n/r)$","title":"\u4e3a\u4ec0\u4e48\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236"},{"location":"NLP/paper/transformer/#_6","text":"This section describes the training regime for our models.","title":"\u8bad\u7ec3"},{"location":"NLP/paper/transformer/#51-traning-data-and-batching","text":"We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38].Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. byte-pair: \u540c\u4e00\u4e2a\u82f1\u8bed\u8bcd\u53ef\u80fd\u6709\u591a\u79cd\u5f62\u5f0f\uff0c\u4f8b\u5982 like/liking\u7b49\uff0c\u63d0\u53d6\u76f8\u540c\u82f1\u8bed\u8bcd\u7684\u8bcd\u6839 \u5f04\u5230\u8bcd\u5178\u91cc\uff0c\u8fd9\u6837\u8bcd\u5178\u4e2d\u7684\u8bcd\u53ef\u4ee5\u6bd4\u8f83\u5c11\u3002\u8fd9\u91cc \u82f1\u8bed\u548c\u5fb7\u8bed\u4f7f\u7528\u76f8\u540c\u7684\u8bcd\u5e93\uff0c\u4e5f\u5c31\u6709\u76f8\u540c\u7684\u8bcd\u5411\u91cf\u3002 \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [3] Massive Exploration of Neural Machine Translation Architectures - 2017-03-11 [38] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26","title":"5.1 Traning Data and Batching"},{"location":"NLP/paper/transformer/#52-hardware-and-schedule","text":"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).","title":"5.2 Hardware and Schedule"},{"location":"NLP/paper/transformer/#53-optimizer","text":"We used the Adam optimizer[20] with $\\beta_1=0.9$, $\\beta_2=0.98$, and $\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula: $$ lrate = d_{model}^{-0.5} \\cdot min(step_num^{-0.5}, step_num \\cdot warmup_steps^{-1.5}) $$ This corresponds to increasing the learning rate linearly for the first $warmup_steps$ and decreasing it thereafter(\u6b64\u540e) proportionally(\u6210\u6bd4\u4f8b) to the inverse(\u5012\u6570) square root of the step number. We used $warmup_steps=4000$.","title":"5.3 Optimizer"},{"location":"NLP/paper/transformer/#54-regularization","text":"We employ three types of regularization during training: Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}=0.1$. Label Smoothing During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [33] Dropout: a simple way to prevent neural networks from overfiting Dropout 2014-01-01 [36] Rethinking the Inception Architecture for Computer Vision - 2015-12-02","title":"5.4 Regularization"},{"location":"NLP/paper/transformer/#6-results","text":"6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than $1/4$ the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate $P_{drop}=0.1$, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints. We used beam search(\u675f\u641c\u7d22) with a beam size of 4 and length penalty $\\alpha=0.6$[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - 2016-09-26 Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2.While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size $d_k$ hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to(\u53d7\u5236\u4e8e) strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes(\u65b9\u6848).[37] \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [37] Grammer as a Foreign Language - 2014-12-23 We trained a 4-layer transformer with $d_{model}=1024$ on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and $\\alpha=0.3$ for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [25] Building a large annotated corpus of English: the penn treebank [37] Grammer as a Foreign Language - 2014-12-23 [8] Recurrent Neural Network Grammars - 2016-02-25 [29] Learning Accurate, Compact, and interpretable Tree Annotation - 2006-07-17","title":"6 Results"},{"location":"NLP/paper/transformer/#_7","text":"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly(\u663e\u8457\u5730) faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential(\u4f7f\u751f\u6210\u4e0d\u90a3\u4e48\u65f6\u5e8f\u5316) is another research goals of ours.","title":"\u7ed3\u8bba"},{"location":"VisionMultiModal/dataset/","text":"1 \u6587\u6863\u603b\u89c8 \u6570\u636e\u96c6\u540d\u79f0 \u7f51\u5740 Kinetics - 400/600/700 https://www.deepmind.com/open-source/kinetics 2. Kinetics\u6570\u636e\u96c6 \u6570\u636e\u96c6\u7b80\u4ecb\uff1a - \u91cf\u7ea7: - \u5206\u5e03: - \u793a\u4f8b: \u6570\u636e\u96c6\u4e0b\u8f7d\u65b9\u6cd5\uff1a https://github.com/cvdfoundation/kinetics-dataset \u53ef\u4ee5\u4e0b\u8f7dKinetics-400/600/700\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u65b9\u6cd5\u7b80\u5355\uff1b \u89c6\u9891\u77ed\u8fb9\u5c3a\u5bf8\u4fee\u6539 3. AVA Actions\u6570\u636e\u96c6","title":"Dataset"},{"location":"VisionMultiModal/dataset/#1","text":"\u6570\u636e\u96c6\u540d\u79f0 \u7f51\u5740 Kinetics - 400/600/700 https://www.deepmind.com/open-source/kinetics","title":"1 \u6587\u6863\u603b\u89c8"},{"location":"VisionMultiModal/dataset/#2-kinetics","text":"\u6570\u636e\u96c6\u7b80\u4ecb\uff1a - \u91cf\u7ea7: - \u5206\u5e03: - \u793a\u4f8b: \u6570\u636e\u96c6\u4e0b\u8f7d\u65b9\u6cd5\uff1a https://github.com/cvdfoundation/kinetics-dataset \u53ef\u4ee5\u4e0b\u8f7dKinetics-400/600/700\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u65b9\u6cd5\u7b80\u5355\uff1b \u89c6\u9891\u77ed\u8fb9\u5c3a\u5bf8\u4fee\u6539","title":"2. Kinetics\u6570\u636e\u96c6"},{"location":"VisionMultiModal/dataset/#3-ava-actions","text":"","title":"3. AVA Actions\u6570\u636e\u96c6"},{"location":"VisionMultiModal/%E6%B1%87%E6%80%BB/","text":"\u89c6\u9891\u7c7b\u7684CLIP: CLIP4clip X-CLIP","title":"\u6c47\u603b"},{"location":"VisionMultiModal/CLIP/paper_interpretation/","text":"","title":"\u8bba\u6587\u8be6\u89e3"},{"location":"VisionMultiModal/CLIP/paper_read/","text":"\u8bba\u6587\u540d\u79f0: Learning Transferable(\u8fc1\u79fb\u6027\u597d\u7684) Visual Models From Natural Language Supervision \u8bba\u6587\u5730\u5740: https://arxiv.org/abs/2103.00020 readpaper\u5730\u5740: https://readpaper.com/pdf-annotate/note?pdfId=4498485025120608257&noteId=671513309851955200 \u6458\u8981 State-of-the-art computer vision systems are trained to predict a fixed set of predetermined\uff08\u56fa\u5b9a\u7684\u3001\u63d0\u524d\u5b9a\u4e49\u597d\u7684\uff09 object categories. This restricted form of supervision limits their generality\uff08\u6cdb\u5316\u6027\uff09 and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative\uff08\u6709\u524d\u9014\u7684\u7248\u672c\uff09 which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient\uff08\u9ad8\u6548\u7684\uff09 and scalable\uff08\u53ef\u6269\u5c55\u7684\uff09 way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. \u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60 \u3001 prompt \u4f7f\u7528ImageNet\u8bad\u7ec3\u7684ResNet50\u7684\u6a21\u578b\u548cCLIP\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5bf9\u6bd4\u6548\u679c\uff0c\u53ef\u4ee5\u6253\u5e73\u3002 \u5f15\u8a00 Pre-training methods which learn directly from raw text have revolutionized(\u5f7b\u5e95\u6539\u53d8) NLP over the last few years . (Dai &Le, 2015; Peters er al., 2018; Howard & Ruder, 2018; Radford et at., 2018; Devlin et al., 2018; Raffel et al., 2019). Task-agnostic objectives such as autoregressive\uff08\u81ea\u56de\u5f52\u9884\u6d4b\uff09 and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \"text-to-text\"\uff08\u6587\u5b57\u8fdb\u53bb\u3001\u6587\u5b57\u51fa\u53bb\uff09 as a standardized input-output interface (McCann et al., 2018; Radford et al., 2019; Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization.Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Semi-supervised Sequence Learning \u57fa\u4e8e\u534a\u76d1\u7763\u7684\u5e8f\u5217\u5b66\u4e60 - Dai &Le, 2015 Deep contextualized word representations \u6df1\u5ea6\u4e0a\u4e0b\u6587\u7684\u8bcd\u8868\u8fbe - Peters er al., 2018 Universal Language Model Fine-tuning for Text Classification \u6587\u672c\u5206\u7c7b\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5 ULMFiT Howard & Ruder, 2018 Improving Language Understanding by Generative Pre-Training \u901a\u8fc7\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6539\u5584\u8bed\u8a00\u7406\u89e3\u80fd\u529b GPT Radford et at., 2018 BERT: Pre-training of Deep Bidirectional Transformer for Language Understanding \u7528\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u6df1\u5ea6\u53cc\u5411T\u9884\u8bad\u7ec3\u65b9\u6cd5 BERT Devlin et al., 2018 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer \u4f7f\u7528\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u5230\u6587\u672c\u7684T\u6765\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u7684\u9650\u5236 - Raffel et al., 2019 The Natural Language Decathlon: Multitask Learning as Question Answering \u81ea\u7136\u8bed\u8a00\u5341\u9879\u5168\u80fd\uff1a\u591a\u4efb\u52a1\u5b66\u4e60\u5f53\u4f5c\u95ee\u7b54 MQAN McCann et al., 2018 Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u5668 GPT-2 Radford er al, 2019 Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 Brown et al., 2020 These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 ImageNet: A large-scale hierarchical image database \u5927\u89c4\u6a21\u5206\u5c42\u56fe\u7247\u6570\u636e\u5e93 ImageNet Deng et al., 2009 Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold (\u6d41\u5f62)learning in the weight space of classifiers trained to predict words in captions associated with images. Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al.(2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations. They converted the title, description, and hashtag\uff08\u6807\u7b7e\uff09 metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al.(2017) then extended this approach to predicting phrase n-grams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architecture and pre-training approaches, VirTex(Desai & Johnson,2020), ICMLM(Bulent Sariyildiz et al., 2020), and ConVIRT(Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text. \u8ddf\u8fd9\u7bc7\u6587\u7ae0\u5f88\u76f8\u4f3c Li et al.(2017) \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Image-to-word transformation based on dividing and vector quantizing images with words \u4f9d\u636e\u5355\u8bcd\u5c06\u56fe\u7247\u5206\u5272\u5e76\u5411\u91cf\u5316\uff0c\u6765\u5b9e\u73b0\u56fe\u7247\u5230\u6587\u672c\u7684\u8f6c\u6362 - Mori et al. (1999) Learning Visual Representations using Images with Captions \u5b66\u4e60\u6709\u5b57\u5e55\u56fe\u7247\u7684\u89c6\u89c9\u8868\u5f81 - Quattoni et al. (2007) Multimodal Learning with Deep Boltzmann Machines \u4f7f\u7528\u73bb\u5c14\u5179\u66fc\u673a\u505a\u591a\u6a21\u6001\u5b66\u4e60 - Srivastava & Salakhutdinov (2012) Learning Visual Features from Large Weakly Supervised Data \u4ece\u5927\u89c4\u6a21\u7684\u5f31\u76d1\u7763\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9\u7279\u5f81 - Joulin et al.(2016) YFCC100M: The New Data in Multimedia Research \u7528\u4e8e\u591a\u5a92\u4f53\u7814\u7a76\u7684\u65b0\u6570\u636e\u96c6 YFCC100M (Thomee et al., 2016) ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u505aImageNet\u5206\u7c7b AlexNet (Krizhevsky et al., 2012) Learning Visual N-Grams from Web Data \u4ece\u7f51\u9875\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9n-grams - Li et al.(2017) VirTex: Learning Visual Representations from Textual Annotations \u4ece\u6587\u672c\u6ce8\u91ca\u4e2d\u5b66\u4e60\u89c6\u89c9\u8868\u5f81 VirTex VirTex(Desai & Johnson,2020) Learning Visual Representations with Caption Annotations \u4ece\u5b57\u5e55\u4e2d\u5b66\u4e60\u662f\u89c6\u89c9\u8868\u5f81 ICMLM ICMLM(Bulent Sariyildiz et al., 2020) Contrastive Learning of Medical Visual Representations from Paired Images and Text \u4ece\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\u4e2d\u5b66\u4e60\u533b\u7597\u89c6\u89c9\u8868\u5f81 ConVIRT ConVIRT(Zhang et al., 2020) While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches(\u7531\u4e8e\u6027\u80fd\u4e0d\u9ad8\uff0c\u6240\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u7684\u76d1\u7763\u4fe1\u53f7\u4f7f\u7528\u7684\u56fe\u7247\u4e0a\uff0c\u8fd9\u6837\u7684\u5de5\u4f5c\u8fd8\u662f\u6bd4\u8f83\u5c11\u7684). For example, Li et al.(2017) reach only 15% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accurary of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012).(Li\u662f\u96f6\u6837\u672c\u63a8\u7406\uff0c\u5176\u4ed6\u5de5\u4f5c\u662f\u5728\u6709\u76d1\u7763\u8bad\u7ec3\u540e\u8fdb\u884c\u7684\u8bc4\u4f30) Instead, more narrowly(\u72ed\u9698\u5730) scoped but well-targeted uses of weak supervision have improved performance.(\u76f8\u6bd4\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u5f31\u76d1\u7763\u8bad\u7ec3\u7684\u6548\u679c\u76f8\u5bf9\u66f4\u597d\u4e00\u70b9) Mahajan et al.(2018) showed that predicting ImageNet-related hashtags(\u6807\u7b7e)(\u4e5f\u662f\u4e00\u79cd\u76d1\u7763\u4fe1\u53f7\uff0c\u53ea\u4e0d\u8fc7\u4e0d\u662f\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u4fe1\u53f7) on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-training models increased accuracy by 5% and improved the overall(\u5168\u9762\u7684) state of the art at the time.Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large grains on a broader set of transfer benchmarks by pre-training models to predicts the classes of the noisily labeled JFT-300M dataset(\u8fd9\u4e9b\u810f\u6570\u636e\u867d\u7136\u6ca1\u90a3\u4e48\u51c6\uff0c\u4f46\u4e5f\u662f\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u7684\u5f31\u76d1\u7763\u4fe1\u53f7). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learning Visual N-Grams from Web Data \u4ece\u7f51\u9875\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9n-grams - Li et al.(2017) Self-training with Noisy Student improves ImageNet classification \u4f7f\u7528\u566a\u58f0\u5b66\u751f\u7684\u81ea\u5b66\u4e60\u6765\u63d0\u5347\u56fe\u7247\u5206\u7c7b\u6548\u679c - (Xie et al., 2020) Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) Large Scale Learning of General Visual Representations for Transfer \u5b66\u4e60\u5927\u89c4\u6a21\u901a\u7528\u89c6\u89c9\u8868\u5f81\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60 BiT Kolesnikov et al. (2019) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale transformer\u7528\u4e8e\u56fe\u7247\u5206\u7c7b ViT Dosovitskiy et al. (2020) This line of work represents the current pragmatic(\u52a1\u5b9e\u7684\u3001\u5b9e\u7528\u4e3b\u4e49\u7684) middle ground\uff08\u4e2d\u95f4\u7acb\u573a/\u4e2d\u95f4\u5730\u5e26\uff09between learning from a limited amount of supervised \"gold-labels\"\uff08\u56fa\u5b9a\u6807\u7b7e\u7684\uff09 learning from practically unlimited amounts of raw text. However, it it not without compromises(\u7136\u800c\uff0c\u8fd9\u5e76\u975e\u4e0d\u80fd\u59a5\u534f). Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classfifiers to perform prediction and lack a mechanism for dynamic outputs. This severely(\u4e25\u91cd\u5730) curtails\uff08\u524a\u51cf\u4e86\uff09 their flexibility and limits their \"zero-shot\" capabilities. A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale(\u4e24\u8005\u7684\u533a\u522b\u662f\u89c4\u6a21\u7684\u4e0d\u540c). While Mahajan et al.(2018) and Kolesnikov et al. (2019) trained their models for accelerator\uff08gpu\u6216\u8005tpu\u7b49\u786c\u4ef6\uff0c\u76f8\u6bd4cpu\u662f\u52a0\u901f\u7684\uff09 years on millions to billions of images\uff08\u767e\u4e07\u5230\u5341\u4ebf\u7ea7\u89c4\u6a21\uff09, VirTex, ICMLM, and ConVIRT (\u8fd9\u4e09\u4e2a\u65b9\u6cd5\u76f8\u6bd4\u524d\u9762\u51e0\u4e2a\u65b9\u6cd5\uff0c\u8bad\u7ec3\u7684\u65f6\u95f4\u77ed\uff0c\u4f7f\u7528\u7684\u6570\u636e\u91cf\u5c0f)trained for accelerator days\uff08\u591a\u5c11\u5929\uff09 on one to two hundred thousand images\uff08\u51e0\u5341\u4e07\u89c4\u6a21\uff09. In this work\uff08\u6570\u636e\u89c4\u6a21\u53d8\u5927\uff09, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConvVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models\uff08\u4eceresnet->efficientnet->vit-L\uff09 spanning almost 2 orders of magnitude of compute(\u8ba1\u7b97\u91cf\u63d0\u5347\u4e86100\u500d) and observe that transfer performance is a smoothly predictable function of compute\uff08\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u679c\u548c\u8ba1\u7b97\u91cf\u57fa\u672c\u4e0a\u662f\u6210\u6b63\u76f8\u5173\u7684\uff09 (Hestness et al., 2017; Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find it can be competitive with prior task-specific supervised models. We also confirm(\u786e\u8ba4) these findings with linear-probe(\u9aa8\u5e72\u7f51\u7edc\u51bb\u4f4f\uff0c\u7528\u4f5c\u62bd\u53d6\u7279\u5f81\uff0c\u518d\u52a0\u5206\u7c7b\u5934) representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust\uff08\u66f4\u9c81\u68d2\u7684\uff0c\u5728\u7d20\u63cf\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u4e5f\u633a\u597d\uff09 than equivalent(\u76f8\u7b49\u7684) accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model's capability. (\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u96f6\u6837\u672c CLIP \u6a21\u578b\u6bd4\u540c\u7b49\u7cbe\u5ea6\u7684\u76d1\u7763 ImageNet \u6a21\u578b\u66f4\u7a33\u5065\uff0c\u8fd9\u8868\u660e\u4efb\u52a1\u65e0\u5173\u6a21\u578b\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u66f4\u80fd\u4ee3\u8868\u6a21\u578b\u7684\u80fd\u529b). These results have significant policy(\u653f\u7b56\u7684) and ethical\uff08\u4f26\u7406\u7684\uff09 implications, which we consifer in Section 7. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) \u5f31\u76d1\u7763\u65b9\u6cd5 Large Scale Learning of General Visual Representations for Transfer \u5b66\u4e60\u5927\u89c4\u6a21\u901a\u7528\u89c6\u89c9\u8868\u5f81\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60 BiT Kolesnikov et al. (2019) \u5f31\u76d1\u7763\u65b9\u6cd5 Deep Learning Scaling is Predictable, Empirically \u6df1\u5ea6\u5b66\u4e60\u7684\u6269\u5c55\u60c5\u51b5\u662f\u53ef\u9884\u6d4b\u7684\uff0c\u662f\u6709\u7ecf\u9a8c\u503c\u7684 - Hestness et al., 2017 Scaling Laws for Neural Language Models \u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u89c4\u5219 - Kaplan et al., 2020 \u65b9\u6cd5 \u81ea\u7136\u8bed\u8a00\u76d1\u7763 At the core of our approach is the idea of learning perception from supervision contained in natural language. As discussed in the introduction, this is not at all a new idea, however, terminology(\u672f\u8bed\u3001\u7528\u4e8e) used to describe work in this space is varied, even seemingly contradictory(\u77db\u76fe\u7684), and stated motivations are diverse. Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016), and Desai & Johnson (2020) all introduce methods which learn visual representations from text paired with images but describe their approaches as unsupervised, self-supervised, weakly supervised, and supervised respectively. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Contrastive Learning of Medical Visual Representations from Paired Images and Text \u4ece\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\u4e2d\u5b66\u4e60\u533b\u7597\u89c6\u89c9\u8868\u5f81 ConVIRT ConVIRT(Zhang et al., 2020) Self-Supervised Learning of Visual Features through Embedding Images into Text Topic Spaces \u901a\u8fc7\u628a\u56fe\u7247\u5d4c\u5165\u5230\u6587\u672c\u7a7a\u95f4\u4e2d\uff0c\u6765\u7528\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u5b66\u4e60\u89c6\u89c9\u7279\u5f81 - Gomez et al. (2017) Learning Visual Features from Large Weakly Supervised Data \u4ece\u5927\u89c4\u6a21\u7684\u5f31\u76d1\u7763\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9\u7279\u5f81 - Joulin et al.(2016) VirTex: Learning Visual Representations from Textual Annotations \u4ece\u6587\u672c\u6ce8\u91ca\u4e2d\u5b66\u4e60\u89c6\u89c9\u8868\u5f81 VirTex VirTex(Desai & Johnson,2020) We emphasize that what is common across this line of work is not any of the details of th particular methods used but the appreciation(\u6b23\u8d4f) of natural language as a training signal(\u628a\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u4e00\u79cd\u8bad\u7ec3\u4fe1\u53f7). All these approaches are learning from natrual language supervision. Although early work wrestled(\u6454\u8de4) with the complexity of natural language when using topic model and n-gram representations, improvements in deep contextual representation learning\uff08\u5177\u6709\u4e0a\u4e0b\u6587\u8bed\u4e49\u73af\u5883\u7684\u5b66\u4e60\u65b9\u5f0f\uff09 suggest we now have the tools to effectively leverage\uff08\u5229\u7528\uff09 this abundant source of supervision(McCann et al., 2017). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learned in Translation: Contextualized Word Vectors \u4e0a\u4e0b\u6587\u8bcd\u5411\u91cf\uff0c\u5b66\u4e60\u7528\u4e8e\u7ffb\u8bd1 - McCann et al., 2017 Learning from natual language has several potential strengths(\u4f18\u52bf) over other training methods. It's much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification since it does not require annotations to be in a classic \"machine learning compatible(\u517c\u5bb9\u7684) format\" such as the canonical(\u5178\u8303) 1-of-N majority vote \"gold label\". Instead, methods which work on natural language can learn passively(\u88ab\u52a8\u5730) from the supervision contained in the vast amount of text on the internet. Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn't \"just\" learn a representation but also connects that repesentation to language which enables flexible zero-shot transfer. (\u4e0e\u5927\u591a\u6570\u65e0\u76d1\u7763\u6216\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u5b66\u4e60\u8fd8\u6709\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u4e0d\u4ec5\u201c\u53ea\u662f\u201d\u5b66\u4e60\u4e00\u79cd\u8868\u793a\uff0c\u800c\u4e14\u8fd8\u5c06\u8fd9\u79cd\u8868\u793a\u4e0e\u8bed\u8a00\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u5c31\u80fd\u5b9e\u73b0\u7075\u6d3b\u7684\u96f6\u6837\u672c\u8fc1\u79fb) In the following subsections, we detail the specific approach we settled on. Creating a Sufficiently Large Dataset Existing work has mainly used three datasets, MS-COCO(Lin et al., 2014), Visual Genome(Krishna et al., 2017), and YFCC100M(Thomee et al., 2016). While MS-COCO and Visual Genome are high quality crowd-labeled datasets, they are small by modern standards with approximately 100,000 training photos each. By comparison, other computer vision systems are trained on up to 3.5 billion Instagram photos(Mahajan et al., 2018). YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality(\u7a00\u758f\u4e14\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684). Many images use automatically generated filenames like 20160716_113957.JPG as \"titles\" or contain \"descriptions\" of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos. This is approximately the same size as ImageNet. A major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet. Since existing datasets do not adequately(\u5145\u5206\u5730) reflect this possibility, considering results only on them would underestimate(\u4f4e\u4f30) the potential of this line of research.(\u73b0\u6709\u7684\u6570\u636e\u96c6\u91cf\u7ea7\u6bd4\u8f83\u5c0f\uff0c\u56e0\u6b64\u4e0d\u80fd\u5b8c\u5168\u53cd\u6620\u51fa\u7528\u6d77\u91cf\u4e92\u8054\u7f51\u7684\u81ea\u7136\u8bed\u8a00\u6570\u636e\u505a\u76d1\u7763\u80fd\u505a\u6210\u4e8b\u60c5\u7684\u53ef\u80fd\u6027\uff0c\u4ec5\u4ec5\u4f7f\u7528\u8fd9\u6837\u7684\u6570\u636e\u96c6\uff0c\u53ef\u80fd\u4f1a\u4f4e\u4f30\u8fd9\u65b9\u9762\u7814\u7a76\u7684\u6f5c\u529b). To address this, we constructed a new dataset of 400 million (image,text) pairs collected form a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process(\u6784\u5efa\u6570\u636e\u8fc7\u7a0b\u4e2d) whose text includes one of a set of 500,000 queries. We approximately class balance the results by including up to 20,000(image,text) pairs per query. The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2. We refer to this dataset as WIT for WebImageText. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Microsoft COCO: Common Objects in Context \u4e0a\u4e0b\u6587\u4e2d\u7684\u5e38\u89c1\u7269\u4f53 COCO Lin et al., 2014 Visual Genome: Connecting Language and Vision Using Crowd sourced Dense Image Annotations \u89c6\u89c9\u57fa\u56e0\u7ec4: \u4f7f\u7528\u4f17\u5305\u7684\u5bc6\u96c6\u56fe\u50cf\u6807\u6ce8\u6765\u8fde\u63a5\u8bed\u8a00\u548c\u89c6\u89c9 - Krishna et al., 2017 YFCC100M: The New Data in Multimedia Research \u591a\u5a92\u4f53\u7814\u7a76\u4e2d\u7684\u65b0\u6570\u636e\u96c6 YFCC100M Thomee et al., 2016 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) 2.3 Selecting an Efficient Pre-Training Method State-of-the-art computer vision systems use very large amounts of compute. Mahajan et al.(2018) required 19 GPU years to train their ResNeXt101-32x48d and Xie et al.(2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2. When considering that both these systems were trained to predict only 1000 ImageNet classes, the task of learning an open set of visual concepts from natural language seems daunting(\u4ee4\u4eba\u751f\u754f). In the course of our efforts, we found training efficiency(\u8bad\u7ec3\u6548\u7387) was key to successfully scaling natural language supervision(\u6210\u529f\u6269\u5c55\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u7684\u5173\u952e) and we selected our final pre-training method based on this metric. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) Self-training with Noisy Student improves ImageNet classification \u4f7f\u7528\u566a\u58f0\u5b66\u751f\u7684\u81ea\u5b66\u4e60\u6765\u63d0\u5347\u56fe\u7247\u5206\u7c7b\u6548\u679c - (Xie et al., 2020) Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image. However, we encountered difficulties efficiently scaling this method. In Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet-50 image encoder, learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of-words encoding of the same text. Both these approaches share a key similarity. They try to predict the exact words of the text accompanying each image. This is a difficult task due to the wide variety of descriptions, comments, and related text that co-occur with images. Recent work in contrastive representation learning for images has found that contrastive objectives can learn better representations than their equivalent predictive objective (Tian et al., 2019). Other work has found that although generative models of images can learn high quality image representations, they require over an order of magnitude (\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7) of more compute than contrastive models with the same performance(Chen et al., 2020a). Noting these findings, we explored training a system to solve the potentially easier proxy task of predicting only which text as a whole(\u6574\u4e2a\u6587\u672c) is paired with which image and not the exact words(\u786e\u5207\u7684\u8bcd) of that text. Starting with the same bag-of-words encoding baseline, we swapped the predictive objective\uff08\u9884\u6d4b\u6027\u7684\u4efb\u52a1\uff09 for a contrastive objective\uff08\u5bf9\u6bd4\u6027\u7684\u7684\u4efb\u52a1\uff09 in Figure 2 and observed a further 4x efficientcy improvement in the rate of zero-shot transfer to ImageNet. \u53bb\u9884\u6d4b\u56fe\u7247\u5bf9\u5e94\u7684\u6587\u672c\u4e2d\u7684\u5355\u4e2a\u8bcd -> \u53bb\u9884\u6d4b\u56fe\u7247\u5bf9\u5e94\u7684\u6587\u672c\u4e2d\u7684\u5355\u4e2a\u8bcd\u7684\u8868\u5f81\uff08\u6548\u679c\u63d0\u53473\u500d\uff09 -> \u53bb\u9884\u6d4b\u56fe\u7247\u4e0e\u6587\u672c\u662f\u5426\u5bf9\u5e94\uff08\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u53474\u500d\uff09 \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Contrastive Multiview Coding \u5bf9\u6bd4\u7684\u591a\u89d2\u5ea6\u7f16\u7801 - Tian et al., 2019 Generative Pretraining From Pixels \u4ece\u50cf\u7d20\u505a\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3 - Chen et al., 2020a \u56fe2 CLIP is much more efficient at zero-shot transfer than our image caption baseline. Althougn highly expressive(\u5bcc\u6709\u8868\u73b0\u529b), we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words(BoW) encoding of the text(Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x. Given a batch of N(image, text)pairs, CLIP is trained to predict which of the NxN possible(image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizeing the cosine similarity of the embeddings of the N^2 - N incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores. In figure 3, we include pseudocode of the core of an implementation of CLIP. To our knowledge this batch construction technique and objectives was first introduced in area of deep metric learning as the multi-class N-pair loss Sohn(2016) was popularized for contrastive representation learning by Oord et al.(2018) as the InfoNCE loss, and was recently adapted for contrastive(text, image) representation learning in the domain of medical imaging by Zhang et al.(2020). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Improved deep metric learning with multi-class N-pair loss objective \u4f7f\u7528\u591a\u5206\u7c7b\u7684\u591a\u5bf9\u7684\u635f\u5931\u51fd\u6570\u6765\u63d0\u5347\u6df1\u5ea6\u8bc4\u4f30\u5b66\u4e60\u80fd\u529b - 2016-12-05 Representation Learning with Contrastive Predictive Coding \u4f7f\u7528\u5bf9\u6bd4\u9884\u6d4b\u7f16\u7801\u6765\u8fdb\u884c\u8868\u5f81\u5b66\u4e60 CPC Oord et al.(2018) Contrastive Learning of Medical Visual Representations form Paired Images and Text \u4f7f\u7528\u6210\u5bf9\u7684\u56fe\u7247\u6587\u672c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7528\u4e8e\u533b\u7597\u89c6\u89c9\u8868\u5f81 Zhang et al.(2020) # image_encoder - ResNet of Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) # [n, d_i] T_f = text_encoder(T) # [n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t) / 2 \u56fe3 Numpy-like pseudocode for the core of an implementation of CLIP Due to the large size of our pre-training dataset, over-fitting is not a major concern and the details of training CLIP are simplified compared to the implementation of Zhang et al.(2020). We train CLIP form scratch without initiallizeing the image encoder with ImageNet weights or the text encoder with pre-trained weights. We do not use the non-linear projection between the representation and the contrastive embedding space, a change which was introduced by Bachman et al.(2019) and popularized by Chen et al.(2020b). We instead use only a linear projection to map from each encoder's representation to the multi-modal embedding space. We did not notice a difference in training two versions and speculate(\u63a8\u6d4b) that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods. We also remove the text transformation function {t_u} from Zhang et al.(2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP's pre-training dataset are only a single sentence. We also simplify the image transformation function $t_v$. A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, $\\tau$, is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Contrastive Learning of Medical Visual Representations form Paired Images and Text \u4f7f\u7528\u6210\u5bf9\u7684\u56fe\u7247\u6587\u672c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7528\u4e8e\u533b\u7597\u89c6\u89c9\u8868\u5f81 Zhang et al.(2020) A Simple Framework for Contrastive Learning of Visual Representations \u4e00\u4e2a\u7b80\u5355\u7684\u6846\u67b6\u7528\u4e8e\u89c6\u89c9\u8868\u5f81\u7684\u5bf9\u6bd4\u5b66\u4e60 SimCLR Chen et al.(2020b) Choosing and Scaling a Model We consider two different architectures for the image encoder. For the first, we use ResNet-50 as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNet-D improvements from He et al.(2019) and the antialiased(\u6297\u952f\u9f7f) rect-2 blur(\u6a21\u7cca) pooling from Zhang(2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of \"transformer-style\" multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer(ViT)(Dosovitskiy et al.,2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme(\u65b9\u6848). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Bag of Tricks from Image Classification with Convolutional Neural Networks \u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b\u7684\u4e00\u5806\u6280\u5de7 - He et al.(2019) Making Convolutional Networks Shift-Invariant Again \u4f7f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u66f4\u52a0\u5c3a\u5ea6\u4e0d\u53d8 - Zhang(2019) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale transformer\u7528\u4e8e\u56fe\u7247\u5206\u7c7b ViT Dosovitskiy et al. (2020) The text encoder is a Transformer(Vaswani et al.,2017) with the architecture modifications described in Radford et al.(2019). As a base size we use a 63M-parameter 12 layer 512-wide model with 8 attention heads. The transformer operates on lower-cased byte pair encoding(BPE) representation of the text with a 49,152 vocab size(Sennrich et al.,2015). For computational efficiency, the max sequence length was capped(\u4e0a\u9650) at 76. The text sequence is bracketed with(\u7528\u62ec\u53f7\u62ec\u8d77\u6765) [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary(\u8f85\u52a9) objective, though exploration of this is left as future work. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 ViT Vaswani et al.,2017 Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u5668 GPT-2 Radford er al, 2019 While previous computer vision research has often scaled models by increasing the width(Mahajan et al., 2018) or depth(He et al., 2016a) in isolation(\u9694\u79bb\u7684\uff0c\u5355\u72ec\u7684), for the ResNet image encoders we adapt the approach of Tan & Le(2019) which found that allocating(\u5206\u914d) additional compute across all of width, depth, and resolution outperforms only allocating(\u5206\u914d) it to only one dimension of the model. While Tan & Le(2019) tune the ratio of compute allocated to each dimension for their EfficientNet architecture, we use a simple baseline of allocating additional compute equally to increasing the width, depth, and resolution of the model. For the text encoder, we only scale the width of the model to be proportional(\u6210\u6bd4\u4f8b\u7684) to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP's performance to be less sensitive to the capacity of the text encoder. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) Deep Residual Learning for Image Recognition \u4f7f\u7528\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u6765\u505a\u89c6\u9891\u5206\u7c7b\u4efb\u52a1 ResNet He et al., 2016a EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks EfficientNet: \u91cd\u65b0\u601d\u8003\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6269\u5c55\u95ee\u9898 EfficientNet Tan & Le(2019) 2.5 Training We train a series of 5 ResNets and 3 Vision Transformers. For the ResNets we train a ResNet-50, a ResNet-101, and then 3 more which follow EfficientNet-style model scaling and use approximately 4x, 16x, 64x the compute of a ResNet-50. They are denoted as RN50x4, RN50x16, and RN50x64 respectively. For the Vision Transformers we train a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all models for 32 epochs. We use the Adam optimizer(Kingma & Ba, 2014) with decoupled(\u89e3\u8026\u7684) weight decay regularization (Loshchilov & Hutter, 2017) applied to all weights that are not gains or biases, and decay the learning rate unsing a cosine schedule(Loshchilov & Hutter, 2016). Initial hyper-parameters were set using a comnination of grid searches, random search, and manual tuning on the baseline ResNet-50 model when trained for 1 epoach. Hyper-parameters were then adapted heuristically(\u542f\u53d1\u5f0f\u7684) for larger models due to computational constraints. The learnable temperature parameter $\\tau$ was initialized to the equivalent of 0.07 from (Wu et al., 2018). and clipped(\u526a\u5207) to prevent scaling the logits by more than 100 which we found necessary to prevent(\u9632\u6b62) training instability\uff08\u4e0d\u7a33\u5b9a\uff09. We use a very large minibatch size of 32,768. Mixed-precision(Micikevicius et al., 2017) was used to accelerate training and save memory. To save additional memory, gradient checkpointing(\u68af\u5ea6\u68c0\u67e5\u70b9) (Griewank & Walther, 2000; Chen et al.,2016), half-precision Adam statistics(Dhariwal et al.,2020), and half-precision stochastically rounded(\u534a\u7cbe\u5ea6\u968f\u673a\u56db\u820d\u4e94\u5165\u7684) text encoder weights were used. The calculation(\u8ba1\u7b97) of embedding similarities was also sharded with individual GPUs computing only the subset of the pairwise similarities necessary for their local batch of embeddings. The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs. For the ViT-L/14 we also pre-train at a higher 336 pixel resolution for one additional epoch to boost performance similar to FixRes(Touvron et al.,2019). We denote this model as ViT-L/14@336px. Unless otherwise specified(\u9664\u975e\u53e6\u6709\u89c4\u5b9a), all results reported in this as \"CLIP\" use this model which we found to perform best. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Adam: A Method for Stochastic(\u968f\u673a) Optimization \u968f\u673a\u4f18\u5316\u7684\u4e00\u79cd\u65b9\u6cd5 Adam Kingma & Ba, 2014 Decoupled Weight Decay Regularization \u89e3\u8026\u7684\u6743\u91cd\u8870\u51cf\u6b63\u5219\u5316 - Loshchilov & Hutter, 2017 SGDR: Stochastic Gradient Descent with Warm Restarts \u4f7f\u7528\u70ed\u542f\u52a8\u7684\u65b9\u5f0f\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f8\u7ed3\u5408 SGDR Loshchilov & Hutter, 2016 Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination \u901a\u8fc7\u65e0\u53c2\u6570\u5b9e\u4f8b\u7ea7\u522b\u5224\u522b\u7684\u65e0\u76d1\u7763\u7279\u5f81\u5b66\u4e60 - Wu et al., 2018 Mixed Precision Training \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - Micikevicius et al., 2017 Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of cumputational differentiation \u8ba1\u7b97\u5fae\u5206\u7684\u53cd\u5411\u6216\u4f34\u968f\u6a21\u5f0f\u7684\u68c0\u67e5\u70b9\u5b9e\u73b0 - Griewank & Walther, 2000 Training Deep Nets with Sublinear Memory Cost. \u4f7f\u7528\u6b21\u7ebf\u6027\u5185\u5b58\u6210\u672c\u8bad\u7ec3\u6df1\u5ea6\u7f51\u7edc - Chen et al.,2016 Jukebox: A Generative Model for Music \u7528\u4e8e\u97f3\u4e50\u7684\u4e00\u4e2a\u751f\u6210\u5f0f\u6a21\u578b - Dhariwal et al.,2020 Fixing the train-test resolution dicrepancy \u4fee\u590d\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u8fa8\u7387\u5dee\u5f02 - Touvron et al.,2019 3. \u5b9e\u9a8c 3.1 \u96f6\u6837\u672c\u8fc1\u79fb 3.1.1 Motivation In computer vision, zero-shot learning usually refers to the study of generalizing to unseen object categories in image classification(Lampert et al.,2009). We instead use the term in a broader sense and study generalization to unseen datasets. We motivate this as a proxy for performing unseen tasks, as aspired to in the zero-data learning paper of Larochelle et al.(2008). While much research in the filed of unsupervised learning focuses on the representation learning capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the task-learning capabilities of machine learning systems. In this view, a dataset evaluates performance on a task on a specific distribution. However, many popular computer vision datasets were created by the research community primarily as benchmarks to guide the development of generic image classification methods rather than measuring performance on a specific task. While it is reasonable to say that(\u53ef\u4ee5\u8fd9\u4e48\u8bf4) the SVHN dataset measures the task of street number transciption on the distribution of Google Street View photos, it is unclear what \"real\" task the CIFAR-10 dataset measures. It is clear, however, what distribution CIFAR-10 is drawn from - TinyImages(Torralba et al.,2008). On these kinds of datasets, zero-shot transfer is more an evaluation of CLIP's robustness to distribution shift and domain generalization rather tahn task generalization. Please Section 3.3 for analysis focused on this. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer \u901a\u8fc7\u7c7b\u4e4b\u95f4\u5c5e\u6027\u8fc1\u79fb\uff0c\u6765\u5b66\u4f1a\u68c0\u6d4b\u6ca1\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b - Lampert et al.,2009 Zero-data learning of new tasks \u65b0\u4efb\u52a1\u7684\u96f6\u6837\u672c\u5b66\u4e60 - Larochelle et al.(2008) To our knowledge, Visual N-Grams(Li et al.,2017) first studied zero-shot transfer to existing image classification datasets in the manner described above. It is also the only other work we are aware of that has studied zero-shot transfer to standard image classification datasets using a generically pre-trained model and serves as(\u5145\u5f53) the best reference point for contextualizing CLIP. Their approach learns the parameters of a dictionary of 142806 visual n-grams(spaning 1- to 5- grams) and optimizes these n-grams using a differential version of Jelinek-Mercer smoothing to maximize the probability of all text n-grams for a given image. In order to perform zero-shot transfer, they first convert the text of each of the dataset's class names into its n-gram representation and then compute its probability according to their model, predicting the one with the highest score. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learning Visual N-Grams from Web Data \u4ece\u7f51\u9875\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9N-Grams - Li et al.,2017 \u63d2\u5165\u4e00\u70b9\u5185\u5bb9 Learning Visual N-Grams from Web Data Real-world image recognition systems need to recognize tens of thousands of classes that constitute(\u6784\u6210) a plethora of (\u8bb8\u591a\u7684) visual concepts. The traditional approach of annotating thousands of images per class for training is infeasible(\u4e0d\u53ef\u884c\u7684) in such a scenario(\u8fd9\u79cd\u60c5\u51b5\u4e0b), prompting the use of webly supervised data. This paper explores the training of image-recognition systems on large numbers of images and associated user comments. In particular, we develop visual n-gram models that can predict arbitrary phrases that are relevant to the content of an image. Our visual n-gram models are feed-forward convolutional networks trained using new loss functions that are inspired by n-gram models commonly used in language modeling. We demonstrate the merits of our models in phrase prediction, phrase-based image retrieval, relating images and captions, and zero-shot transfer. Our focus on studying zero-shot transfer as an evaluation of task learning is inspired by work demonstrating task learning in the field of NLP. To our knowledge Liu et al.(2018) first identified task learning as an \"unexpected side-effect\"(\u4e0d\u671f\u671b\u7684\u526f\u4f5c\u7528) when a language model trained to generate Wikipedia articles learned to reliably transliterate(\u97f3\u8bd1) names between languages. While GPT-1(Radford et al.,2018) focused on pre-training as a transfer learning method to improve supervised fine-tuning, , it also included an ablation study demonstrating that the performance of four heuristic(\u542f\u53d1\u5f0f\u7684) zero-shot transfer methods improved steadily over the course of pre-training(\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7a33\u5b9a\u5730\u6539\u5584), without any supervised adaption. This analysis served as the basis for GPT-2(Radford et al.,2019) which focused exclusively(\u4ec5) on studying the task-learning capabilities of language models via zero-shot transfer. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Generating Wikipedia by Summarizing Long Sequences \u901a\u8fc7\u5f52\u603b\u957f\u5e8f\u5217\u6765\u751f\u6210wiki - Liu et al.(2018) 3.1.2 Using CLIP for Zero-shot Transfer CLIP is pre-trained to predict if an image and a text snippet(\u7247\u6bb5) are paired together in its dataset. To perform zero-shot classification, we reuse this capability. For each dataset, we use the names of all the classes in the dataset as the set of potential text pairings and predict the most probable(\u53ef\u80fd\u7684)(image, text) pair according to CLIP. In a bit more detail, we first compute the feature embedding of the image and the feature embedding of the set of possible texts by their respective(\u5404\u81ea\u7684) encoders. The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter $\\tau$, and normalized into a probability distribution via a softmax. Note that this prediction layer is a multinormial(\u591a\u9879\u7684) logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling. When interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork(Ha et al.,2016) which generates the weights of a linear classifier based on the text specifying the visual concepts that the classes represent. Lei Ba et al.(2015) first introduced a zero-shot image classifier of this form while the idea of generating a classifier from natual language dates back to at least Elhoseiny et al.(2013). Continuing with this interpretation, every step of CLIP pre-training can be viewed as optimizeing the performance of a randomly created proxy to a computer vision dataset which contains 1 example per class and has 32768 total classes defined via natural language descriptions. For zero-shot evaluation, we cache the zero-shot classifier once it has been computed by the text encoder and reuse it for all subsequent predictions. This allows the cost(\u6210\u672c) of generating it to be amortized(\u5206\u644a) across all the predictions in a dataset. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 HyperNetworks \u8d85\u7f51\u7edc - Ha et al.,2016 Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions \u4f7f\u7528\u6587\u672c\u6027\u7684\u63cf\u8ff0\u6765\u9884\u6d4b\u6df1\u5ea6\u96f6\u6837\u672c\u5377\u79ef\u7f51\u7edc - Lei Ba et al.(2015) Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions \u4f7f\u7528\u7eaf\u6587\u672c\u63cf\u8ff0\u6765\u505a\u96f6\u6837\u672c\u5b66\u4e60 - Elhoseiny et al.(2013) 3.1.3 Initial Comparison to Visual N-Grams In Table 1 we compare Visual N-Grams to CLIP. The best CLIP model improves accuracy on ImageNet from a proof of concept 11.5% to 76.2% and matches the performance of the original ResNet-50 despite using none of the 1.28 million crowd-labeled training examples available for this dataset. Additionally, the top-5 accuracy of CLIP models are noticeably higher than their top-1, and this model has a 95% top-5 accuracy, matching Inception-V4(Szegedy et al.,2016). The ability to match the performance of a strong, fully supervised baselines in a zero-shot setting suggests CLIP is a significant step towards flexible and practical zero-shot computer vision classifiers. As mentioned above, the comparison to Visual N-Grams is meant for contextualizing the performance of CLIP and should not be interpreted as a direct methods comparison between CLIP and Visual N-Grams as many performance relevant differences between the two systems were not controlled for. For instance, we train on a dataset that is 10x larger, use a vision model that requires nearly 100x more compute per prediction, likely used over 1000x their training compute, and use a transformer-based model which did not exist when Visual N-Grams was published. As a closer comparison, we trained a CLIP ResNet-50 on the same YFCC100M dataset that Visual N-Grams was trained on and found it matched their reported ImageNet performance within a V100 GPU day. This baseline was also trained from scatch instead of being initiallized from pre-trained ImageNet weights as in Visual N-Grams. - aYahoo ImageNet SUN Visual N-Grams 72.4 11.5 23.0 CLIP 98.4 76.2 58.5 Table 1. Comparing CLIP to prior zero-shot transfer image classification results. CLIP improves performance on all three datasets by a large amount. This improvement reflects(\u53cd\u6620\u4e86) many differences in the 4 years since the development of Visual N-Grams(Li et al.,2017). CLIP also outperforms Visual N-Grams on the other 2 reported datasets. On a Yahoo, CLIP achieves a 95% reduction(\u51cf\u5c11) in the number of errors, and on SUN, CLIP more than doubles the accuracy of Visual N-Grams. To conduct a more comprehensive analysis and stress test, we implement a much larger evaluation suite detailed in Appendix A. In total we expand from the 3 datasets reported in Visual N-Grams to include over 30 datasets and compare to over 50 existing computer vision systems to contextualize results. 3.1.4 Prompt Engineering And Ensembling Most standard image classification datasets treat the information naming or describing classes which enables narural language based(\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00) zero-shot transfer as an afterthought(\u4e8b\u540e\u60f3\u6cd5). The vast majority of datasets annotate images with just a numeric id of the label and contain a file mapping these ids back to their names in English. Some datasets, such as Flowers102 and GTSRB, don't appear to include this mapping at all in their released versions preventing(\u9632\u6b62) zero-shot transfer entirely. For many datasets, we observed these labels may be chosen somewhat(\u6709\u4e9b) haphazardly(\u968f\u610f) and do not anticipate(\u9884\u6d4b) issues related to zero-shot transfer which relies on task description in order to transfer successfully. A common issue is polysemy(\u6b67\u4e49\u6027). When the name of a class is the only information provided to CLIP's text encoder it is unable to differentiate which word sense is meant due to the lack of context. In some cases multiple meanings of the same word might be included as different classes in the same dataset! This happens in ImageNet which contains both construction cranes(\u5efa\u7b51\u8d77\u91cd\u673a) and cranes that fly.(\u98de\u7684\u9e64). Another example is found in classes of the Oxford-IIIT Pet dataset where the word boxer(\u62f3\u5e08\u72ac\u3001\u62f3\u51fb\u624b) is, from context, clearly referring to a breed of dog, but to a text encoder lacking context could just as likely refer to a type of athlete(\u8fd0\u52a8\u5458). Another issue we encountered is that it's relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template \"A photo of a {label}.\" to be a good default that helps the text is about the content of the image. This often improves performance over the baseline of using only the label text. For instance, just using this prompt improves accuracy on ImageNet by 1.3%. Similar to the \"prompt engineering\" discussion around GPT-3(Brown et al.,2020; Gao et al.,2020), we have also observed that zero-shot performance can be significantly improved by customizing(\u5b9a\u5236) the prompt text to each task. A few, non exhaustive, examples follow(\u4ee5\u4e0b\u662f\u4e00\u4e9b\u975e\u8be6\u5c3d\u7684\u793a\u4f8b). We found that on several fine-grained image classification datasets that it helped to specify(\u6307\u5b9a) the category. For example on Oxford-IIIT Pets, using \"A photo of a {label}, a type of pet.\" to help provide context worked well. Likewise, on Food101 specifying a type of food and on FGVC Aircraft a type of aircraft helped too. For OCR datasets, we found that putting quotes(\u5f15\u53f7)around the text or number to be recognized improved performance. Finally, we found that on satellite image classification datasets it helped to specify that the images were of this form and we use variants of \"a satellite phot of a {label}.\" We also experimented with ensembling over multiple zero-shot classifiers as another way of improving performance.These classifiers are computed by using different context prompts such as \"A photo of a big {label}\" and \"A photo of a small {label}\". We construct the ensemble over the embedding space instead of probability space. This allows us to cache a single set of averaged text embeddings so that the compute cost of the emsemble is the same as using a single classifier when amortized(\u5206\u644a) over many predictions. We've observed ensembling across many generated zero-shot classifiers to reliably improve performance and use it for the majority of datasets. On ImageNet, we ensemble 80 different context prompts and this improves performance by an additional 3.5% over the single default prompt discussed above. When considerd together, prompt engineering and ensembling improve ImageNet accuracy by almost 5%. In Figure 4 we visualize how prompt engineering and ensembling change the performance of a set of CLIP models compared to the contextless baseline approach of directly embedding the class name as done in Li et al.(2017). Figure4. Prompt engineering and ensembling improve zero-shot performance, Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is \"free\" when amortized over many predictions. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 Brown et al., 2020 Making Pre-trained Language Models Better Few-shot Learners. \u4f7f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6bd4\u5c11\u6837\u672c\u5b66\u4e60\u5668\u66f4\u597d - Gao et al.,2020 3.1.5 Analysis of Zero-shot CLIP Performance Since task-agnostic zero-shot classifiers for computer vision have been understudied, CLIP provides a promising opportunity to gain a better understanding of this type of model. In this section, we conduct a study of various properties of CLIP's zero-shot classifiers. As a first question, we look simply at how well zero-shot classifiers perform. To contextualize this, we compare to the performance of a simple off-the-shelf(\u73b0\u6210\u7684) baseline: fitting a fully supervised, regularized, logistic regression classifier on the features of the canonical(\u7ecf\u5178\u7684) ResNet-50. In Figure 5 we show this comparison across 27 datasets. Please see Apprendix A for details of datasets and setup. Figure 5. Zero-shot CLIP is competitive with a fully supervised baseline. Across a 27 dataset eval suite, a zero-shot CLIP classifier outperforms a fully supervised linear classifier fitted on ResNet-50 features on 16 datasets, including ImageNet. Zero-shot CLIP outperforms this baseline slightly more often than not and wins on 16 of the 27 datasets. Looking at individual datasets reveals(\u63ed\u793a\u4e86) some interesting behavior. On fine-grained classification tasks, we observe a wide spread in performance. On two of these datasets, Stanford Cars and Food101, zero-shot CLIP outperforms logistic regression on ResNet-50 features by over 20% while on two others, Flowers102 and FGVCAircraft, zero-shot CLIP underperforms by over 10%. On OxfordPets and Birdsnap, performance is much closer. We suspect these difference are primarily due to varying amounts of per-task supervision between WIT and ImageNet. On \"general\" object classification datasets such as ImageNet, CIFAR10/100,STL10, and PascalVOC2007 performance is relatively similar with a slight advantage for zero-shot CLIP in all cases. On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. Zero-shot CLIP significantly outperforms a ResNet-50 on two datasets measuting action recognition in videos. On Kinetics700, CLIP outperforma a ResNet-50 by 14.5%. Zero-shot CLIP also outperforms a ResNet-50's features by 7.7% on UCF1010. We speculate this is due to natural language providing wider supervision for visual concepts involving verbs, compared to the noun-centric object supervision in ImageNet. Looking at where zero-shot CLIP notably(\u5c24\u5176) underperforms, we see that zero-shot CLIP is quite weak on several specialized, complex, or abstract tasks such as satellite image classification (EuroSAT and RESISC45), lymph node tumor detection (PatchCamelyon), counting objects in synthetic scenes(CLEVRCounts), self-driving related tasks such as German traffic sign recognition(GTSRB), recognizing distance to the nearest car (KITTI Distance). These results highlight the poor capability of zero-shot CLIP on more complex tasks. By contrast, non-expert humans can robustly perform several of these tasks, such as counting, satellite image classification, and traffic sign recognition, suggesting significant room for improvement. However, we caution(\u8b66\u544a) that it is unclear whether measuring zero-shot transfer, as opposed to few-shot transfer, is a meaningful evaluation for difficult tasks that a learner has no prior experience with, such as lymph node tumor classification for almost all humans(and possibly CLIP). While comparing zero-shot performance to fully supervised models contextualizes the task-learning capabilities of CLIP, comparing to few-shot methods if a more direct comparison, since zero-shot is its limit. In Figure 6, we visualize how zero-shot CLIP compares to few-shot logistic regression on the features of many image models including the best publicly available ImageNet models, self-supervised learning methods, and CLIP itself. While it is intuitive(\u76f4\u89c9\u7684) to expect zero-shot to underperform one-shot, we instead find that zero-shot CLIP matches the performance of 4-shot logistic regression on the same feature space. This is likely due to an important difference between the zero-shot and few-shot approach. First, CLIP's zero-shot classifier is generated via natural language which allows for visual concepts to be directly specified (\"communicated\"). By contrast, \"normal\" supervised learning must infer concepts indirectly form training examples. Context-less example-based learning has the drawback(\u7f3a\u70b9) that many different hypotheses can be consistent with the data, especially in the one-shot case. A single image often contains many different visual concepts. Although a capable learner is able to exploit visual cues and heuristics, such as assuming that the concept being demonstrated is the primary object in an image, there is no guarantee. Figure 6. Zero-shot CLIP outperforms few-shot linear probes. Zero-shot CLIP matches the average performance of a 4-shot linear classifier trained on the same feature space and nearly matches the best results of a 16-shot linear across classifier publicly available models. For both BiT-M and SimCLRv2, the best performing model is highlighted. Light gray lines are other models in the eval suite. The 20 datasets with at least 16 examples per class were used in this analysis. A potential resolution(\u89e3\u51b3\u65b9\u6848) of this discrepancy(\u5dee\u5f02) between zero-shot and few-shot performance is to use CLIP's zero-shot classfifier as a prior for the weights of the few-shot classifier. While adding an L2 penlty towards the generated weights is a straightforward implementation of this idea, we found that hyperparameter optimization would often select for such a large value of this regularizer that resulting few-shot classifier was \"just\" the zero-shot classifier. Research into better methods of combing the strength of zero-shot transfer with flexibility of few-shot learning is a promising direction for future work. \u8be5\u6bb5\u7684\u542b\u4e49\u4e0d\u592a\u61c2\u3002 When comparing zero-shot CLIP to few-shot logistic regression on the features of other models, zero-shot CLIP roughtly(\u5927\u81f4) matches the performance of the best performing 16-shot classifier in out evaluation suite, which uses the features of a BiT-M ResNet-152x2 trained on ImageNet-21K. We are certain that a BiT-L model trained on JFT-300M would perform even better but these models have not been publicly released. That a BiT-M ResNet-152x2 performs best in a 16-shot setting is somewhat surprising since, as analyzed in Section 3.2, the Noisy Student EfficientNet-L2 outperforms it in a fully supervised setting by almost 5% on average across 27 datasets. In addition to studying the average performance of zero-shot CLIP and few-shot logistic regression, we also examine performance on individual datasets. In Figure 7, we show estimates for the number of labeled examples per class that a logistic regression classifier on the same feature space requires to match the performance of zero-shot CLIP. Since zero-shot CLIP is also a linear classifier, this estimates the effective data efficiency of zero-shot transfer in this setting. In order to avoid training thousands of linear classifiers, we estimate the effective data efficiency based on a log-linear interpolation of the performance of a 1,2,4,6,8,16-shot(when possible), and a fully supervised linear classifier trained on each dataset. We find that zero-shot transfer can have widely varying efficiency per dataset from less than 1 labeled example per class to 184. Two datasets, Flowers102 and EuroSAT underperform one-shot models. Half of the datasets require less than 5 examples per class with a median of 5.4. However, the mean estimated data efficiency is 20.8 examples per class. This is due to the 20% of datasets where supervised classifiers require many labeled examples per class in order to match performance . On ImageNet, zero-shot CLIP matches the performance of a 16-shot linear classifier trained on the same feature space. \u56fe7 Figure 7. The data efficiency of zero-shot transfer varies widely. Calculating the number of labeled examples per class a linear classifier on the same CLIP feature space requires to match the performance of the zeot-shot classifier contextualizes the effectiveness of zero-shot transfer. Values are estimated based on log-linear interpolation(\u63d2\u503c) of 1,2,4,8,16-shot and fully supervised results. Performance varies widely from still underperforming a ont-shot classifier on two datasets to matching an estimated 184 labeled examples per class. If we assume(\u5047\u5b9a) that evaluation(\u8bc4\u4f30) datasets are large enough that the parameters of linear classifiers trained on them are well estimated(\u4f30\u7b97), then because CLIP's zero-shot classifier is also a linear classifier, the performance of the fully supervised classifiers roughly sets an upper bound(\u4e0a\u9650) for what zero-shot transfer can achieve. In Figure 8 we compare CLIP's zero-shot performance with fully supervised linear classifiers across datastes. The dashed, y=x line represents an \"optimal\" zero-shot classifier that matches the performance of its fully supervised equivalent. For most datasets, the performance of zero-shot classifiers still underperform fully supervised classifiers by 10% to 25%, suggesting that there is still plenty(\u8db3\u591f) of headroom for improving CLIP's task-learning and zero-shot transfer capabilities. \u56fe8 Figure 8. Zero-shot performance is correlated with linear probe performance but still mostly sub-optimal. Comparing zero-shot and linear probe performance across datasets shows a strong correlation with zero-shot performance mostly shifted 10 to 25 points lower. On only 5 datasets does zero-shot performance approach linear probe performance. There is a positive correlation of 0.82 (p-value < 10^-6) between zero-shot performance and fully supervised performance, suggesting that CLIP is relatively(\u76f8\u5f53\u5730) consistent(\u4e00\u81f4\u7684) at connecting underlying(\u6839\u672c\u7684\u3001\u6f5c\u5728\u7684) representation and task learning to zero-shot transfer. However, zero-shot CLIP only approaches(\u63a5\u8fd1) fully supervised performance on 5 datasets: STL10, CIFAR10, Food101, OxfordPets, and Caltech101. On all 5 datasets, both zero-shot accuracy and fully supervised accuracy are over 90%. This suggests that CLIP may be more effective at zero-shot transfer for tasks where its underlying representations are also high quality. The slope of a linear regression model predicting zero-shot performance as a function of fully supervised performance, zero-shot performance improves by 1.28%. However, the 95th-percentile confidence intervals still include values of less than 1(0.93-1.79). Over the past few years, empirical studies of deep learning systems have documented that performance is predictable as a function of important quantities such as training compute and dataset size(Hestness et al., 2017; Kaplan et al.,2020). The GPT family of models has so far demonstrated consistent improvements in zero-shot performance across a 1000x increase in training compute. In Figure 9, we check whether the zero-shot performance of CLIP follows a similar scaling pattern. We plot the average error rate of the 5 ResNet CLIP models across 39 evaluations on 36 different datastes and find that a similar log-log linear scaling trend holds for CLIP across a 44x increase in model compute. While the overall trend is smooth, we found that performance on individual evaluations can be much noisier. We are unsure whether this is caused by high variance between individual training runs sub-tasks(as documented in D'Amour et al.(2020)) masking a steadily improving trend or whether performance is actually non-monotonic(\u975e\u5355\u8c03\u7684) as a function of compute on some tasks. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Deep Learning Scaling is Predictable, Empirically \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6269\u5c55\u6027\u662f\u53ef\u4ee5\u9884\u6d4b\u7684 - Scaling Laws for Neural Language Models \u6df1\u5ea6\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u6027\u89c4\u5219 Figure 9. Zero-shot CLIP performance scales smoothly as a function of model compute. Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log linear trend across a 44x range of compute spanning 5 different CLIP models. Lightly shaded lines are performance on individual evals, showing that performance is much more varied despite the smooth overall trend. 3.2 Representation Learning While we have extensively(\u5e7f\u6cdb\u5730) analyzed the task-learning capabilities of CLIP through zero-shot transfer in the pervious section, it is more common to study the representation learning capabilities of a model. There exist many ways to evaluate the quality of representations as well as disagreements(\u5206\u6b67) over what properties an \"ideal\" representation should have (Locatello et al.,2020). Fitting a linear classifier on a representation extracted from the model and measuring its performance on various datasets is a common approach. An alternative is measuring the performance of end-to-end fine-tuning of the model. This increases flexibility(\u7075\u6d3b\u6027), and prior work has convincingly demonstrated that fine-tuning outperforms linear classification on most image classification datasets(Kornblith et al.,2019;Zhai et al.,2019). While the high performance of fine-tuning motivates its study for practical reasons, we still opt for linear classifier based evaluation for several reasons. Our work is focused on developing a high-performing task and dataset-agnostic pre-training approach. Fine-tuning, because it adapts(\u9002\u5e94\u3001\u5e94\u7528) representations to each dataset during the fine-tuning phase, can compensate for(\u8865\u507f) and potentially mask failures to learn general and robust representations during the pre-training phase. Linear classifiers, because of their limited flexibility, instead hightlight these failures and provide clear feedback during development. For CLIP, training supervised linear classifier has the added benefit of being very similar to the approach used for its zero-shot classifiers which enables extensive comparisons and analysis in Section 3.1. Finally, we aim to compare CLIP to a comprehensive(\u7efc\u5408\u7684) set of existing models across many tasks. Studying 66 diferent models on 27 different datasets requires tuning 1782 different evaluations. Fine-tuning opens up a much larger design and hyper-parameter space, which makes it difficult to fairly evaluate and computationally expensive to compare a diverse set of techniques as discussed in other large scale empirical studies(Lucic et al.,2018; Choi et al.,2019). By comparison, linear classifiers require minimal hyper-parameter tuning and have standardized implementations and evaluation procedures. Please see Appendix A for further details on evaluation. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 A Sober look at the Unsupervised Learning of Disentangled Representations and their Evaluation \u6e05\u9192\u5730\u770b\u5f85\u89e3\u5f00\u8868\u5f81\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u4ed6\u4eec\u7684\u8bc4\u4f30 - Locatello et al.,2020 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark - - Zhai et al.,2019 Are GANs Created Equal? A Large-Scale Study - - Lucic et al.,2018 On Empirical Comparisons of Optimizers for Deep Learning \u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u5316\u5668\u7684\u7ecf\u9a8c\u6027\u6bd4\u8f83 - Choi et al.,2019 Figure 10 summarizes our findings. To minimize selection effects that could raise concerns of confirmation or reporting bias, we first study performance on the 12 dataset evaluation suite from Kornblith et al.(2019). While small CLIP models such as a ResNet-50 and ResNet-101 outperform other ResNets trained on ImageNet-1K(BiT-S and the originals), they underperform ResNets trained on ImageNet-21K(BiT-M). These small CLIP models also underperform models in the EfficeintNet family with similar compute requirements. However, models trained with CLIP scale very well and the largest model we trained (ResNet-50x64) slightly outperforms the best performing existing model (a Noisy Student EfficientNet-L2) on both overall score and compute efficiency. We also find that CLIP vision transformers are about 3x more compute efficient than CLIP ResNets, which allows us to reach higher overall performance within our compute budget(\u9884\u7b97). These results qualitatively replicate(\u590d\u5236) the findings of Dosovitskiy et al.(2020) which reported that vision transformers are more compute efficient than convnets when trained on sufficiently large datasets. Our best overall model is a ViT-L/14 that is fine-tuned at a higher resolution of 336 pixels on our dataset for 1 additional epoch. This model outperforms the best existing model across this evaluation suite by an average of 2.6%. Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfficientNet(Tan & Le,2019; Xie et al.,2020), MoCo(Chen et al.,2020d), Instagram-pretrained ResNeXt models (Mahajan et al.,2018;Touvron et al,.2019), BiT(Kolesnikov et al.,2019), ViT(Dosovitskiy et al.,2020), SimCLRv2(Chen et al.,2020c), BYOL(Grill et al.,2020), and the original ResNet models. (Left) Scores are averaged over 12 datasets studied by Kornblith et al.(2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Efficientnet: Rethinking model scaling for convolutional neural networks. - - Tan & Le,2019 Self-training with Noisy Student improves ImageNet classification - - Xie et al.,2020 Improved Baselines with Momentum Contrastive Learning - MoCo Chen et al.,2020d Exploring the Limits of Weakly Supervised Pretraining - - Mahajan et al.,2018 Fixing the train-test resolution discrepancy - - Touvron et al,.2019 Large Scale Learning of General Visual Representations for Transfer. - BiT Kolesnikov et al.,2019 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - ViT Dosovitskiy et al.,2020 Big Self-Supervised Models are Strong Semi-Supervised Learners - SimCLRv2 Chen et al.,2020c Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning - BYOL rill et al.,2020 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 As Figure 21 qualitatively shows, CLIP models learn a wider(\u76f8\u6bd4\u66f4\u5e7f\u6cdb\u7684) set of tasks than has previously been demonstrated in a single computer vision model trained ene-to-end from random initialization. These tasks include geo-localization, optical character recognition, facial emotion recognition, and action recognition. None of these tasks are measured in the evaluation suite of Kornblith et al.(2019). This could be argued to be a form of selection bias in Kornblith et al.(2019)'s study towards tasks that overlap(\u91cd\u53e0) with ImageNet. To address this, we also measure performance on a broader 27 dataset evaluation suite. This evaluation suite, detailed in Appendix A includes datasets representing the aforementioned tasks, German Traffic Signs Recognition Benchamark(Stallamp et al.,2011), as well as several other datasets adapted from VTAB(Zhai et al.,2019). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark - - Zhai et al.,2019 On this broader evaluation suite, the benefits of CLIP are more clear. All CLIP models, regradless of scale, outperform all evaluated systems in terms of compute efficiency. The improvement in average score of the best model over previous systems increases from 2.6% to 5%. We also find taht self-supervised systems do noticeably(\u660e\u663e) better on our broader evaluation suite. For instance, while SimCLRv2 still underperforms BiT-M on average on the 12 datasets of Kornblith et al.(2019), SimCLRv2 outperforms BiT-M on our 27 dataset evaluation suite. These findings suggest continuing to expand task diversity and coverage in order to better understand the \"general\" performance of systems. We suggest additional evaluation efforts along the lines of VTAB to be valuable. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 In addition to the aggregate analysis above, we visualize per-dataset differences in the performance of the best CLIP model and the best model in our evaluation suite across all 27 datasets in Figure 11. CLIP outperforms the Noisy Student EfficientNet-L2 on 21 of the 27 datasets. CLIP improves the most on tasks which require OCR(SST2 and HatefulMemes), geo-localization and scene recognition(Country211, SUN397), and activity recognition in videos(Kinetics700 and UCF101). In addition CLIP also does much better on fine-grained car and traffic sign recognition(Stanford Cars and GTSRB). This may reflect a problem with overly narrow supervision in ImageNet. A result such as the 14.7% improvement on GTSRB could be indicative of an issue with ImageNet-1K, which has only a single label for all traffic and street signs. This could encourage a supervised representation to collapse(\u5d29\u6e83\u3001\u574d\u584c) intra-class details and hurt accuracy on a fine-grained downstream task. As mentioned, CLIP still underperforms the EfficientNet on several datasets. Unsurprisingly, the dataset that the EfficientNet does best relative to CLIP on is the one it was trained on: ImageNet. The EfficientNet also slightly outperforms CLIP on low-resolution datasets such as CIFAR10 and CIFAR100. We suspect this is at least partly due to the lack of scale-based data augmentation in CLIP. The EfficientNet also does slightly better on PatchCamelyon and CLEVRCounts, datasets where overall performance is still low for both approaches. Figure 11. CLIP's features outperform the features of the best ImageNet model on a wide variety of datasets. Fitting a linear classifier on CLIP's features outperforms using the Noisy Student EfficientNet-L2 on 21 out of 27 datasets. 3.3 Robustness to Natural Distribution Shift (\u6709\u81ea\u7136\u7684\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b CLIP\u7684\u9c81\u68d2\u6027\u662f\u600e\u6837\u7684) In 2015, it was announced that a deep learning model exceeded human performance on the ImageNet test set(He et al.,2015). However, research in the subsequent(\u540e\u6765\u7684) years has repeatedly found that these models still make many simple mistakes(Dodge & Karam,2017;Geirhos et al.,2018;Alcorn et al.,2019), and new benchmarks testing these systems has often found their performance to be much lower than both their ImageNet accuracy and human accuracy(Recht et al.,2019;Barbu et al.,2019). What explains this discrepancy? Various ideas have been suggested and studied (IIyas et al.,2019;Geirhos et al.,2020). A common theme of proposed explanations is that deep learning models are exceedingly adept at(\u975e\u5e38\u64c5\u957f\u7684) finding correlations and patterns which hold across their trainiing dataset and thus improve in-distribution performance. However many of these correlations and patterns are actually spurious(\u865a\u5047\u7684) and do not hold for other distributions and result in large drops in performance on other datasets. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions - - Dodge & Karam,2017 ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness - - Geirhos et al.,2018 Strike(With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familliar Objects - - Alcorn et al.,2019 Do ImageNet Classifiers Generalize to ImageNet - - Recht et al.,2019 ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models - - Barbu et al.,2019 Adversarial(\u5bf9\u6297\u7684) Examples Are Not Bugs, They Are Features - - IIyas et al.,2019 Shortcut Learning in Deep Neural Networks - - Geirhos et al.,2020 We caution that(\u8b66\u544a), to date, (\u81f3\u4eca\u4e3a\u6b62) most of these studies limit their evaluation to models trained on ImageNet. Recalling the topic of discussion, it may be a mistake to generalize too far from these initial findings. To what degree are these failures attibutable to deep learning, ImageNet, or some combination of the two? CLIP models, which are trained via natural language supervision on a very large dataset and are capable of high zero-shot performance, are an opportunity to investigate this question from a different angle. Taori et al.(2020) is a recent comprehensive study moving towards quantifying and understanding these bahaviors for ImageNet models.Taori et al.(2020) study how the performance of ImageNet models change where evaluated on natural distribution shifts. They measure performance on a set of 7 distribution shifts: ImageNetV2 (Recht et al.,2019), ImageNet Sketch(Wang et al.,2019), Youtube-BB and ImageNet-Vid(Shankar et al.,2019), ObjectNet(Barbu et al.,2019), ImageNet Adversarial(Hendrycks et al.,2019), and ImageNet Rendition(Hendrycks et al.,2020a). They distinguish(\u533a\u5206) these datasets, which all consist of novel images collected from a variety of sources, from synthetic distribition shifts such as ImageNet-C(Hendrycks & Dietterich,2019), Stylized ImageNet(Geirhos et al.,2018), or adversarial attacks(Goodfellow et al.,2014) which are created by perturbing(\u6270\u52a8) existing images in various ways. They propose this distinction(\u533a\u522b) because in part because they find that while several techniques have been demonstrated to improve performance on synthetic distribution shifts, they often fail to yield consistent improvements on natural distributions. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Measuring Robustness to Natural Distribution Shifts in Image Classification - - Taori et al.(2020) Do ImageNet Classifiers Generalize to ImageNet - ImageNetV2 Recht et al.,2019 Learning Robust Global Representations by Penalizing Local Predictive Power - ImageNet Sketch Wang et al.,2019 Do Image Classifiers Generalize Across Time - ImageNet-Vid Shankar et al.,2019 ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models - ObjectNet Barbu et al.,2019 The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization - ImageNet Rendition Hendrycks et al.,2020a Benchmarking Neural Network Robustness to Common Corruptions and Perturbations - ImageNet-C Hendrycks & Dietterich,2019 ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness - Stylized ImageNet Geirhos et al.,2018 Explaining and Harnessing Adversarial Examples - - Goodfellow et al.,2014 Across these collected datasets, the accuracy of ImageNet models drop well below the expectation set by the ImageNet validation set. For the following summary discussion we report average accuracy across all 7 natural distribution shift datasets and average accuracy across the corresonding class subsets of ImageNet unless otherwise specified. Additionally, for Youtube-BB and ImageNet-Vid, which have two different evaluation settings, we use the average of pm-0 and pm-10 accuracy. ... 6. Limitations There are still many limitations to CLIP. While several of these are discussed as part of analysis in various sections, we summarize and collect them here. On datasets with training splits, the performance of zero-shot CLIP is on average competitive with the simple supervised baseline of a linear classifier on top of ResNet-50 features. On most of these datasets, the performance of this baseline is now well below the overall state of the art. Significant work is still needed to improve the task learning and trasfer capabilities of CLIP. While scaling has so fat steadily improved performance and suggests a route for continued improvement, we estimate around a 1000x increase in compute is required for zero-shot CLIP to reach overall state-of-the-art performance. This is infeasible(\u4e0d\u53ef\u884c\u7684) to train with current hardware. Further research into improving upon the computational and data efficiency of CLIP will be necessary. Analysis in Section 3.1 found that CLIP's zero-shot performance is still quite weak on several kinds of tasks. When compared to task-specific models, the performance of CLIP is poor on several types of fine-grained classification such as differentiating models of cars, species of flowers, and variants of aircraft. CLIP also struggles with more abstract and systematic tasks such as counting the number of objects in an image. Finally for novel tasks which are unlikely to be included in CLIP's pre-training dataset, such as classifying the distance to the nearest car in a photo, CLIP's performance can be near random. We are confident that there are still many, many, tasks where CLIP's zero-shot performance is near chance level. While zero-shot CLIP generalizes well to many natural image distributions as investigated in Section 3.3, we've observed that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. An illustrative(\u8bf4\u660e\u6027\u7684) example occurs for the task of OCR as reported in Appendix E. CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accuracy on the handwritten digits of MNIST. An embarrassingly(\u5c34\u5c2c\u5730) simple baseline of logistic regression on raw pixels outperforms zero-shot CLIP. Both semantic and near-duplicate nearest-neighbor retrieval verify that there are almost no images that resemble MNIST digits in our pre-training dataset. This suggests CLIP does little to address the underlying(\u6f5c\u5728\u7684) problem of brittle(\u8106\u7684) generalization of deep learning models. Instead CLIP tries to circumvent(\u89c4\u907f) the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution. This is a naive assumption that, as MNIST demonstrates, is easy to violate(\u8fdd\u53cd). Although CLIP can flexibly generate zero-shot classifiers for a wide variety of tasks and datasets, CLIP is still limited to choosing from only those concepts in a given zero-shot classifier. This is a significant restriction compared to a truly flexible approach like image captioning which could generate novel outputs.","title":"\u8bba\u6587\u901a\u8bfb"},{"location":"VisionMultiModal/CLIP/paper_read/#_1","text":"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined\uff08\u56fa\u5b9a\u7684\u3001\u63d0\u524d\u5b9a\u4e49\u597d\u7684\uff09 object categories. This restricted form of supervision limits their generality\uff08\u6cdb\u5316\u6027\uff09 and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative\uff08\u6709\u524d\u9014\u7684\u7248\u672c\uff09 which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient\uff08\u9ad8\u6548\u7684\uff09 and scalable\uff08\u53ef\u6269\u5c55\u7684\uff09 way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. \u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60 \u3001 prompt \u4f7f\u7528ImageNet\u8bad\u7ec3\u7684ResNet50\u7684\u6a21\u578b\u548cCLIP\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5bf9\u6bd4\u6548\u679c\uff0c\u53ef\u4ee5\u6253\u5e73\u3002","title":"\u6458\u8981"},{"location":"VisionMultiModal/CLIP/paper_read/#_2","text":"Pre-training methods which learn directly from raw text have revolutionized(\u5f7b\u5e95\u6539\u53d8) NLP over the last few years . (Dai &Le, 2015; Peters er al., 2018; Howard & Ruder, 2018; Radford et at., 2018; Devlin et al., 2018; Raffel et al., 2019). Task-agnostic objectives such as autoregressive\uff08\u81ea\u56de\u5f52\u9884\u6d4b\uff09 and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \"text-to-text\"\uff08\u6587\u5b57\u8fdb\u53bb\u3001\u6587\u5b57\u51fa\u53bb\uff09 as a standardized input-output interface (McCann et al., 2018; Radford et al., 2019; Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization.Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Semi-supervised Sequence Learning \u57fa\u4e8e\u534a\u76d1\u7763\u7684\u5e8f\u5217\u5b66\u4e60 - Dai &Le, 2015 Deep contextualized word representations \u6df1\u5ea6\u4e0a\u4e0b\u6587\u7684\u8bcd\u8868\u8fbe - Peters er al., 2018 Universal Language Model Fine-tuning for Text Classification \u6587\u672c\u5206\u7c7b\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5 ULMFiT Howard & Ruder, 2018 Improving Language Understanding by Generative Pre-Training \u901a\u8fc7\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6539\u5584\u8bed\u8a00\u7406\u89e3\u80fd\u529b GPT Radford et at., 2018 BERT: Pre-training of Deep Bidirectional Transformer for Language Understanding \u7528\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u6df1\u5ea6\u53cc\u5411T\u9884\u8bad\u7ec3\u65b9\u6cd5 BERT Devlin et al., 2018 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer \u4f7f\u7528\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u5230\u6587\u672c\u7684T\u6765\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u7684\u9650\u5236 - Raffel et al., 2019 The Natural Language Decathlon: Multitask Learning as Question Answering \u81ea\u7136\u8bed\u8a00\u5341\u9879\u5168\u80fd\uff1a\u591a\u4efb\u52a1\u5b66\u4e60\u5f53\u4f5c\u95ee\u7b54 MQAN McCann et al., 2018 Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u5668 GPT-2 Radford er al, 2019 Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 Brown et al., 2020 These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 ImageNet: A large-scale hierarchical image database \u5927\u89c4\u6a21\u5206\u5c42\u56fe\u7247\u6570\u636e\u5e93 ImageNet Deng et al., 2009 Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold (\u6d41\u5f62)learning in the weight space of classifiers trained to predict words in captions associated with images. Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al.(2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations. They converted the title, description, and hashtag\uff08\u6807\u7b7e\uff09 metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al.(2017) then extended this approach to predicting phrase n-grams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architecture and pre-training approaches, VirTex(Desai & Johnson,2020), ICMLM(Bulent Sariyildiz et al., 2020), and ConVIRT(Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text. \u8ddf\u8fd9\u7bc7\u6587\u7ae0\u5f88\u76f8\u4f3c Li et al.(2017) \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Image-to-word transformation based on dividing and vector quantizing images with words \u4f9d\u636e\u5355\u8bcd\u5c06\u56fe\u7247\u5206\u5272\u5e76\u5411\u91cf\u5316\uff0c\u6765\u5b9e\u73b0\u56fe\u7247\u5230\u6587\u672c\u7684\u8f6c\u6362 - Mori et al. (1999) Learning Visual Representations using Images with Captions \u5b66\u4e60\u6709\u5b57\u5e55\u56fe\u7247\u7684\u89c6\u89c9\u8868\u5f81 - Quattoni et al. (2007) Multimodal Learning with Deep Boltzmann Machines \u4f7f\u7528\u73bb\u5c14\u5179\u66fc\u673a\u505a\u591a\u6a21\u6001\u5b66\u4e60 - Srivastava & Salakhutdinov (2012) Learning Visual Features from Large Weakly Supervised Data \u4ece\u5927\u89c4\u6a21\u7684\u5f31\u76d1\u7763\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9\u7279\u5f81 - Joulin et al.(2016) YFCC100M: The New Data in Multimedia Research \u7528\u4e8e\u591a\u5a92\u4f53\u7814\u7a76\u7684\u65b0\u6570\u636e\u96c6 YFCC100M (Thomee et al., 2016) ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u505aImageNet\u5206\u7c7b AlexNet (Krizhevsky et al., 2012) Learning Visual N-Grams from Web Data \u4ece\u7f51\u9875\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9n-grams - Li et al.(2017) VirTex: Learning Visual Representations from Textual Annotations \u4ece\u6587\u672c\u6ce8\u91ca\u4e2d\u5b66\u4e60\u89c6\u89c9\u8868\u5f81 VirTex VirTex(Desai & Johnson,2020) Learning Visual Representations with Caption Annotations \u4ece\u5b57\u5e55\u4e2d\u5b66\u4e60\u662f\u89c6\u89c9\u8868\u5f81 ICMLM ICMLM(Bulent Sariyildiz et al., 2020) Contrastive Learning of Medical Visual Representations from Paired Images and Text \u4ece\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\u4e2d\u5b66\u4e60\u533b\u7597\u89c6\u89c9\u8868\u5f81 ConVIRT ConVIRT(Zhang et al., 2020) While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches(\u7531\u4e8e\u6027\u80fd\u4e0d\u9ad8\uff0c\u6240\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u7684\u76d1\u7763\u4fe1\u53f7\u4f7f\u7528\u7684\u56fe\u7247\u4e0a\uff0c\u8fd9\u6837\u7684\u5de5\u4f5c\u8fd8\u662f\u6bd4\u8f83\u5c11\u7684). For example, Li et al.(2017) reach only 15% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accurary of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012).(Li\u662f\u96f6\u6837\u672c\u63a8\u7406\uff0c\u5176\u4ed6\u5de5\u4f5c\u662f\u5728\u6709\u76d1\u7763\u8bad\u7ec3\u540e\u8fdb\u884c\u7684\u8bc4\u4f30) Instead, more narrowly(\u72ed\u9698\u5730) scoped but well-targeted uses of weak supervision have improved performance.(\u76f8\u6bd4\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u5f31\u76d1\u7763\u8bad\u7ec3\u7684\u6548\u679c\u76f8\u5bf9\u66f4\u597d\u4e00\u70b9) Mahajan et al.(2018) showed that predicting ImageNet-related hashtags(\u6807\u7b7e)(\u4e5f\u662f\u4e00\u79cd\u76d1\u7763\u4fe1\u53f7\uff0c\u53ea\u4e0d\u8fc7\u4e0d\u662f\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u4fe1\u53f7) on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-training models increased accuracy by 5% and improved the overall(\u5168\u9762\u7684) state of the art at the time.Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large grains on a broader set of transfer benchmarks by pre-training models to predicts the classes of the noisily labeled JFT-300M dataset(\u8fd9\u4e9b\u810f\u6570\u636e\u867d\u7136\u6ca1\u90a3\u4e48\u51c6\uff0c\u4f46\u4e5f\u662f\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u7684\u5f31\u76d1\u7763\u4fe1\u53f7). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learning Visual N-Grams from Web Data \u4ece\u7f51\u9875\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9n-grams - Li et al.(2017) Self-training with Noisy Student improves ImageNet classification \u4f7f\u7528\u566a\u58f0\u5b66\u751f\u7684\u81ea\u5b66\u4e60\u6765\u63d0\u5347\u56fe\u7247\u5206\u7c7b\u6548\u679c - (Xie et al., 2020) Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) Large Scale Learning of General Visual Representations for Transfer \u5b66\u4e60\u5927\u89c4\u6a21\u901a\u7528\u89c6\u89c9\u8868\u5f81\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60 BiT Kolesnikov et al. (2019) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale transformer\u7528\u4e8e\u56fe\u7247\u5206\u7c7b ViT Dosovitskiy et al. (2020) This line of work represents the current pragmatic(\u52a1\u5b9e\u7684\u3001\u5b9e\u7528\u4e3b\u4e49\u7684) middle ground\uff08\u4e2d\u95f4\u7acb\u573a/\u4e2d\u95f4\u5730\u5e26\uff09between learning from a limited amount of supervised \"gold-labels\"\uff08\u56fa\u5b9a\u6807\u7b7e\u7684\uff09 learning from practically unlimited amounts of raw text. However, it it not without compromises(\u7136\u800c\uff0c\u8fd9\u5e76\u975e\u4e0d\u80fd\u59a5\u534f). Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classfifiers to perform prediction and lack a mechanism for dynamic outputs. This severely(\u4e25\u91cd\u5730) curtails\uff08\u524a\u51cf\u4e86\uff09 their flexibility and limits their \"zero-shot\" capabilities. A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale(\u4e24\u8005\u7684\u533a\u522b\u662f\u89c4\u6a21\u7684\u4e0d\u540c). While Mahajan et al.(2018) and Kolesnikov et al. (2019) trained their models for accelerator\uff08gpu\u6216\u8005tpu\u7b49\u786c\u4ef6\uff0c\u76f8\u6bd4cpu\u662f\u52a0\u901f\u7684\uff09 years on millions to billions of images\uff08\u767e\u4e07\u5230\u5341\u4ebf\u7ea7\u89c4\u6a21\uff09, VirTex, ICMLM, and ConVIRT (\u8fd9\u4e09\u4e2a\u65b9\u6cd5\u76f8\u6bd4\u524d\u9762\u51e0\u4e2a\u65b9\u6cd5\uff0c\u8bad\u7ec3\u7684\u65f6\u95f4\u77ed\uff0c\u4f7f\u7528\u7684\u6570\u636e\u91cf\u5c0f)trained for accelerator days\uff08\u591a\u5c11\u5929\uff09 on one to two hundred thousand images\uff08\u51e0\u5341\u4e07\u89c4\u6a21\uff09. In this work\uff08\u6570\u636e\u89c4\u6a21\u53d8\u5927\uff09, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConvVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models\uff08\u4eceresnet->efficientnet->vit-L\uff09 spanning almost 2 orders of magnitude of compute(\u8ba1\u7b97\u91cf\u63d0\u5347\u4e86100\u500d) and observe that transfer performance is a smoothly predictable function of compute\uff08\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u679c\u548c\u8ba1\u7b97\u91cf\u57fa\u672c\u4e0a\u662f\u6210\u6b63\u76f8\u5173\u7684\uff09 (Hestness et al., 2017; Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find it can be competitive with prior task-specific supervised models. We also confirm(\u786e\u8ba4) these findings with linear-probe(\u9aa8\u5e72\u7f51\u7edc\u51bb\u4f4f\uff0c\u7528\u4f5c\u62bd\u53d6\u7279\u5f81\uff0c\u518d\u52a0\u5206\u7c7b\u5934) representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust\uff08\u66f4\u9c81\u68d2\u7684\uff0c\u5728\u7d20\u63cf\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u4e5f\u633a\u597d\uff09 than equivalent(\u76f8\u7b49\u7684) accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model's capability. (\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u96f6\u6837\u672c CLIP \u6a21\u578b\u6bd4\u540c\u7b49\u7cbe\u5ea6\u7684\u76d1\u7763 ImageNet \u6a21\u578b\u66f4\u7a33\u5065\uff0c\u8fd9\u8868\u660e\u4efb\u52a1\u65e0\u5173\u6a21\u578b\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u66f4\u80fd\u4ee3\u8868\u6a21\u578b\u7684\u80fd\u529b). These results have significant policy(\u653f\u7b56\u7684) and ethical\uff08\u4f26\u7406\u7684\uff09 implications, which we consifer in Section 7. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) \u5f31\u76d1\u7763\u65b9\u6cd5 Large Scale Learning of General Visual Representations for Transfer \u5b66\u4e60\u5927\u89c4\u6a21\u901a\u7528\u89c6\u89c9\u8868\u5f81\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60 BiT Kolesnikov et al. (2019) \u5f31\u76d1\u7763\u65b9\u6cd5 Deep Learning Scaling is Predictable, Empirically \u6df1\u5ea6\u5b66\u4e60\u7684\u6269\u5c55\u60c5\u51b5\u662f\u53ef\u9884\u6d4b\u7684\uff0c\u662f\u6709\u7ecf\u9a8c\u503c\u7684 - Hestness et al., 2017 Scaling Laws for Neural Language Models \u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u89c4\u5219 - Kaplan et al., 2020","title":"\u5f15\u8a00"},{"location":"VisionMultiModal/CLIP/paper_read/#_3","text":"","title":"\u65b9\u6cd5"},{"location":"VisionMultiModal/CLIP/paper_read/#_4","text":"At the core of our approach is the idea of learning perception from supervision contained in natural language. As discussed in the introduction, this is not at all a new idea, however, terminology(\u672f\u8bed\u3001\u7528\u4e8e) used to describe work in this space is varied, even seemingly contradictory(\u77db\u76fe\u7684), and stated motivations are diverse. Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016), and Desai & Johnson (2020) all introduce methods which learn visual representations from text paired with images but describe their approaches as unsupervised, self-supervised, weakly supervised, and supervised respectively. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Contrastive Learning of Medical Visual Representations from Paired Images and Text \u4ece\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\u4e2d\u5b66\u4e60\u533b\u7597\u89c6\u89c9\u8868\u5f81 ConVIRT ConVIRT(Zhang et al., 2020) Self-Supervised Learning of Visual Features through Embedding Images into Text Topic Spaces \u901a\u8fc7\u628a\u56fe\u7247\u5d4c\u5165\u5230\u6587\u672c\u7a7a\u95f4\u4e2d\uff0c\u6765\u7528\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u5b66\u4e60\u89c6\u89c9\u7279\u5f81 - Gomez et al. (2017) Learning Visual Features from Large Weakly Supervised Data \u4ece\u5927\u89c4\u6a21\u7684\u5f31\u76d1\u7763\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9\u7279\u5f81 - Joulin et al.(2016) VirTex: Learning Visual Representations from Textual Annotations \u4ece\u6587\u672c\u6ce8\u91ca\u4e2d\u5b66\u4e60\u89c6\u89c9\u8868\u5f81 VirTex VirTex(Desai & Johnson,2020) We emphasize that what is common across this line of work is not any of the details of th particular methods used but the appreciation(\u6b23\u8d4f) of natural language as a training signal(\u628a\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u4e00\u79cd\u8bad\u7ec3\u4fe1\u53f7). All these approaches are learning from natrual language supervision. Although early work wrestled(\u6454\u8de4) with the complexity of natural language when using topic model and n-gram representations, improvements in deep contextual representation learning\uff08\u5177\u6709\u4e0a\u4e0b\u6587\u8bed\u4e49\u73af\u5883\u7684\u5b66\u4e60\u65b9\u5f0f\uff09 suggest we now have the tools to effectively leverage\uff08\u5229\u7528\uff09 this abundant source of supervision(McCann et al., 2017). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learned in Translation: Contextualized Word Vectors \u4e0a\u4e0b\u6587\u8bcd\u5411\u91cf\uff0c\u5b66\u4e60\u7528\u4e8e\u7ffb\u8bd1 - McCann et al., 2017 Learning from natual language has several potential strengths(\u4f18\u52bf) over other training methods. It's much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification since it does not require annotations to be in a classic \"machine learning compatible(\u517c\u5bb9\u7684) format\" such as the canonical(\u5178\u8303) 1-of-N majority vote \"gold label\". Instead, methods which work on natural language can learn passively(\u88ab\u52a8\u5730) from the supervision contained in the vast amount of text on the internet. Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn't \"just\" learn a representation but also connects that repesentation to language which enables flexible zero-shot transfer. (\u4e0e\u5927\u591a\u6570\u65e0\u76d1\u7763\u6216\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u5b66\u4e60\u8fd8\u6709\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u4e0d\u4ec5\u201c\u53ea\u662f\u201d\u5b66\u4e60\u4e00\u79cd\u8868\u793a\uff0c\u800c\u4e14\u8fd8\u5c06\u8fd9\u79cd\u8868\u793a\u4e0e\u8bed\u8a00\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u5c31\u80fd\u5b9e\u73b0\u7075\u6d3b\u7684\u96f6\u6837\u672c\u8fc1\u79fb) In the following subsections, we detail the specific approach we settled on.","title":"\u81ea\u7136\u8bed\u8a00\u76d1\u7763"},{"location":"VisionMultiModal/CLIP/paper_read/#creating-a-sufficiently-large-dataset","text":"Existing work has mainly used three datasets, MS-COCO(Lin et al., 2014), Visual Genome(Krishna et al., 2017), and YFCC100M(Thomee et al., 2016). While MS-COCO and Visual Genome are high quality crowd-labeled datasets, they are small by modern standards with approximately 100,000 training photos each. By comparison, other computer vision systems are trained on up to 3.5 billion Instagram photos(Mahajan et al., 2018). YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality(\u7a00\u758f\u4e14\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684). Many images use automatically generated filenames like 20160716_113957.JPG as \"titles\" or contain \"descriptions\" of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos. This is approximately the same size as ImageNet. A major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet. Since existing datasets do not adequately(\u5145\u5206\u5730) reflect this possibility, considering results only on them would underestimate(\u4f4e\u4f30) the potential of this line of research.(\u73b0\u6709\u7684\u6570\u636e\u96c6\u91cf\u7ea7\u6bd4\u8f83\u5c0f\uff0c\u56e0\u6b64\u4e0d\u80fd\u5b8c\u5168\u53cd\u6620\u51fa\u7528\u6d77\u91cf\u4e92\u8054\u7f51\u7684\u81ea\u7136\u8bed\u8a00\u6570\u636e\u505a\u76d1\u7763\u80fd\u505a\u6210\u4e8b\u60c5\u7684\u53ef\u80fd\u6027\uff0c\u4ec5\u4ec5\u4f7f\u7528\u8fd9\u6837\u7684\u6570\u636e\u96c6\uff0c\u53ef\u80fd\u4f1a\u4f4e\u4f30\u8fd9\u65b9\u9762\u7814\u7a76\u7684\u6f5c\u529b). To address this, we constructed a new dataset of 400 million (image,text) pairs collected form a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process(\u6784\u5efa\u6570\u636e\u8fc7\u7a0b\u4e2d) whose text includes one of a set of 500,000 queries. We approximately class balance the results by including up to 20,000(image,text) pairs per query. The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2. We refer to this dataset as WIT for WebImageText. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Microsoft COCO: Common Objects in Context \u4e0a\u4e0b\u6587\u4e2d\u7684\u5e38\u89c1\u7269\u4f53 COCO Lin et al., 2014 Visual Genome: Connecting Language and Vision Using Crowd sourced Dense Image Annotations \u89c6\u89c9\u57fa\u56e0\u7ec4: \u4f7f\u7528\u4f17\u5305\u7684\u5bc6\u96c6\u56fe\u50cf\u6807\u6ce8\u6765\u8fde\u63a5\u8bed\u8a00\u548c\u89c6\u89c9 - Krishna et al., 2017 YFCC100M: The New Data in Multimedia Research \u591a\u5a92\u4f53\u7814\u7a76\u4e2d\u7684\u65b0\u6570\u636e\u96c6 YFCC100M Thomee et al., 2016 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018)","title":"Creating a Sufficiently Large Dataset"},{"location":"VisionMultiModal/CLIP/paper_read/#23-selecting-an-efficient-pre-training-method","text":"State-of-the-art computer vision systems use very large amounts of compute. Mahajan et al.(2018) required 19 GPU years to train their ResNeXt101-32x48d and Xie et al.(2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2. When considering that both these systems were trained to predict only 1000 ImageNet classes, the task of learning an open set of visual concepts from natural language seems daunting(\u4ee4\u4eba\u751f\u754f). In the course of our efforts, we found training efficiency(\u8bad\u7ec3\u6548\u7387) was key to successfully scaling natural language supervision(\u6210\u529f\u6269\u5c55\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u7684\u5173\u952e) and we selected our final pre-training method based on this metric. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) Self-training with Noisy Student improves ImageNet classification \u4f7f\u7528\u566a\u58f0\u5b66\u751f\u7684\u81ea\u5b66\u4e60\u6765\u63d0\u5347\u56fe\u7247\u5206\u7c7b\u6548\u679c - (Xie et al., 2020) Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image. However, we encountered difficulties efficiently scaling this method. In Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet-50 image encoder, learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of-words encoding of the same text. Both these approaches share a key similarity. They try to predict the exact words of the text accompanying each image. This is a difficult task due to the wide variety of descriptions, comments, and related text that co-occur with images. Recent work in contrastive representation learning for images has found that contrastive objectives can learn better representations than their equivalent predictive objective (Tian et al., 2019). Other work has found that although generative models of images can learn high quality image representations, they require over an order of magnitude (\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7) of more compute than contrastive models with the same performance(Chen et al., 2020a). Noting these findings, we explored training a system to solve the potentially easier proxy task of predicting only which text as a whole(\u6574\u4e2a\u6587\u672c) is paired with which image and not the exact words(\u786e\u5207\u7684\u8bcd) of that text. Starting with the same bag-of-words encoding baseline, we swapped the predictive objective\uff08\u9884\u6d4b\u6027\u7684\u4efb\u52a1\uff09 for a contrastive objective\uff08\u5bf9\u6bd4\u6027\u7684\u7684\u4efb\u52a1\uff09 in Figure 2 and observed a further 4x efficientcy improvement in the rate of zero-shot transfer to ImageNet. \u53bb\u9884\u6d4b\u56fe\u7247\u5bf9\u5e94\u7684\u6587\u672c\u4e2d\u7684\u5355\u4e2a\u8bcd -> \u53bb\u9884\u6d4b\u56fe\u7247\u5bf9\u5e94\u7684\u6587\u672c\u4e2d\u7684\u5355\u4e2a\u8bcd\u7684\u8868\u5f81\uff08\u6548\u679c\u63d0\u53473\u500d\uff09 -> \u53bb\u9884\u6d4b\u56fe\u7247\u4e0e\u6587\u672c\u662f\u5426\u5bf9\u5e94\uff08\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u53474\u500d\uff09 \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Contrastive Multiview Coding \u5bf9\u6bd4\u7684\u591a\u89d2\u5ea6\u7f16\u7801 - Tian et al., 2019 Generative Pretraining From Pixels \u4ece\u50cf\u7d20\u505a\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3 - Chen et al., 2020a \u56fe2 CLIP is much more efficient at zero-shot transfer than our image caption baseline. Althougn highly expressive(\u5bcc\u6709\u8868\u73b0\u529b), we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words(BoW) encoding of the text(Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x. Given a batch of N(image, text)pairs, CLIP is trained to predict which of the NxN possible(image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizeing the cosine similarity of the embeddings of the N^2 - N incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores. In figure 3, we include pseudocode of the core of an implementation of CLIP. To our knowledge this batch construction technique and objectives was first introduced in area of deep metric learning as the multi-class N-pair loss Sohn(2016) was popularized for contrastive representation learning by Oord et al.(2018) as the InfoNCE loss, and was recently adapted for contrastive(text, image) representation learning in the domain of medical imaging by Zhang et al.(2020). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Improved deep metric learning with multi-class N-pair loss objective \u4f7f\u7528\u591a\u5206\u7c7b\u7684\u591a\u5bf9\u7684\u635f\u5931\u51fd\u6570\u6765\u63d0\u5347\u6df1\u5ea6\u8bc4\u4f30\u5b66\u4e60\u80fd\u529b - 2016-12-05 Representation Learning with Contrastive Predictive Coding \u4f7f\u7528\u5bf9\u6bd4\u9884\u6d4b\u7f16\u7801\u6765\u8fdb\u884c\u8868\u5f81\u5b66\u4e60 CPC Oord et al.(2018) Contrastive Learning of Medical Visual Representations form Paired Images and Text \u4f7f\u7528\u6210\u5bf9\u7684\u56fe\u7247\u6587\u672c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7528\u4e8e\u533b\u7597\u89c6\u89c9\u8868\u5f81 Zhang et al.(2020) # image_encoder - ResNet of Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) # [n, d_i] T_f = text_encoder(T) # [n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t) / 2 \u56fe3 Numpy-like pseudocode for the core of an implementation of CLIP Due to the large size of our pre-training dataset, over-fitting is not a major concern and the details of training CLIP are simplified compared to the implementation of Zhang et al.(2020). We train CLIP form scratch without initiallizeing the image encoder with ImageNet weights or the text encoder with pre-trained weights. We do not use the non-linear projection between the representation and the contrastive embedding space, a change which was introduced by Bachman et al.(2019) and popularized by Chen et al.(2020b). We instead use only a linear projection to map from each encoder's representation to the multi-modal embedding space. We did not notice a difference in training two versions and speculate(\u63a8\u6d4b) that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods. We also remove the text transformation function {t_u} from Zhang et al.(2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP's pre-training dataset are only a single sentence. We also simplify the image transformation function $t_v$. A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, $\\tau$, is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Contrastive Learning of Medical Visual Representations form Paired Images and Text \u4f7f\u7528\u6210\u5bf9\u7684\u56fe\u7247\u6587\u672c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7528\u4e8e\u533b\u7597\u89c6\u89c9\u8868\u5f81 Zhang et al.(2020) A Simple Framework for Contrastive Learning of Visual Representations \u4e00\u4e2a\u7b80\u5355\u7684\u6846\u67b6\u7528\u4e8e\u89c6\u89c9\u8868\u5f81\u7684\u5bf9\u6bd4\u5b66\u4e60 SimCLR Chen et al.(2020b)","title":"2.3 Selecting an Efficient Pre-Training Method"},{"location":"VisionMultiModal/CLIP/paper_read/#choosing-and-scaling-a-model","text":"We consider two different architectures for the image encoder. For the first, we use ResNet-50 as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNet-D improvements from He et al.(2019) and the antialiased(\u6297\u952f\u9f7f) rect-2 blur(\u6a21\u7cca) pooling from Zhang(2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of \"transformer-style\" multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer(ViT)(Dosovitskiy et al.,2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme(\u65b9\u6848). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Bag of Tricks from Image Classification with Convolutional Neural Networks \u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b\u7684\u4e00\u5806\u6280\u5de7 - He et al.(2019) Making Convolutional Networks Shift-Invariant Again \u4f7f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u66f4\u52a0\u5c3a\u5ea6\u4e0d\u53d8 - Zhang(2019) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale transformer\u7528\u4e8e\u56fe\u7247\u5206\u7c7b ViT Dosovitskiy et al. (2020) The text encoder is a Transformer(Vaswani et al.,2017) with the architecture modifications described in Radford et al.(2019). As a base size we use a 63M-parameter 12 layer 512-wide model with 8 attention heads. The transformer operates on lower-cased byte pair encoding(BPE) representation of the text with a 49,152 vocab size(Sennrich et al.,2015). For computational efficiency, the max sequence length was capped(\u4e0a\u9650) at 76. The text sequence is bracketed with(\u7528\u62ec\u53f7\u62ec\u8d77\u6765) [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary(\u8f85\u52a9) objective, though exploration of this is left as future work. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 ViT Vaswani et al.,2017 Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u5668 GPT-2 Radford er al, 2019 While previous computer vision research has often scaled models by increasing the width(Mahajan et al., 2018) or depth(He et al., 2016a) in isolation(\u9694\u79bb\u7684\uff0c\u5355\u72ec\u7684), for the ResNet image encoders we adapt the approach of Tan & Le(2019) which found that allocating(\u5206\u914d) additional compute across all of width, depth, and resolution outperforms only allocating(\u5206\u914d) it to only one dimension of the model. While Tan & Le(2019) tune the ratio of compute allocated to each dimension for their EfficientNet architecture, we use a simple baseline of allocating additional compute equally to increasing the width, depth, and resolution of the model. For the text encoder, we only scale the width of the model to be proportional(\u6210\u6bd4\u4f8b\u7684) to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP's performance to be less sensitive to the capacity of the text encoder. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining \u63a2\u7d22\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u9650\u5236 - Mahajan et al.(2018) Deep Residual Learning for Image Recognition \u4f7f\u7528\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u6765\u505a\u89c6\u9891\u5206\u7c7b\u4efb\u52a1 ResNet He et al., 2016a EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks EfficientNet: \u91cd\u65b0\u601d\u8003\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6269\u5c55\u95ee\u9898 EfficientNet Tan & Le(2019)","title":"Choosing and Scaling a Model"},{"location":"VisionMultiModal/CLIP/paper_read/#25-training","text":"We train a series of 5 ResNets and 3 Vision Transformers. For the ResNets we train a ResNet-50, a ResNet-101, and then 3 more which follow EfficientNet-style model scaling and use approximately 4x, 16x, 64x the compute of a ResNet-50. They are denoted as RN50x4, RN50x16, and RN50x64 respectively. For the Vision Transformers we train a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all models for 32 epochs. We use the Adam optimizer(Kingma & Ba, 2014) with decoupled(\u89e3\u8026\u7684) weight decay regularization (Loshchilov & Hutter, 2017) applied to all weights that are not gains or biases, and decay the learning rate unsing a cosine schedule(Loshchilov & Hutter, 2016). Initial hyper-parameters were set using a comnination of grid searches, random search, and manual tuning on the baseline ResNet-50 model when trained for 1 epoach. Hyper-parameters were then adapted heuristically(\u542f\u53d1\u5f0f\u7684) for larger models due to computational constraints. The learnable temperature parameter $\\tau$ was initialized to the equivalent of 0.07 from (Wu et al., 2018). and clipped(\u526a\u5207) to prevent scaling the logits by more than 100 which we found necessary to prevent(\u9632\u6b62) training instability\uff08\u4e0d\u7a33\u5b9a\uff09. We use a very large minibatch size of 32,768. Mixed-precision(Micikevicius et al., 2017) was used to accelerate training and save memory. To save additional memory, gradient checkpointing(\u68af\u5ea6\u68c0\u67e5\u70b9) (Griewank & Walther, 2000; Chen et al.,2016), half-precision Adam statistics(Dhariwal et al.,2020), and half-precision stochastically rounded(\u534a\u7cbe\u5ea6\u968f\u673a\u56db\u820d\u4e94\u5165\u7684) text encoder weights were used. The calculation(\u8ba1\u7b97) of embedding similarities was also sharded with individual GPUs computing only the subset of the pairwise similarities necessary for their local batch of embeddings. The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs. For the ViT-L/14 we also pre-train at a higher 336 pixel resolution for one additional epoch to boost performance similar to FixRes(Touvron et al.,2019). We denote this model as ViT-L/14@336px. Unless otherwise specified(\u9664\u975e\u53e6\u6709\u89c4\u5b9a), all results reported in this as \"CLIP\" use this model which we found to perform best. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Adam: A Method for Stochastic(\u968f\u673a) Optimization \u968f\u673a\u4f18\u5316\u7684\u4e00\u79cd\u65b9\u6cd5 Adam Kingma & Ba, 2014 Decoupled Weight Decay Regularization \u89e3\u8026\u7684\u6743\u91cd\u8870\u51cf\u6b63\u5219\u5316 - Loshchilov & Hutter, 2017 SGDR: Stochastic Gradient Descent with Warm Restarts \u4f7f\u7528\u70ed\u542f\u52a8\u7684\u65b9\u5f0f\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f8\u7ed3\u5408 SGDR Loshchilov & Hutter, 2016 Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination \u901a\u8fc7\u65e0\u53c2\u6570\u5b9e\u4f8b\u7ea7\u522b\u5224\u522b\u7684\u65e0\u76d1\u7763\u7279\u5f81\u5b66\u4e60 - Wu et al., 2018 Mixed Precision Training \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - Micikevicius et al., 2017 Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of cumputational differentiation \u8ba1\u7b97\u5fae\u5206\u7684\u53cd\u5411\u6216\u4f34\u968f\u6a21\u5f0f\u7684\u68c0\u67e5\u70b9\u5b9e\u73b0 - Griewank & Walther, 2000 Training Deep Nets with Sublinear Memory Cost. \u4f7f\u7528\u6b21\u7ebf\u6027\u5185\u5b58\u6210\u672c\u8bad\u7ec3\u6df1\u5ea6\u7f51\u7edc - Chen et al.,2016 Jukebox: A Generative Model for Music \u7528\u4e8e\u97f3\u4e50\u7684\u4e00\u4e2a\u751f\u6210\u5f0f\u6a21\u578b - Dhariwal et al.,2020 Fixing the train-test resolution dicrepancy \u4fee\u590d\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u8fa8\u7387\u5dee\u5f02 - Touvron et al.,2019","title":"2.5 Training"},{"location":"VisionMultiModal/CLIP/paper_read/#3","text":"","title":"3. \u5b9e\u9a8c"},{"location":"VisionMultiModal/CLIP/paper_read/#31","text":"","title":"3.1 \u96f6\u6837\u672c\u8fc1\u79fb"},{"location":"VisionMultiModal/CLIP/paper_read/#311-motivation","text":"In computer vision, zero-shot learning usually refers to the study of generalizing to unseen object categories in image classification(Lampert et al.,2009). We instead use the term in a broader sense and study generalization to unseen datasets. We motivate this as a proxy for performing unseen tasks, as aspired to in the zero-data learning paper of Larochelle et al.(2008). While much research in the filed of unsupervised learning focuses on the representation learning capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the task-learning capabilities of machine learning systems. In this view, a dataset evaluates performance on a task on a specific distribution. However, many popular computer vision datasets were created by the research community primarily as benchmarks to guide the development of generic image classification methods rather than measuring performance on a specific task. While it is reasonable to say that(\u53ef\u4ee5\u8fd9\u4e48\u8bf4) the SVHN dataset measures the task of street number transciption on the distribution of Google Street View photos, it is unclear what \"real\" task the CIFAR-10 dataset measures. It is clear, however, what distribution CIFAR-10 is drawn from - TinyImages(Torralba et al.,2008). On these kinds of datasets, zero-shot transfer is more an evaluation of CLIP's robustness to distribution shift and domain generalization rather tahn task generalization. Please Section 3.3 for analysis focused on this. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer \u901a\u8fc7\u7c7b\u4e4b\u95f4\u5c5e\u6027\u8fc1\u79fb\uff0c\u6765\u5b66\u4f1a\u68c0\u6d4b\u6ca1\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b - Lampert et al.,2009 Zero-data learning of new tasks \u65b0\u4efb\u52a1\u7684\u96f6\u6837\u672c\u5b66\u4e60 - Larochelle et al.(2008) To our knowledge, Visual N-Grams(Li et al.,2017) first studied zero-shot transfer to existing image classification datasets in the manner described above. It is also the only other work we are aware of that has studied zero-shot transfer to standard image classification datasets using a generically pre-trained model and serves as(\u5145\u5f53) the best reference point for contextualizing CLIP. Their approach learns the parameters of a dictionary of 142806 visual n-grams(spaning 1- to 5- grams) and optimizes these n-grams using a differential version of Jelinek-Mercer smoothing to maximize the probability of all text n-grams for a given image. In order to perform zero-shot transfer, they first convert the text of each of the dataset's class names into its n-gram representation and then compute its probability according to their model, predicting the one with the highest score. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Learning Visual N-Grams from Web Data \u4ece\u7f51\u9875\u6570\u636e\u4e2d\u5b66\u4e60\u89c6\u89c9N-Grams - Li et al.,2017 \u63d2\u5165\u4e00\u70b9\u5185\u5bb9 Learning Visual N-Grams from Web Data Real-world image recognition systems need to recognize tens of thousands of classes that constitute(\u6784\u6210) a plethora of (\u8bb8\u591a\u7684) visual concepts. The traditional approach of annotating thousands of images per class for training is infeasible(\u4e0d\u53ef\u884c\u7684) in such a scenario(\u8fd9\u79cd\u60c5\u51b5\u4e0b), prompting the use of webly supervised data. This paper explores the training of image-recognition systems on large numbers of images and associated user comments. In particular, we develop visual n-gram models that can predict arbitrary phrases that are relevant to the content of an image. Our visual n-gram models are feed-forward convolutional networks trained using new loss functions that are inspired by n-gram models commonly used in language modeling. We demonstrate the merits of our models in phrase prediction, phrase-based image retrieval, relating images and captions, and zero-shot transfer. Our focus on studying zero-shot transfer as an evaluation of task learning is inspired by work demonstrating task learning in the field of NLP. To our knowledge Liu et al.(2018) first identified task learning as an \"unexpected side-effect\"(\u4e0d\u671f\u671b\u7684\u526f\u4f5c\u7528) when a language model trained to generate Wikipedia articles learned to reliably transliterate(\u97f3\u8bd1) names between languages. While GPT-1(Radford et al.,2018) focused on pre-training as a transfer learning method to improve supervised fine-tuning, , it also included an ablation study demonstrating that the performance of four heuristic(\u542f\u53d1\u5f0f\u7684) zero-shot transfer methods improved steadily over the course of pre-training(\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7a33\u5b9a\u5730\u6539\u5584), without any supervised adaption. This analysis served as the basis for GPT-2(Radford et al.,2019) which focused exclusively(\u4ec5) on studying the task-learning capabilities of language models via zero-shot transfer. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Generating Wikipedia by Summarizing Long Sequences \u901a\u8fc7\u5f52\u603b\u957f\u5e8f\u5217\u6765\u751f\u6210wiki - Liu et al.(2018)","title":"3.1.1 Motivation"},{"location":"VisionMultiModal/CLIP/paper_read/#312-using-clip-for-zero-shot-transfer","text":"CLIP is pre-trained to predict if an image and a text snippet(\u7247\u6bb5) are paired together in its dataset. To perform zero-shot classification, we reuse this capability. For each dataset, we use the names of all the classes in the dataset as the set of potential text pairings and predict the most probable(\u53ef\u80fd\u7684)(image, text) pair according to CLIP. In a bit more detail, we first compute the feature embedding of the image and the feature embedding of the set of possible texts by their respective(\u5404\u81ea\u7684) encoders. The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter $\\tau$, and normalized into a probability distribution via a softmax. Note that this prediction layer is a multinormial(\u591a\u9879\u7684) logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling. When interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork(Ha et al.,2016) which generates the weights of a linear classifier based on the text specifying the visual concepts that the classes represent. Lei Ba et al.(2015) first introduced a zero-shot image classifier of this form while the idea of generating a classifier from natual language dates back to at least Elhoseiny et al.(2013). Continuing with this interpretation, every step of CLIP pre-training can be viewed as optimizeing the performance of a randomly created proxy to a computer vision dataset which contains 1 example per class and has 32768 total classes defined via natural language descriptions. For zero-shot evaluation, we cache the zero-shot classifier once it has been computed by the text encoder and reuse it for all subsequent predictions. This allows the cost(\u6210\u672c) of generating it to be amortized(\u5206\u644a) across all the predictions in a dataset. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 HyperNetworks \u8d85\u7f51\u7edc - Ha et al.,2016 Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions \u4f7f\u7528\u6587\u672c\u6027\u7684\u63cf\u8ff0\u6765\u9884\u6d4b\u6df1\u5ea6\u96f6\u6837\u672c\u5377\u79ef\u7f51\u7edc - Lei Ba et al.(2015) Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions \u4f7f\u7528\u7eaf\u6587\u672c\u63cf\u8ff0\u6765\u505a\u96f6\u6837\u672c\u5b66\u4e60 - Elhoseiny et al.(2013)","title":"3.1.2 Using CLIP for Zero-shot Transfer"},{"location":"VisionMultiModal/CLIP/paper_read/#313-initial-comparison-to-visual-n-grams","text":"In Table 1 we compare Visual N-Grams to CLIP. The best CLIP model improves accuracy on ImageNet from a proof of concept 11.5% to 76.2% and matches the performance of the original ResNet-50 despite using none of the 1.28 million crowd-labeled training examples available for this dataset. Additionally, the top-5 accuracy of CLIP models are noticeably higher than their top-1, and this model has a 95% top-5 accuracy, matching Inception-V4(Szegedy et al.,2016). The ability to match the performance of a strong, fully supervised baselines in a zero-shot setting suggests CLIP is a significant step towards flexible and practical zero-shot computer vision classifiers. As mentioned above, the comparison to Visual N-Grams is meant for contextualizing the performance of CLIP and should not be interpreted as a direct methods comparison between CLIP and Visual N-Grams as many performance relevant differences between the two systems were not controlled for. For instance, we train on a dataset that is 10x larger, use a vision model that requires nearly 100x more compute per prediction, likely used over 1000x their training compute, and use a transformer-based model which did not exist when Visual N-Grams was published. As a closer comparison, we trained a CLIP ResNet-50 on the same YFCC100M dataset that Visual N-Grams was trained on and found it matched their reported ImageNet performance within a V100 GPU day. This baseline was also trained from scatch instead of being initiallized from pre-trained ImageNet weights as in Visual N-Grams. - aYahoo ImageNet SUN Visual N-Grams 72.4 11.5 23.0 CLIP 98.4 76.2 58.5 Table 1. Comparing CLIP to prior zero-shot transfer image classification results. CLIP improves performance on all three datasets by a large amount. This improvement reflects(\u53cd\u6620\u4e86) many differences in the 4 years since the development of Visual N-Grams(Li et al.,2017). CLIP also outperforms Visual N-Grams on the other 2 reported datasets. On a Yahoo, CLIP achieves a 95% reduction(\u51cf\u5c11) in the number of errors, and on SUN, CLIP more than doubles the accuracy of Visual N-Grams. To conduct a more comprehensive analysis and stress test, we implement a much larger evaluation suite detailed in Appendix A. In total we expand from the 3 datasets reported in Visual N-Grams to include over 30 datasets and compare to over 50 existing computer vision systems to contextualize results.","title":"3.1.3 Initial Comparison to Visual N-Grams"},{"location":"VisionMultiModal/CLIP/paper_read/#314-prompt-engineering-and-ensembling","text":"Most standard image classification datasets treat the information naming or describing classes which enables narural language based(\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00) zero-shot transfer as an afterthought(\u4e8b\u540e\u60f3\u6cd5). The vast majority of datasets annotate images with just a numeric id of the label and contain a file mapping these ids back to their names in English. Some datasets, such as Flowers102 and GTSRB, don't appear to include this mapping at all in their released versions preventing(\u9632\u6b62) zero-shot transfer entirely. For many datasets, we observed these labels may be chosen somewhat(\u6709\u4e9b) haphazardly(\u968f\u610f) and do not anticipate(\u9884\u6d4b) issues related to zero-shot transfer which relies on task description in order to transfer successfully. A common issue is polysemy(\u6b67\u4e49\u6027). When the name of a class is the only information provided to CLIP's text encoder it is unable to differentiate which word sense is meant due to the lack of context. In some cases multiple meanings of the same word might be included as different classes in the same dataset! This happens in ImageNet which contains both construction cranes(\u5efa\u7b51\u8d77\u91cd\u673a) and cranes that fly.(\u98de\u7684\u9e64). Another example is found in classes of the Oxford-IIIT Pet dataset where the word boxer(\u62f3\u5e08\u72ac\u3001\u62f3\u51fb\u624b) is, from context, clearly referring to a breed of dog, but to a text encoder lacking context could just as likely refer to a type of athlete(\u8fd0\u52a8\u5458). Another issue we encountered is that it's relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template \"A photo of a {label}.\" to be a good default that helps the text is about the content of the image. This often improves performance over the baseline of using only the label text. For instance, just using this prompt improves accuracy on ImageNet by 1.3%. Similar to the \"prompt engineering\" discussion around GPT-3(Brown et al.,2020; Gao et al.,2020), we have also observed that zero-shot performance can be significantly improved by customizing(\u5b9a\u5236) the prompt text to each task. A few, non exhaustive, examples follow(\u4ee5\u4e0b\u662f\u4e00\u4e9b\u975e\u8be6\u5c3d\u7684\u793a\u4f8b). We found that on several fine-grained image classification datasets that it helped to specify(\u6307\u5b9a) the category. For example on Oxford-IIIT Pets, using \"A photo of a {label}, a type of pet.\" to help provide context worked well. Likewise, on Food101 specifying a type of food and on FGVC Aircraft a type of aircraft helped too. For OCR datasets, we found that putting quotes(\u5f15\u53f7)around the text or number to be recognized improved performance. Finally, we found that on satellite image classification datasets it helped to specify that the images were of this form and we use variants of \"a satellite phot of a {label}.\" We also experimented with ensembling over multiple zero-shot classifiers as another way of improving performance.These classifiers are computed by using different context prompts such as \"A photo of a big {label}\" and \"A photo of a small {label}\". We construct the ensemble over the embedding space instead of probability space. This allows us to cache a single set of averaged text embeddings so that the compute cost of the emsemble is the same as using a single classifier when amortized(\u5206\u644a) over many predictions. We've observed ensembling across many generated zero-shot classifiers to reliably improve performance and use it for the majority of datasets. On ImageNet, we ensemble 80 different context prompts and this improves performance by an additional 3.5% over the single default prompt discussed above. When considerd together, prompt engineering and ensembling improve ImageNet accuracy by almost 5%. In Figure 4 we visualize how prompt engineering and ensembling change the performance of a set of CLIP models compared to the contextless baseline approach of directly embedding the class name as done in Li et al.(2017). Figure4. Prompt engineering and ensembling improve zero-shot performance, Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is \"free\" when amortized over many predictions. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 Brown et al., 2020 Making Pre-trained Language Models Better Few-shot Learners. \u4f7f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6bd4\u5c11\u6837\u672c\u5b66\u4e60\u5668\u66f4\u597d - Gao et al.,2020","title":"3.1.4 Prompt Engineering And Ensembling"},{"location":"VisionMultiModal/CLIP/paper_read/#315-analysis-of-zero-shot-clip-performance","text":"Since task-agnostic zero-shot classifiers for computer vision have been understudied, CLIP provides a promising opportunity to gain a better understanding of this type of model. In this section, we conduct a study of various properties of CLIP's zero-shot classifiers. As a first question, we look simply at how well zero-shot classifiers perform. To contextualize this, we compare to the performance of a simple off-the-shelf(\u73b0\u6210\u7684) baseline: fitting a fully supervised, regularized, logistic regression classifier on the features of the canonical(\u7ecf\u5178\u7684) ResNet-50. In Figure 5 we show this comparison across 27 datasets. Please see Apprendix A for details of datasets and setup. Figure 5. Zero-shot CLIP is competitive with a fully supervised baseline. Across a 27 dataset eval suite, a zero-shot CLIP classifier outperforms a fully supervised linear classifier fitted on ResNet-50 features on 16 datasets, including ImageNet. Zero-shot CLIP outperforms this baseline slightly more often than not and wins on 16 of the 27 datasets. Looking at individual datasets reveals(\u63ed\u793a\u4e86) some interesting behavior. On fine-grained classification tasks, we observe a wide spread in performance. On two of these datasets, Stanford Cars and Food101, zero-shot CLIP outperforms logistic regression on ResNet-50 features by over 20% while on two others, Flowers102 and FGVCAircraft, zero-shot CLIP underperforms by over 10%. On OxfordPets and Birdsnap, performance is much closer. We suspect these difference are primarily due to varying amounts of per-task supervision between WIT and ImageNet. On \"general\" object classification datasets such as ImageNet, CIFAR10/100,STL10, and PascalVOC2007 performance is relatively similar with a slight advantage for zero-shot CLIP in all cases. On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. Zero-shot CLIP significantly outperforms a ResNet-50 on two datasets measuting action recognition in videos. On Kinetics700, CLIP outperforma a ResNet-50 by 14.5%. Zero-shot CLIP also outperforms a ResNet-50's features by 7.7% on UCF1010. We speculate this is due to natural language providing wider supervision for visual concepts involving verbs, compared to the noun-centric object supervision in ImageNet. Looking at where zero-shot CLIP notably(\u5c24\u5176) underperforms, we see that zero-shot CLIP is quite weak on several specialized, complex, or abstract tasks such as satellite image classification (EuroSAT and RESISC45), lymph node tumor detection (PatchCamelyon), counting objects in synthetic scenes(CLEVRCounts), self-driving related tasks such as German traffic sign recognition(GTSRB), recognizing distance to the nearest car (KITTI Distance). These results highlight the poor capability of zero-shot CLIP on more complex tasks. By contrast, non-expert humans can robustly perform several of these tasks, such as counting, satellite image classification, and traffic sign recognition, suggesting significant room for improvement. However, we caution(\u8b66\u544a) that it is unclear whether measuring zero-shot transfer, as opposed to few-shot transfer, is a meaningful evaluation for difficult tasks that a learner has no prior experience with, such as lymph node tumor classification for almost all humans(and possibly CLIP). While comparing zero-shot performance to fully supervised models contextualizes the task-learning capabilities of CLIP, comparing to few-shot methods if a more direct comparison, since zero-shot is its limit. In Figure 6, we visualize how zero-shot CLIP compares to few-shot logistic regression on the features of many image models including the best publicly available ImageNet models, self-supervised learning methods, and CLIP itself. While it is intuitive(\u76f4\u89c9\u7684) to expect zero-shot to underperform one-shot, we instead find that zero-shot CLIP matches the performance of 4-shot logistic regression on the same feature space. This is likely due to an important difference between the zero-shot and few-shot approach. First, CLIP's zero-shot classifier is generated via natural language which allows for visual concepts to be directly specified (\"communicated\"). By contrast, \"normal\" supervised learning must infer concepts indirectly form training examples. Context-less example-based learning has the drawback(\u7f3a\u70b9) that many different hypotheses can be consistent with the data, especially in the one-shot case. A single image often contains many different visual concepts. Although a capable learner is able to exploit visual cues and heuristics, such as assuming that the concept being demonstrated is the primary object in an image, there is no guarantee. Figure 6. Zero-shot CLIP outperforms few-shot linear probes. Zero-shot CLIP matches the average performance of a 4-shot linear classifier trained on the same feature space and nearly matches the best results of a 16-shot linear across classifier publicly available models. For both BiT-M and SimCLRv2, the best performing model is highlighted. Light gray lines are other models in the eval suite. The 20 datasets with at least 16 examples per class were used in this analysis. A potential resolution(\u89e3\u51b3\u65b9\u6848) of this discrepancy(\u5dee\u5f02) between zero-shot and few-shot performance is to use CLIP's zero-shot classfifier as a prior for the weights of the few-shot classifier. While adding an L2 penlty towards the generated weights is a straightforward implementation of this idea, we found that hyperparameter optimization would often select for such a large value of this regularizer that resulting few-shot classifier was \"just\" the zero-shot classifier. Research into better methods of combing the strength of zero-shot transfer with flexibility of few-shot learning is a promising direction for future work. \u8be5\u6bb5\u7684\u542b\u4e49\u4e0d\u592a\u61c2\u3002 When comparing zero-shot CLIP to few-shot logistic regression on the features of other models, zero-shot CLIP roughtly(\u5927\u81f4) matches the performance of the best performing 16-shot classifier in out evaluation suite, which uses the features of a BiT-M ResNet-152x2 trained on ImageNet-21K. We are certain that a BiT-L model trained on JFT-300M would perform even better but these models have not been publicly released. That a BiT-M ResNet-152x2 performs best in a 16-shot setting is somewhat surprising since, as analyzed in Section 3.2, the Noisy Student EfficientNet-L2 outperforms it in a fully supervised setting by almost 5% on average across 27 datasets. In addition to studying the average performance of zero-shot CLIP and few-shot logistic regression, we also examine performance on individual datasets. In Figure 7, we show estimates for the number of labeled examples per class that a logistic regression classifier on the same feature space requires to match the performance of zero-shot CLIP. Since zero-shot CLIP is also a linear classifier, this estimates the effective data efficiency of zero-shot transfer in this setting. In order to avoid training thousands of linear classifiers, we estimate the effective data efficiency based on a log-linear interpolation of the performance of a 1,2,4,6,8,16-shot(when possible), and a fully supervised linear classifier trained on each dataset. We find that zero-shot transfer can have widely varying efficiency per dataset from less than 1 labeled example per class to 184. Two datasets, Flowers102 and EuroSAT underperform one-shot models. Half of the datasets require less than 5 examples per class with a median of 5.4. However, the mean estimated data efficiency is 20.8 examples per class. This is due to the 20% of datasets where supervised classifiers require many labeled examples per class in order to match performance . On ImageNet, zero-shot CLIP matches the performance of a 16-shot linear classifier trained on the same feature space. \u56fe7 Figure 7. The data efficiency of zero-shot transfer varies widely. Calculating the number of labeled examples per class a linear classifier on the same CLIP feature space requires to match the performance of the zeot-shot classifier contextualizes the effectiveness of zero-shot transfer. Values are estimated based on log-linear interpolation(\u63d2\u503c) of 1,2,4,8,16-shot and fully supervised results. Performance varies widely from still underperforming a ont-shot classifier on two datasets to matching an estimated 184 labeled examples per class. If we assume(\u5047\u5b9a) that evaluation(\u8bc4\u4f30) datasets are large enough that the parameters of linear classifiers trained on them are well estimated(\u4f30\u7b97), then because CLIP's zero-shot classifier is also a linear classifier, the performance of the fully supervised classifiers roughly sets an upper bound(\u4e0a\u9650) for what zero-shot transfer can achieve. In Figure 8 we compare CLIP's zero-shot performance with fully supervised linear classifiers across datastes. The dashed, y=x line represents an \"optimal\" zero-shot classifier that matches the performance of its fully supervised equivalent. For most datasets, the performance of zero-shot classifiers still underperform fully supervised classifiers by 10% to 25%, suggesting that there is still plenty(\u8db3\u591f) of headroom for improving CLIP's task-learning and zero-shot transfer capabilities. \u56fe8 Figure 8. Zero-shot performance is correlated with linear probe performance but still mostly sub-optimal. Comparing zero-shot and linear probe performance across datasets shows a strong correlation with zero-shot performance mostly shifted 10 to 25 points lower. On only 5 datasets does zero-shot performance approach linear probe performance. There is a positive correlation of 0.82 (p-value < 10^-6) between zero-shot performance and fully supervised performance, suggesting that CLIP is relatively(\u76f8\u5f53\u5730) consistent(\u4e00\u81f4\u7684) at connecting underlying(\u6839\u672c\u7684\u3001\u6f5c\u5728\u7684) representation and task learning to zero-shot transfer. However, zero-shot CLIP only approaches(\u63a5\u8fd1) fully supervised performance on 5 datasets: STL10, CIFAR10, Food101, OxfordPets, and Caltech101. On all 5 datasets, both zero-shot accuracy and fully supervised accuracy are over 90%. This suggests that CLIP may be more effective at zero-shot transfer for tasks where its underlying representations are also high quality. The slope of a linear regression model predicting zero-shot performance as a function of fully supervised performance, zero-shot performance improves by 1.28%. However, the 95th-percentile confidence intervals still include values of less than 1(0.93-1.79). Over the past few years, empirical studies of deep learning systems have documented that performance is predictable as a function of important quantities such as training compute and dataset size(Hestness et al., 2017; Kaplan et al.,2020). The GPT family of models has so far demonstrated consistent improvements in zero-shot performance across a 1000x increase in training compute. In Figure 9, we check whether the zero-shot performance of CLIP follows a similar scaling pattern. We plot the average error rate of the 5 ResNet CLIP models across 39 evaluations on 36 different datastes and find that a similar log-log linear scaling trend holds for CLIP across a 44x increase in model compute. While the overall trend is smooth, we found that performance on individual evaluations can be much noisier. We are unsure whether this is caused by high variance between individual training runs sub-tasks(as documented in D'Amour et al.(2020)) masking a steadily improving trend or whether performance is actually non-monotonic(\u975e\u5355\u8c03\u7684) as a function of compute on some tasks. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Deep Learning Scaling is Predictable, Empirically \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6269\u5c55\u6027\u662f\u53ef\u4ee5\u9884\u6d4b\u7684 - Scaling Laws for Neural Language Models \u6df1\u5ea6\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u6027\u89c4\u5219 Figure 9. Zero-shot CLIP performance scales smoothly as a function of model compute. Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log linear trend across a 44x range of compute spanning 5 different CLIP models. Lightly shaded lines are performance on individual evals, showing that performance is much more varied despite the smooth overall trend.","title":"3.1.5 Analysis of Zero-shot CLIP Performance"},{"location":"VisionMultiModal/CLIP/paper_read/#32-representation-learning","text":"While we have extensively(\u5e7f\u6cdb\u5730) analyzed the task-learning capabilities of CLIP through zero-shot transfer in the pervious section, it is more common to study the representation learning capabilities of a model. There exist many ways to evaluate the quality of representations as well as disagreements(\u5206\u6b67) over what properties an \"ideal\" representation should have (Locatello et al.,2020). Fitting a linear classifier on a representation extracted from the model and measuring its performance on various datasets is a common approach. An alternative is measuring the performance of end-to-end fine-tuning of the model. This increases flexibility(\u7075\u6d3b\u6027), and prior work has convincingly demonstrated that fine-tuning outperforms linear classification on most image classification datasets(Kornblith et al.,2019;Zhai et al.,2019). While the high performance of fine-tuning motivates its study for practical reasons, we still opt for linear classifier based evaluation for several reasons. Our work is focused on developing a high-performing task and dataset-agnostic pre-training approach. Fine-tuning, because it adapts(\u9002\u5e94\u3001\u5e94\u7528) representations to each dataset during the fine-tuning phase, can compensate for(\u8865\u507f) and potentially mask failures to learn general and robust representations during the pre-training phase. Linear classifiers, because of their limited flexibility, instead hightlight these failures and provide clear feedback during development. For CLIP, training supervised linear classifier has the added benefit of being very similar to the approach used for its zero-shot classifiers which enables extensive comparisons and analysis in Section 3.1. Finally, we aim to compare CLIP to a comprehensive(\u7efc\u5408\u7684) set of existing models across many tasks. Studying 66 diferent models on 27 different datasets requires tuning 1782 different evaluations. Fine-tuning opens up a much larger design and hyper-parameter space, which makes it difficult to fairly evaluate and computationally expensive to compare a diverse set of techniques as discussed in other large scale empirical studies(Lucic et al.,2018; Choi et al.,2019). By comparison, linear classifiers require minimal hyper-parameter tuning and have standardized implementations and evaluation procedures. Please see Appendix A for further details on evaluation. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 A Sober look at the Unsupervised Learning of Disentangled Representations and their Evaluation \u6e05\u9192\u5730\u770b\u5f85\u89e3\u5f00\u8868\u5f81\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u4ed6\u4eec\u7684\u8bc4\u4f30 - Locatello et al.,2020 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark - - Zhai et al.,2019 Are GANs Created Equal? A Large-Scale Study - - Lucic et al.,2018 On Empirical Comparisons of Optimizers for Deep Learning \u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u5316\u5668\u7684\u7ecf\u9a8c\u6027\u6bd4\u8f83 - Choi et al.,2019 Figure 10 summarizes our findings. To minimize selection effects that could raise concerns of confirmation or reporting bias, we first study performance on the 12 dataset evaluation suite from Kornblith et al.(2019). While small CLIP models such as a ResNet-50 and ResNet-101 outperform other ResNets trained on ImageNet-1K(BiT-S and the originals), they underperform ResNets trained on ImageNet-21K(BiT-M). These small CLIP models also underperform models in the EfficeintNet family with similar compute requirements. However, models trained with CLIP scale very well and the largest model we trained (ResNet-50x64) slightly outperforms the best performing existing model (a Noisy Student EfficientNet-L2) on both overall score and compute efficiency. We also find that CLIP vision transformers are about 3x more compute efficient than CLIP ResNets, which allows us to reach higher overall performance within our compute budget(\u9884\u7b97). These results qualitatively replicate(\u590d\u5236) the findings of Dosovitskiy et al.(2020) which reported that vision transformers are more compute efficient than convnets when trained on sufficiently large datasets. Our best overall model is a ViT-L/14 that is fine-tuned at a higher resolution of 336 pixels on our dataset for 1 additional epoch. This model outperforms the best existing model across this evaluation suite by an average of 2.6%. Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfficientNet(Tan & Le,2019; Xie et al.,2020), MoCo(Chen et al.,2020d), Instagram-pretrained ResNeXt models (Mahajan et al.,2018;Touvron et al,.2019), BiT(Kolesnikov et al.,2019), ViT(Dosovitskiy et al.,2020), SimCLRv2(Chen et al.,2020c), BYOL(Grill et al.,2020), and the original ResNet models. (Left) Scores are averaged over 12 datasets studied by Kornblith et al.(2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Efficientnet: Rethinking model scaling for convolutional neural networks. - - Tan & Le,2019 Self-training with Noisy Student improves ImageNet classification - - Xie et al.,2020 Improved Baselines with Momentum Contrastive Learning - MoCo Chen et al.,2020d Exploring the Limits of Weakly Supervised Pretraining - - Mahajan et al.,2018 Fixing the train-test resolution discrepancy - - Touvron et al,.2019 Large Scale Learning of General Visual Representations for Transfer. - BiT Kolesnikov et al.,2019 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - ViT Dosovitskiy et al.,2020 Big Self-Supervised Models are Strong Semi-Supervised Learners - SimCLRv2 Chen et al.,2020c Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning - BYOL rill et al.,2020 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 As Figure 21 qualitatively shows, CLIP models learn a wider(\u76f8\u6bd4\u66f4\u5e7f\u6cdb\u7684) set of tasks than has previously been demonstrated in a single computer vision model trained ene-to-end from random initialization. These tasks include geo-localization, optical character recognition, facial emotion recognition, and action recognition. None of these tasks are measured in the evaluation suite of Kornblith et al.(2019). This could be argued to be a form of selection bias in Kornblith et al.(2019)'s study towards tasks that overlap(\u91cd\u53e0) with ImageNet. To address this, we also measure performance on a broader 27 dataset evaluation suite. This evaluation suite, detailed in Appendix A includes datasets representing the aforementioned tasks, German Traffic Signs Recognition Benchamark(Stallamp et al.,2011), as well as several other datasets adapted from VTAB(Zhai et al.,2019). \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark - - Zhai et al.,2019 On this broader evaluation suite, the benefits of CLIP are more clear. All CLIP models, regradless of scale, outperform all evaluated systems in terms of compute efficiency. The improvement in average score of the best model over previous systems increases from 2.6% to 5%. We also find taht self-supervised systems do noticeably(\u660e\u663e) better on our broader evaluation suite. For instance, while SimCLRv2 still underperforms BiT-M on average on the 12 datasets of Kornblith et al.(2019), SimCLRv2 outperforms BiT-M on our 27 dataset evaluation suite. These findings suggest continuing to expand task diversity and coverage in order to better understand the \"general\" performance of systems. We suggest additional evaluation efforts along the lines of VTAB to be valuable. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Do Better ImageNet Models Transfer Better? - - Kornblith et al.,2019 In addition to the aggregate analysis above, we visualize per-dataset differences in the performance of the best CLIP model and the best model in our evaluation suite across all 27 datasets in Figure 11. CLIP outperforms the Noisy Student EfficientNet-L2 on 21 of the 27 datasets. CLIP improves the most on tasks which require OCR(SST2 and HatefulMemes), geo-localization and scene recognition(Country211, SUN397), and activity recognition in videos(Kinetics700 and UCF101). In addition CLIP also does much better on fine-grained car and traffic sign recognition(Stanford Cars and GTSRB). This may reflect a problem with overly narrow supervision in ImageNet. A result such as the 14.7% improvement on GTSRB could be indicative of an issue with ImageNet-1K, which has only a single label for all traffic and street signs. This could encourage a supervised representation to collapse(\u5d29\u6e83\u3001\u574d\u584c) intra-class details and hurt accuracy on a fine-grained downstream task. As mentioned, CLIP still underperforms the EfficientNet on several datasets. Unsurprisingly, the dataset that the EfficientNet does best relative to CLIP on is the one it was trained on: ImageNet. The EfficientNet also slightly outperforms CLIP on low-resolution datasets such as CIFAR10 and CIFAR100. We suspect this is at least partly due to the lack of scale-based data augmentation in CLIP. The EfficientNet also does slightly better on PatchCamelyon and CLEVRCounts, datasets where overall performance is still low for both approaches. Figure 11. CLIP's features outperform the features of the best ImageNet model on a wide variety of datasets. Fitting a linear classifier on CLIP's features outperforms using the Noisy Student EfficientNet-L2 on 21 out of 27 datasets.","title":"3.2 Representation Learning"},{"location":"VisionMultiModal/CLIP/paper_read/#33-robustness-to-natural-distribution-shift-clip","text":"In 2015, it was announced that a deep learning model exceeded human performance on the ImageNet test set(He et al.,2015). However, research in the subsequent(\u540e\u6765\u7684) years has repeatedly found that these models still make many simple mistakes(Dodge & Karam,2017;Geirhos et al.,2018;Alcorn et al.,2019), and new benchmarks testing these systems has often found their performance to be much lower than both their ImageNet accuracy and human accuracy(Recht et al.,2019;Barbu et al.,2019). What explains this discrepancy? Various ideas have been suggested and studied (IIyas et al.,2019;Geirhos et al.,2020). A common theme of proposed explanations is that deep learning models are exceedingly adept at(\u975e\u5e38\u64c5\u957f\u7684) finding correlations and patterns which hold across their trainiing dataset and thus improve in-distribution performance. However many of these correlations and patterns are actually spurious(\u865a\u5047\u7684) and do not hold for other distributions and result in large drops in performance on other datasets. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions - - Dodge & Karam,2017 ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness - - Geirhos et al.,2018 Strike(With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familliar Objects - - Alcorn et al.,2019 Do ImageNet Classifiers Generalize to ImageNet - - Recht et al.,2019 ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models - - Barbu et al.,2019 Adversarial(\u5bf9\u6297\u7684) Examples Are Not Bugs, They Are Features - - IIyas et al.,2019 Shortcut Learning in Deep Neural Networks - - Geirhos et al.,2020 We caution that(\u8b66\u544a), to date, (\u81f3\u4eca\u4e3a\u6b62) most of these studies limit their evaluation to models trained on ImageNet. Recalling the topic of discussion, it may be a mistake to generalize too far from these initial findings. To what degree are these failures attibutable to deep learning, ImageNet, or some combination of the two? CLIP models, which are trained via natural language supervision on a very large dataset and are capable of high zero-shot performance, are an opportunity to investigate this question from a different angle. Taori et al.(2020) is a recent comprehensive study moving towards quantifying and understanding these bahaviors for ImageNet models.Taori et al.(2020) study how the performance of ImageNet models change where evaluated on natural distribution shifts. They measure performance on a set of 7 distribution shifts: ImageNetV2 (Recht et al.,2019), ImageNet Sketch(Wang et al.,2019), Youtube-BB and ImageNet-Vid(Shankar et al.,2019), ObjectNet(Barbu et al.,2019), ImageNet Adversarial(Hendrycks et al.,2019), and ImageNet Rendition(Hendrycks et al.,2020a). They distinguish(\u533a\u5206) these datasets, which all consist of novel images collected from a variety of sources, from synthetic distribition shifts such as ImageNet-C(Hendrycks & Dietterich,2019), Stylized ImageNet(Geirhos et al.,2018), or adversarial attacks(Goodfellow et al.,2014) which are created by perturbing(\u6270\u52a8) existing images in various ways. They propose this distinction(\u533a\u522b) because in part because they find that while several techniques have been demonstrated to improve performance on synthetic distribution shifts, they often fail to yield consistent improvements on natural distributions. \u8bba\u6587\u540d\u79f0 \u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Measuring Robustness to Natural Distribution Shifts in Image Classification - - Taori et al.(2020) Do ImageNet Classifiers Generalize to ImageNet - ImageNetV2 Recht et al.,2019 Learning Robust Global Representations by Penalizing Local Predictive Power - ImageNet Sketch Wang et al.,2019 Do Image Classifiers Generalize Across Time - ImageNet-Vid Shankar et al.,2019 ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models - ObjectNet Barbu et al.,2019 The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization - ImageNet Rendition Hendrycks et al.,2020a Benchmarking Neural Network Robustness to Common Corruptions and Perturbations - ImageNet-C Hendrycks & Dietterich,2019 ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness - Stylized ImageNet Geirhos et al.,2018 Explaining and Harnessing Adversarial Examples - - Goodfellow et al.,2014 Across these collected datasets, the accuracy of ImageNet models drop well below the expectation set by the ImageNet validation set. For the following summary discussion we report average accuracy across all 7 natural distribution shift datasets and average accuracy across the corresonding class subsets of ImageNet unless otherwise specified. Additionally, for Youtube-BB and ImageNet-Vid, which have two different evaluation settings, we use the average of pm-0 and pm-10 accuracy. ...","title":"3.3 Robustness to Natural Distribution Shift (\u6709\u81ea\u7136\u7684\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b CLIP\u7684\u9c81\u68d2\u6027\u662f\u600e\u6837\u7684)"},{"location":"VisionMultiModal/CLIP/paper_read/#6-limitations","text":"There are still many limitations to CLIP. While several of these are discussed as part of analysis in various sections, we summarize and collect them here. On datasets with training splits, the performance of zero-shot CLIP is on average competitive with the simple supervised baseline of a linear classifier on top of ResNet-50 features. On most of these datasets, the performance of this baseline is now well below the overall state of the art. Significant work is still needed to improve the task learning and trasfer capabilities of CLIP. While scaling has so fat steadily improved performance and suggests a route for continued improvement, we estimate around a 1000x increase in compute is required for zero-shot CLIP to reach overall state-of-the-art performance. This is infeasible(\u4e0d\u53ef\u884c\u7684) to train with current hardware. Further research into improving upon the computational and data efficiency of CLIP will be necessary. Analysis in Section 3.1 found that CLIP's zero-shot performance is still quite weak on several kinds of tasks. When compared to task-specific models, the performance of CLIP is poor on several types of fine-grained classification such as differentiating models of cars, species of flowers, and variants of aircraft. CLIP also struggles with more abstract and systematic tasks such as counting the number of objects in an image. Finally for novel tasks which are unlikely to be included in CLIP's pre-training dataset, such as classifying the distance to the nearest car in a photo, CLIP's performance can be near random. We are confident that there are still many, many, tasks where CLIP's zero-shot performance is near chance level. While zero-shot CLIP generalizes well to many natural image distributions as investigated in Section 3.3, we've observed that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. An illustrative(\u8bf4\u660e\u6027\u7684) example occurs for the task of OCR as reported in Appendix E. CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accuracy on the handwritten digits of MNIST. An embarrassingly(\u5c34\u5c2c\u5730) simple baseline of logistic regression on raw pixels outperforms zero-shot CLIP. Both semantic and near-duplicate nearest-neighbor retrieval verify that there are almost no images that resemble MNIST digits in our pre-training dataset. This suggests CLIP does little to address the underlying(\u6f5c\u5728\u7684) problem of brittle(\u8106\u7684) generalization of deep learning models. Instead CLIP tries to circumvent(\u89c4\u907f) the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution. This is a naive assumption that, as MNIST demonstrates, is easy to violate(\u8fdd\u53cd). Although CLIP can flexibly generate zero-shot classifiers for a wide variety of tasks and datasets, CLIP is still limited to choosing from only those concepts in a given zero-shot classifier. This is a significant restriction compared to a truly flexible approach like image captioning which could generate novel outputs.","title":"6. Limitations"},{"location":"VisionMultiModal/CLIP/paper_code/main/","text":"","title":"\u4ee3\u7801\u9605\u8bfb"},{"location":"VisionMultiModal/MAE/paper_interpretation/","text":"","title":"\u8bba\u6587\u8be6\u89e3"},{"location":"VisionMultiModal/MAE/paper_read/","text":"\u8bba\u6587\u540d\u79f0\uff1aMasked Autoencoders Are Scalable Vision Learners \u8bba\u6587\u5730\u5740\uff1ahttps://arxiv.org/abs/2111.06377 readpaper\u5730\u5740\uff1ahttps://readpaper.com/pdf-annotate/note?pdfId=4556957922861522945&noteId=724534621592129536 \u8bba\u6587\u65f6\u95f4\uff1a2021.11.11 \u4f5c\u8005\u4fe1\u606f\uff1aKaiming He, ..., Ross Girshick \u6458\u8981 This paper shows that masked autoencoders(MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the subset of patchesss(without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g. 75%, yields a nontrival and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity(\u5927\u5bb9\u91cf) models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy(87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior. \u5f15\u8a00 Deep learning has witnessed(\u89c1\u8bc1\u4e86) an explosion of architectures of continuously growing capability\uff08\u80fd\u505a\u6210\u4e00\u4ef6\u4e8b\u7684\u80fd\u529b\uff09 and capacity\uff08\u662f\u5426\u80fd\u9ad8\u6548\u5730\u5b8c\u6210\u4e00\u4ef6\u4e8b\u60c5\uff09[AlexNet, ResNet, Transformer]. Aided by the rapid gains in hardware, models today can easily overfit one million images [ImageNet ] and begin to demand(\u8981\u6c42\uff0c\u9700\u8981) hundreds of millions of - often publicly inaccessible(\u516c\u4f17\u65e0\u6cd5\u8bbf\u95ee\u7684\u3001\u4e0d\u6613\u8bbf\u95ee\u7684) - labeled images [ViT ]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b AlexNet 2012-12-03 Deep Residual Learning for Image Recognition \u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6765\u505a\u56fe\u7247\u5206\u7c7b ResNet 2015-12-10 Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 Transformer - ImageNet: A large-scale hierarchical image database \u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5c42\u6b21\u7684\u56fe\u7247\u6570\u636e\u96c6 ImageNet 2009-06-20 This appetite(\u98df\u6b32) of data has been successfully addressed in natural language processing (NLP) by self-supervised pre-training. The solutions, based on autoregressive language modeling in GPT and masked autoencdoing in BERT, are conceptually(\u6982\u5ff5\u4e0a) simple: they remove a portion of the data and learn to predict the removed content. These methods now enable training of generalizable\uff08\u5177\u6709\u6cdb\u5316\u6027\u7684\uff09NLP models containing over one hundred of billion parameters. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT - The idea of masked autoencoders, a form of more general denoising autoencoders[48], is natural and applicable(\u9002\u7528\u7684) in computer vision as well. Indeed, closely related research in vision[49,39], preceded(\u5148\u4e8e) BERT. However, despite significant interest in this idea following the success of BERT, progress of autoencoding methods in vision lags behind NLP. We ask: what makes masked autoencoding different between vision and language? We attempt to answer this question from the following perspectives: (i) Until recently, architectures\uff08\u67b6\u6784\uff09 were different. In vision, convolutional networks[29] were dominant(\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u7684) over the last decade[28]. Convolutions typically operate on regular grids and it is not straightforward to integrate(\u6574\u5408) 'indicators'(\u6307\u6807) such as mask tokens[14] or positional embeddings[47] into convolutional networks. This architectural gap, however, has been addressed with the introduction of Vision Transformers(ViT)[16] and should no longer present an obstacle(\u969c\u788d). (ii) Information density(\u5bc6\u5ea6) is different between language and vision. Languages are human-generated signals that are highly semantic and information-dense. When training a model to predict only a few missing words per sentence, this task appears to induce(\u8bf1\u5bfc) sophisticated(\u590d\u6742\u7684) language understanding. Images, on the contrary, are natural signals with heavy spatial redundancy(\u5197\u4f59) - e.g., a missing patch can be recoverted from neighboring patches with little high-level understanding of parts, objects, and scenes. To overcome this difference and encourage learning useful features, we show that a simple strategy works well in computer vision: masking a very high portion of random patches. This strategy largely reduces redundancy and creates a chanllenging self-supervisory task that requires holistic(\u6574\u4f53\u7684) understanding beyond(\u8d85\u8fc7) low-level image statistics. To get a qualitative(\u5b9a\u6027\u7684) sense of our reconstruction task, see Figures 2 - 4. (iii) The autoencoder's decoder, which maps the latent representation back to the input, plays a different role between reconstructing text and images. In vision, the decoder reconstructs pixels, hence its output is of a lower semantic level than common recognition tasks. This is in contrast to language, where the decoder predicts missing words that contain rich semantic information. While in BERT the decoder can be trival(\u7410\u788e\u7684) (an MLP) [14], we found that for images, the decoder design plays a key role in determining the semantic level of the learned latent representations. Driven by this analysis, we present a simple, effective, and scalable form of a masked autoencoder(MAE) for visual representation learning. Our MAE masks random patches from the input image and reconstructs the missing patches in the pixel space. It has an asymmetric encoder-decoder design. Our encoder operates only on the visible subset of patches (without mask tokens), and our decoder is lightweight and reconstructs the input from the latent representation along with mask tokens (Figure 1). Shifting the mask tokens to the small decoder in our asymmetric(\u4e0d\u5bf9\u79f0\u7684) encoder-decoder results in a large reduction in computation. Under this design, a very high masking ratio (e.g., 75%) can achieve a win-win scenario(\u53cc\u8d62\u7684\u8bbe\u60f3):it optimizes accuracy while allowing the encoder to process only a small portion(e.g., 25%) of patches. This can reduce overall pre-training time by 3x or more and likewise(\u540c\u6837\u7684) reduce memory consumption(\u6d88\u8017), enabling us to easily scale our MAE to large models. Our MAE learns very high-capacity that generalize well. With MAE pre-training, we can train data-hungry models like ViT-large/-Huge[16] on ImageNet-1K with improved generalization performance. With a vanilla ViT-Huge model, we achieve 87.8% accuracy when fine-tuned on ImageNet-1K. This outperforms all previous results that use only ImageNet-1K data. We also evaluate transfer learning on object detection, instance segmentation, and semantic segmentation. In these tasks, our pre-training achieves better results than its supervised pre-training counterparts, and more importantly, we observe significant gains by scaling up models. These observations are aligned with those witnessed in self-supervised pre-training in NLP[14,40,41,4] and we hope that they will enable our field to explore a similar trajectory(\u8f68\u8ff9). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [48]Extracting and Composing robust features with denoising auto encoders \u4f7f\u7528\u566a\u58f0\u81ea\u7f16\u7801\u5668\u7684\u65b9\u5f0f\u63d0\u53d6\u5e76\u7ec4\u7ec7\u9c81\u68d2\u7684\u7279\u5f81 - 2008-07-05 [49]Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(\u6807\u51c6) \u5806\u53e0\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff1a\u7528\u4e00\u4e2a\u5c40\u90e8\u7684\u566a\u58f0\u6807\u51c6\u53bb\u5b66\u4e60\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6709\u7528\u7684\u8868\u5f81 - 2010-12-01 [39]Context Encoders: Feature Learning by Inpainting \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff1a\u4f7f\u7528\u4fee\u590d\u7684\u65b9\u5f0f\u6765\u505a\u7279\u5f81\u5b66\u4e60 - 2016-04-25 [29] Backpropagation applied to handwritten zip code recognition \u53cd\u5411\u4f20\u64ad\u5e94\u7528\u5230\u624b\u5199\u4f53\u8bc6\u522b - 1989-12-01 [28] ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b AlexNet 2012-12-03 [14] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT [47]Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 Transformer - [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 [40]Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - [41]Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - [4]Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 \u76f8\u5173\u5de5\u4f5c Masked language modeling and its autoregressive counterparts, e.g., BERT[14] and GPT[40,41,4], are highly successful methods for pre-training in NLP. These methods hold out a portion of the input sequence and train models to predict the missing content. These methods have been shown to scale excellently(\u975e\u5e38\u597d\u5730)[4] and a large abundance of evidence indicates that these pre-trained representations generalize well to various downstream tasks. Autoencoding is a classical(\u7ecf\u5178\u7684) method for learning representations. It has encoder that maps an input to a latent representation and a decoder that reconstructs the input. For example, PCA and k-means are autoencoders[25]. Denoising autoencoders (DAE) [48] are a class of autoencoders that corrupt(\u8150\u8d25\u3001\u8d25\u574f) an input signal and learn to reconstruct the original, uncorrupted signal. A series of methods can be thought of as a generalized DAE under different corruptions, e.g., masking pixels[49,39,6] or removing color channels[59]. Our MAE is a form of denosing autoencoding, but different from the classical DAE in numerous ways. Masked image encoding methods learn representations from images corrupted by masking. The pioneering work of [49] presents masking as a noise type in DAE. Context Encoder[39] inpaints(\u4fee\u8865\u3001\u4fee\u590d) large missing regions using convolutional networks. Motivated by the success in NLP, related recent methods[6,16,2] are based on Transformers[47]. iGPT[6] operates on sequences of pixels and predicts unknown pixels. The ViT paper[16] studies masked patch prediction for self-supervised learning. Most recently, BEiT[2] proposes to predict discrete(\u79bb\u6563\u7684) tokens[37,43]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [40]Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - [41]Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - [4]Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 [25]Autoencoders, Minimum Descirption Length and Helmholtz Free Energy \u81ea\u52a8\u7f16\u7801\u5668\u3001\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u548cHelmholtz\u81ea\u7531\u80fd - 1993-11-29 [48]Extracting and Composing robust features with denoising auto encoders \u4f7f\u7528\u566a\u58f0\u81ea\u7f16\u7801\u5668\u7684\u65b9\u5f0f\u63d0\u53d6\u5e76\u7ec4\u7ec7\u9c81\u68d2\u7684\u7279\u5f81 - 2008-07-05 [49]Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(\u6807\u51c6) \u5806\u53e0\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff1a\u7528\u4e00\u4e2a\u5c40\u90e8\u7684\u566a\u58f0\u6807\u51c6\u53bb\u5b66\u4e60\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6709\u7528\u7684\u8868\u5f81 - 2010-12-01 [39]Context Encoders: Feature Learning by Inpainting \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff1a\u4f7f\u7528\u4fee\u590d\u7684\u65b9\u5f0f\u6765\u505a\u7279\u5f81\u5b66\u4e60 - 2016-04-25 [6] Generative Pretraining From Pixels \u4ece\u50cf\u7d20\u4e2d\u505a\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3 iGPT 2020-07-12 [59] Colorful Image Colorization \u5f69\u8272\u56fe\u50cf\u7740\u8272 - 2016-03-28 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 [2]BEiT: BERT Pre-Training of Image Transformers \u5c06bert\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u4f7f\u7528\u5230\u56fe\u7247transformer\u4e2d BEiT 2021-06-15 [47]Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 Transformer - [37]Neural Discrete Representation Learning. \u795e\u7ecf\u7684\u79bb\u6563\u8868\u5f81\u5b66\u4e60 VQ-VAE 2017-11-02 [43]Zero-Shot Text-to-Image Generation \u96f6\u6837\u672c\u7684\u6587\u672c\u5230\u56fe\u7247\u7684\u751f\u6210 - 2021-02-24 Self-supervised learning approaches have been significant interest in computer vision, often focusing on different pretext tasks for pre-training [15,50,35,59,38,17]. Recently, contrastive learning[3,21] has been popular, e.g., [51,36,22,7], which models image similarity and disimilarity(\u5dee\u5f02\u6027)(or only similarity[20,8]) between two or more views. Contrastive and related methods strongly depend on data augmentation[7,20,8]. Autoencoding pursues(\u8ffd\u6c42) a conceptually different direction, and it exhibits(\u5c55\u793a) different behaviors as we will present. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [15]Unsupervised visual representation learning by context prediction. \u901a\u8fc7\u4e0a\u4e0b\u6587\u9884\u6d4b\u6765\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u89c6\u89c9\u8868\u5f81\u5b66\u4e60 - 2015 [50]Unsupervised Learning of Visual Representations using Videos \u4f7f\u7528\u89c6\u9891\u7684\u65e0\u76d1\u7763\u89c6\u9891\u8868\u5f81\u5b66\u4e60 - 2015-05-04 [35]Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles \u901a\u8fc7\u89e3\u51b3\u62fc\u56fe \u7684\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60 2016-03-30 [59] Colorful Image Colorization \u5f69\u8272\u56fe\u50cf\u7740\u8272 - 2016-03-28 [38]Learning Features by Watching Objects Move \u901a\u8fc7\u89c2\u5bdf\u7269\u4f53\u79fb\u52a8\u6765\u5b66\u4e60\u7279\u5f81 - 2016-12-19 [17]Unsupervised Representation Learning by Predicting Image Rotations \u901a\u8fc7\u9884\u6d4b\u56fe\u7247\u65cb\u8f6c\u6765\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u8868\u5f81\u5b66\u4e60 [3]Self-organizing neural network that discovers surfaces in random-dot stereograms \u5728\u968f\u673a\u70b9\u7acb\u4f53\u56fe\u4e2d\u53d1\u73b0\u8868\u9762\u7684\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7f51\u7edc - 2992-01-09 [21]Dimensionality Reduction by Learning an Invariant Mapping \u901a\u8fc7\u5b66\u4e00\u4e2a\u4e0d\u53d8\u7684\u6620\u5c04\uff0c\u6765\u662f\u7ef4\u5ea6\u51cf\u5c0f DrLIM 2006-06-17 [51]Unsupervised Feature Learning via Non-Parametric Instance Discrimination \u901a\u8fc7\u65e0\u53c2\u6570\u7684\u5b9e\u4f8b\u5224\u522b\u4efb\u52a1\u6765\u8fdb\u884c\u65e0\u76d1\u7763\u7279\u5f81\u5b66\u4e60 - - [36]Representation Learning with Contrastive Predictive Coding \u57fa\u4e8e\u5bf9\u6bd4\u9884\u6d4b\u7f16\u7801\u7684\u8868\u5f81\u5b66\u4e60 - 2018-07-10 [22]Momentum Contrast for Unsupervised Visual Representation Learning \u52a8\u91cf\u5bf9\u6bd4\u65b9\u6cd5\u7528\u4e8e\u65e0\u76d1\u7763\u89c6\u9891\u8868\u5f81\u5b66\u4e60 MoCo - [7]A Simple Framework for Contrastive Learning of Visual Representations \u4e00\u79cd\u89c6\u89c9\u8868\u5f81\u5bf9\u6bd4\u5b66\u4e60\u7684\u7b80\u5355\u6846\u67b6 SimCLR 2020-02-13 [20]Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning \u5f15\u5bfc\u4f60\u81ea\u5df1\u7684\u6f5c\u529b\uff1a\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5 T3C 2020-06-13 [8]Exploring Simple Siamese Representation Learning \u63a2\u7d22\u7b80\u5355\u7684\u8fde\u4f53\u8868\u793a\u5b66\u4e60 - 2020-11-20 \u65b9\u6cd5 Our masked autoencoder(MAE) is a simple autoencoding approach that reconstructs the original signal given its partial observation(\u57fa\u4e8e\u5b83\u90e8\u5206\u7684\u89c2\u5bdf). Like all autoencoders, our approach has an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the original signal from the latent representation. Unlike classical autoencoders, we adopt an asymmetric design that allows the encoder to operate only on the partial, observed signal(without mask tokens) and a lightweight decoder that reconstructs the full signal from the latent representation and mask tokens. Figure 1 illustrates the idea, introduced next. \u56fe1 Figure 1. Our MAE architecture . During pre-training, a large random subset of image patches(e.g., 75%) is masked out. The encoder is applied to the subset of visible patches. Mask tokens are introduced after the encoder, and the full set of encoded patches and mask tokens is processed by a small decoder that reconstructs the original image in pixels. After pre-training, the decoder is discarded(\u4e22\u5f03) and the encoder is applied to uncorrupted images to produce representations for recognition tasks. Masking. Followiing ViT[16], we divide an image into regular non-overlapping patches. Then we sample a subset of patches and mask (i.e., remove) the remaining ones. Our sampling strategy is straightforward: we sample random patches without replacement, following a uniform distribution. We simply refer to this as \"random sampling\". \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 Random sampling with a high masking ratio (i.e., the ratio of removed patches) largely eliminates(\u6d88\u9664) redundancy, thus creating a task that cannot be easily solved by extrapolation(\u5916\u63d2\u3001\u5916\u5ef6) from visible neighboring patches (see Figures 2-4). The uniform distribution prevents a potential center bias (i.e., more masked patches near the image center). Finally, the highly sparse input creates an opportunity for designing an efficient encoder, introduced text. \u56fe2 Figure 2. Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction (middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix. As no loss is computed on visible patches. the model output on visible patches is qualitatively worse. One can simply overlay(\u8986\u76d6) the output with the visible patches to improve visual quality. We intentionally(\u6545\u610f\u5730) opt not do this, so we can more comprehensively demonstrate the method's behavior. \u56fe3 Figure 3. Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights in Figure 2). Observe the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible(\u4f3c\u662f\u800c\u975e). \u56fe4 Figure 4. Reconstructions of ImageNet validation images using an MAE pre-trained with a masking ratio of 75% but applied on inputs with higher masking ratios. The predictions differ plausibly from the original images, showing that the method can generalize. MAE encoder. Our encoder is a ViT[16] but applied only on visible, unmasked patches. Just as in a standard ViT, our encoder embeds patches by a linear projection with added positional embeddings, and then processes the resulting set via a series of Transformer blocks. However, our encoder only operates on a small subset (e.g. 25%) of the full set. Masked patches are removed; no mask tokens are used. This allows us to train very large encoders with only a fraction of compute and memory. The full set is handled by a lightweight decoder, described next. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 MAE decoder. The input to the MAE decoder is the full set of tokens consisting of (i) encoded visible patches, and (ii) mask tokens. See Figure 1. Each mask token[14] is a shared, learned vector that indicates the presence of (\u8868\u793a..\u7684\u5b58\u5728) a missing patch to be predicted. We add positional embeddings to all tokens in this full set; without this, mask tokens would have no information about their location in the image. The decoder has another series of Transformer blocks. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [14] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT The MAE decoder is only used during pre-training to perform the image reconstruction task (only the encoder is used to produce image representations for recognition). Therefore, the decoder architecture can be flexibly designed in a manner that is independent of the encoder design. We experiment with very samll decoders, narrower and shallowe than the encoder. For example, our default decoder has <10% computation per token vs. the encoder. With this asymmetrical design, the full set of tokens are only processed by the lightweight decoder, which significantly reduces pre-training time. Reconstruction target. Our MAE reconstructs the input by predicting the pixel values for each masked patch. Each element in the decoder's output is a vector of pixel values representing a patch. The last layer of the decoder is a linear projection whose number of output channels equals the number of pixel values in a patch. The decoder's output is reshaped to form a reconstructed image. Our loss function computes the mean squared error (MSE) between the reconstructed and original images in the pixel space. We compute the loss only on masked patches, similar to BERT[14]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [14] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT We also study a variant whose reconstruction target is the normalized pixel values of each masked patch. Specifically, we compute the mean and standard deviation of all pixels in a patch and use them to normalize this patch. Using normalized pixels as the reconstruction target improves representation quality in our experiments. Simple implementation. Our MAE pre-training can be implemented efficiently, and importantly, does not require any specialized sparse operations. First we generate a token for every input patch (by linear projection with an added positional embedding). Next we randomly shuffle the list of tokens and remove the last portion of the list, based on the masking ratio. This process produces a small subset of tokens for the encoder and is equivalent to sampling patches withour replacement. After encoding, we append a list of mask tokens to the list of encoded patches, and unshuffle this full list (inverting the random shuffle operation) to align all tokens with their targets. The decoder is applied to this full list (with positional embeddings added). As noted, no sparse operations are needed. This simple implementation introduces negligible(\u5fae\u4e0d\u8db3\u9053\u7684) overhead(\u5f00\u9500) as the shuffling and unshuffling operations are fast. \u8ba8\u8bba\u548c\u7ed3\u8bba Simple algorithms that scale well are the core of deep learning. In NLP, simple self-supervised learning methods (e.g. [40,14,41,4]) enable benefits from exponentially scaling models. In computer vision, practical pre-training paradigms are dominantly supervised (e.g. [28,44,24,16]) despite progress in self-supervised learning. In this study, we observe on ImageNet and in transfer learning that an autoencoder - a simple self-supervised method similar to techniques in NLP - provides scalable benefits. Self-supervised learning in vision may now be embarking on a similar trajectory as in NLP. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [14]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT - [40]Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - [41]Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - [4]Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 [28]ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b AlexNet 2012-12-03 [44]Very Deep Convolutional Networks for Large-Scale Image Recognition - - 2014-09-04 [24]Deep Residual Learning for Image Recognition \u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6765\u505a\u56fe\u7247\u5206\u7c7b ResNet 2015-12-10 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 On the other hand, we note that images and languages are signals of a different nature and this difference must be addressed carefully. Images are merely(\u4ec5\u4ec5) recorded light(\u5149) without a semantic decomposition into the visual analogue of words. Instead of attempting to remove objects, we remove random patches that most likely do not form a semantic segment. Likewise, our MAE reconstructs pixels, which are not semantic entities. Nevertheless, we observe (e.g., Figure 4) that our MAE infers complex, holistic(\u6574\u4f53\u7684) reconstructions, suggesting it has learned numerous(\u5f88\u591a\u7684) visual concepts, i.e. semantics. We hypothesize that this behavior occurs by way of a rich hidden representation inside the MAE. We hope this perspective will inspire(\u542f\u53d1) future work.","title":"\u8bba\u6587\u901a\u8bfb"},{"location":"VisionMultiModal/MAE/paper_read/#_1","text":"This paper shows that masked autoencoders(MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the subset of patchesss(without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g. 75%, yields a nontrival and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity(\u5927\u5bb9\u91cf) models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy(87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.","title":"\u6458\u8981"},{"location":"VisionMultiModal/MAE/paper_read/#_2","text":"Deep learning has witnessed(\u89c1\u8bc1\u4e86) an explosion of architectures of continuously growing capability\uff08\u80fd\u505a\u6210\u4e00\u4ef6\u4e8b\u7684\u80fd\u529b\uff09 and capacity\uff08\u662f\u5426\u80fd\u9ad8\u6548\u5730\u5b8c\u6210\u4e00\u4ef6\u4e8b\u60c5\uff09[AlexNet, ResNet, Transformer]. Aided by the rapid gains in hardware, models today can easily overfit one million images [ImageNet ] and begin to demand(\u8981\u6c42\uff0c\u9700\u8981) hundreds of millions of - often publicly inaccessible(\u516c\u4f17\u65e0\u6cd5\u8bbf\u95ee\u7684\u3001\u4e0d\u6613\u8bbf\u95ee\u7684) - labeled images [ViT ]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b AlexNet 2012-12-03 Deep Residual Learning for Image Recognition \u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6765\u505a\u56fe\u7247\u5206\u7c7b ResNet 2015-12-10 Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 Transformer - ImageNet: A large-scale hierarchical image database \u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5c42\u6b21\u7684\u56fe\u7247\u6570\u636e\u96c6 ImageNet 2009-06-20 This appetite(\u98df\u6b32) of data has been successfully addressed in natural language processing (NLP) by self-supervised pre-training. The solutions, based on autoregressive language modeling in GPT and masked autoencdoing in BERT, are conceptually(\u6982\u5ff5\u4e0a) simple: they remove a portion of the data and learn to predict the removed content. These methods now enable training of generalizable\uff08\u5177\u6709\u6cdb\u5316\u6027\u7684\uff09NLP models containing over one hundred of billion parameters. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT - The idea of masked autoencoders, a form of more general denoising autoencoders[48], is natural and applicable(\u9002\u7528\u7684) in computer vision as well. Indeed, closely related research in vision[49,39], preceded(\u5148\u4e8e) BERT. However, despite significant interest in this idea following the success of BERT, progress of autoencoding methods in vision lags behind NLP. We ask: what makes masked autoencoding different between vision and language? We attempt to answer this question from the following perspectives: (i) Until recently, architectures\uff08\u67b6\u6784\uff09 were different. In vision, convolutional networks[29] were dominant(\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u7684) over the last decade[28]. Convolutions typically operate on regular grids and it is not straightforward to integrate(\u6574\u5408) 'indicators'(\u6307\u6807) such as mask tokens[14] or positional embeddings[47] into convolutional networks. This architectural gap, however, has been addressed with the introduction of Vision Transformers(ViT)[16] and should no longer present an obstacle(\u969c\u788d). (ii) Information density(\u5bc6\u5ea6) is different between language and vision. Languages are human-generated signals that are highly semantic and information-dense. When training a model to predict only a few missing words per sentence, this task appears to induce(\u8bf1\u5bfc) sophisticated(\u590d\u6742\u7684) language understanding. Images, on the contrary, are natural signals with heavy spatial redundancy(\u5197\u4f59) - e.g., a missing patch can be recoverted from neighboring patches with little high-level understanding of parts, objects, and scenes. To overcome this difference and encourage learning useful features, we show that a simple strategy works well in computer vision: masking a very high portion of random patches. This strategy largely reduces redundancy and creates a chanllenging self-supervisory task that requires holistic(\u6574\u4f53\u7684) understanding beyond(\u8d85\u8fc7) low-level image statistics. To get a qualitative(\u5b9a\u6027\u7684) sense of our reconstruction task, see Figures 2 - 4. (iii) The autoencoder's decoder, which maps the latent representation back to the input, plays a different role between reconstructing text and images. In vision, the decoder reconstructs pixels, hence its output is of a lower semantic level than common recognition tasks. This is in contrast to language, where the decoder predicts missing words that contain rich semantic information. While in BERT the decoder can be trival(\u7410\u788e\u7684) (an MLP) [14], we found that for images, the decoder design plays a key role in determining the semantic level of the learned latent representations. Driven by this analysis, we present a simple, effective, and scalable form of a masked autoencoder(MAE) for visual representation learning. Our MAE masks random patches from the input image and reconstructs the missing patches in the pixel space. It has an asymmetric encoder-decoder design. Our encoder operates only on the visible subset of patches (without mask tokens), and our decoder is lightweight and reconstructs the input from the latent representation along with mask tokens (Figure 1). Shifting the mask tokens to the small decoder in our asymmetric(\u4e0d\u5bf9\u79f0\u7684) encoder-decoder results in a large reduction in computation. Under this design, a very high masking ratio (e.g., 75%) can achieve a win-win scenario(\u53cc\u8d62\u7684\u8bbe\u60f3):it optimizes accuracy while allowing the encoder to process only a small portion(e.g., 25%) of patches. This can reduce overall pre-training time by 3x or more and likewise(\u540c\u6837\u7684) reduce memory consumption(\u6d88\u8017), enabling us to easily scale our MAE to large models. Our MAE learns very high-capacity that generalize well. With MAE pre-training, we can train data-hungry models like ViT-large/-Huge[16] on ImageNet-1K with improved generalization performance. With a vanilla ViT-Huge model, we achieve 87.8% accuracy when fine-tuned on ImageNet-1K. This outperforms all previous results that use only ImageNet-1K data. We also evaluate transfer learning on object detection, instance segmentation, and semantic segmentation. In these tasks, our pre-training achieves better results than its supervised pre-training counterparts, and more importantly, we observe significant gains by scaling up models. These observations are aligned with those witnessed in self-supervised pre-training in NLP[14,40,41,4] and we hope that they will enable our field to explore a similar trajectory(\u8f68\u8ff9). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [48]Extracting and Composing robust features with denoising auto encoders \u4f7f\u7528\u566a\u58f0\u81ea\u7f16\u7801\u5668\u7684\u65b9\u5f0f\u63d0\u53d6\u5e76\u7ec4\u7ec7\u9c81\u68d2\u7684\u7279\u5f81 - 2008-07-05 [49]Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(\u6807\u51c6) \u5806\u53e0\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff1a\u7528\u4e00\u4e2a\u5c40\u90e8\u7684\u566a\u58f0\u6807\u51c6\u53bb\u5b66\u4e60\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6709\u7528\u7684\u8868\u5f81 - 2010-12-01 [39]Context Encoders: Feature Learning by Inpainting \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff1a\u4f7f\u7528\u4fee\u590d\u7684\u65b9\u5f0f\u6765\u505a\u7279\u5f81\u5b66\u4e60 - 2016-04-25 [29] Backpropagation applied to handwritten zip code recognition \u53cd\u5411\u4f20\u64ad\u5e94\u7528\u5230\u624b\u5199\u4f53\u8bc6\u522b - 1989-12-01 [28] ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b AlexNet 2012-12-03 [14] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT [47]Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 Transformer - [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 [40]Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - [41]Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - [4]Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28","title":"\u5f15\u8a00"},{"location":"VisionMultiModal/MAE/paper_read/#_3","text":"Masked language modeling and its autoregressive counterparts, e.g., BERT[14] and GPT[40,41,4], are highly successful methods for pre-training in NLP. These methods hold out a portion of the input sequence and train models to predict the missing content. These methods have been shown to scale excellently(\u975e\u5e38\u597d\u5730)[4] and a large abundance of evidence indicates that these pre-trained representations generalize well to various downstream tasks. Autoencoding is a classical(\u7ecf\u5178\u7684) method for learning representations. It has encoder that maps an input to a latent representation and a decoder that reconstructs the input. For example, PCA and k-means are autoencoders[25]. Denoising autoencoders (DAE) [48] are a class of autoencoders that corrupt(\u8150\u8d25\u3001\u8d25\u574f) an input signal and learn to reconstruct the original, uncorrupted signal. A series of methods can be thought of as a generalized DAE under different corruptions, e.g., masking pixels[49,39,6] or removing color channels[59]. Our MAE is a form of denosing autoencoding, but different from the classical DAE in numerous ways. Masked image encoding methods learn representations from images corrupted by masking. The pioneering work of [49] presents masking as a noise type in DAE. Context Encoder[39] inpaints(\u4fee\u8865\u3001\u4fee\u590d) large missing regions using convolutional networks. Motivated by the success in NLP, related recent methods[6,16,2] are based on Transformers[47]. iGPT[6] operates on sequences of pixels and predicts unknown pixels. The ViT paper[16] studies masked patch prediction for self-supervised learning. Most recently, BEiT[2] proposes to predict discrete(\u79bb\u6563\u7684) tokens[37,43]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [40]Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - [41]Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - [4]Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 [25]Autoencoders, Minimum Descirption Length and Helmholtz Free Energy \u81ea\u52a8\u7f16\u7801\u5668\u3001\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u548cHelmholtz\u81ea\u7531\u80fd - 1993-11-29 [48]Extracting and Composing robust features with denoising auto encoders \u4f7f\u7528\u566a\u58f0\u81ea\u7f16\u7801\u5668\u7684\u65b9\u5f0f\u63d0\u53d6\u5e76\u7ec4\u7ec7\u9c81\u68d2\u7684\u7279\u5f81 - 2008-07-05 [49]Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(\u6807\u51c6) \u5806\u53e0\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff1a\u7528\u4e00\u4e2a\u5c40\u90e8\u7684\u566a\u58f0\u6807\u51c6\u53bb\u5b66\u4e60\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6709\u7528\u7684\u8868\u5f81 - 2010-12-01 [39]Context Encoders: Feature Learning by Inpainting \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff1a\u4f7f\u7528\u4fee\u590d\u7684\u65b9\u5f0f\u6765\u505a\u7279\u5f81\u5b66\u4e60 - 2016-04-25 [6] Generative Pretraining From Pixels \u4ece\u50cf\u7d20\u4e2d\u505a\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3 iGPT 2020-07-12 [59] Colorful Image Colorization \u5f69\u8272\u56fe\u50cf\u7740\u8272 - 2016-03-28 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 [2]BEiT: BERT Pre-Training of Image Transformers \u5c06bert\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u4f7f\u7528\u5230\u56fe\u7247transformer\u4e2d BEiT 2021-06-15 [47]Attention Is All You Need \u6ce8\u610f\u529b\u673a\u5236\u662f\u4f60\u9700\u8981\u7684 Transformer - [37]Neural Discrete Representation Learning. \u795e\u7ecf\u7684\u79bb\u6563\u8868\u5f81\u5b66\u4e60 VQ-VAE 2017-11-02 [43]Zero-Shot Text-to-Image Generation \u96f6\u6837\u672c\u7684\u6587\u672c\u5230\u56fe\u7247\u7684\u751f\u6210 - 2021-02-24 Self-supervised learning approaches have been significant interest in computer vision, often focusing on different pretext tasks for pre-training [15,50,35,59,38,17]. Recently, contrastive learning[3,21] has been popular, e.g., [51,36,22,7], which models image similarity and disimilarity(\u5dee\u5f02\u6027)(or only similarity[20,8]) between two or more views. Contrastive and related methods strongly depend on data augmentation[7,20,8]. Autoencoding pursues(\u8ffd\u6c42) a conceptually different direction, and it exhibits(\u5c55\u793a) different behaviors as we will present. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [15]Unsupervised visual representation learning by context prediction. \u901a\u8fc7\u4e0a\u4e0b\u6587\u9884\u6d4b\u6765\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u89c6\u89c9\u8868\u5f81\u5b66\u4e60 - 2015 [50]Unsupervised Learning of Visual Representations using Videos \u4f7f\u7528\u89c6\u9891\u7684\u65e0\u76d1\u7763\u89c6\u9891\u8868\u5f81\u5b66\u4e60 - 2015-05-04 [35]Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles \u901a\u8fc7\u89e3\u51b3\u62fc\u56fe \u7684\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60 2016-03-30 [59] Colorful Image Colorization \u5f69\u8272\u56fe\u50cf\u7740\u8272 - 2016-03-28 [38]Learning Features by Watching Objects Move \u901a\u8fc7\u89c2\u5bdf\u7269\u4f53\u79fb\u52a8\u6765\u5b66\u4e60\u7279\u5f81 - 2016-12-19 [17]Unsupervised Representation Learning by Predicting Image Rotations \u901a\u8fc7\u9884\u6d4b\u56fe\u7247\u65cb\u8f6c\u6765\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u8868\u5f81\u5b66\u4e60 [3]Self-organizing neural network that discovers surfaces in random-dot stereograms \u5728\u968f\u673a\u70b9\u7acb\u4f53\u56fe\u4e2d\u53d1\u73b0\u8868\u9762\u7684\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7f51\u7edc - 2992-01-09 [21]Dimensionality Reduction by Learning an Invariant Mapping \u901a\u8fc7\u5b66\u4e00\u4e2a\u4e0d\u53d8\u7684\u6620\u5c04\uff0c\u6765\u662f\u7ef4\u5ea6\u51cf\u5c0f DrLIM 2006-06-17 [51]Unsupervised Feature Learning via Non-Parametric Instance Discrimination \u901a\u8fc7\u65e0\u53c2\u6570\u7684\u5b9e\u4f8b\u5224\u522b\u4efb\u52a1\u6765\u8fdb\u884c\u65e0\u76d1\u7763\u7279\u5f81\u5b66\u4e60 - - [36]Representation Learning with Contrastive Predictive Coding \u57fa\u4e8e\u5bf9\u6bd4\u9884\u6d4b\u7f16\u7801\u7684\u8868\u5f81\u5b66\u4e60 - 2018-07-10 [22]Momentum Contrast for Unsupervised Visual Representation Learning \u52a8\u91cf\u5bf9\u6bd4\u65b9\u6cd5\u7528\u4e8e\u65e0\u76d1\u7763\u89c6\u9891\u8868\u5f81\u5b66\u4e60 MoCo - [7]A Simple Framework for Contrastive Learning of Visual Representations \u4e00\u79cd\u89c6\u89c9\u8868\u5f81\u5bf9\u6bd4\u5b66\u4e60\u7684\u7b80\u5355\u6846\u67b6 SimCLR 2020-02-13 [20]Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning \u5f15\u5bfc\u4f60\u81ea\u5df1\u7684\u6f5c\u529b\uff1a\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5 T3C 2020-06-13 [8]Exploring Simple Siamese Representation Learning \u63a2\u7d22\u7b80\u5355\u7684\u8fde\u4f53\u8868\u793a\u5b66\u4e60 - 2020-11-20","title":"\u76f8\u5173\u5de5\u4f5c"},{"location":"VisionMultiModal/MAE/paper_read/#_4","text":"Our masked autoencoder(MAE) is a simple autoencoding approach that reconstructs the original signal given its partial observation(\u57fa\u4e8e\u5b83\u90e8\u5206\u7684\u89c2\u5bdf). Like all autoencoders, our approach has an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the original signal from the latent representation. Unlike classical autoencoders, we adopt an asymmetric design that allows the encoder to operate only on the partial, observed signal(without mask tokens) and a lightweight decoder that reconstructs the full signal from the latent representation and mask tokens. Figure 1 illustrates the idea, introduced next. \u56fe1 Figure 1. Our MAE architecture . During pre-training, a large random subset of image patches(e.g., 75%) is masked out. The encoder is applied to the subset of visible patches. Mask tokens are introduced after the encoder, and the full set of encoded patches and mask tokens is processed by a small decoder that reconstructs the original image in pixels. After pre-training, the decoder is discarded(\u4e22\u5f03) and the encoder is applied to uncorrupted images to produce representations for recognition tasks. Masking. Followiing ViT[16], we divide an image into regular non-overlapping patches. Then we sample a subset of patches and mask (i.e., remove) the remaining ones. Our sampling strategy is straightforward: we sample random patches without replacement, following a uniform distribution. We simply refer to this as \"random sampling\". \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 Random sampling with a high masking ratio (i.e., the ratio of removed patches) largely eliminates(\u6d88\u9664) redundancy, thus creating a task that cannot be easily solved by extrapolation(\u5916\u63d2\u3001\u5916\u5ef6) from visible neighboring patches (see Figures 2-4). The uniform distribution prevents a potential center bias (i.e., more masked patches near the image center). Finally, the highly sparse input creates an opportunity for designing an efficient encoder, introduced text. \u56fe2 Figure 2. Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction (middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix. As no loss is computed on visible patches. the model output on visible patches is qualitatively worse. One can simply overlay(\u8986\u76d6) the output with the visible patches to improve visual quality. We intentionally(\u6545\u610f\u5730) opt not do this, so we can more comprehensively demonstrate the method's behavior. \u56fe3 Figure 3. Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights in Figure 2). Observe the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible(\u4f3c\u662f\u800c\u975e). \u56fe4 Figure 4. Reconstructions of ImageNet validation images using an MAE pre-trained with a masking ratio of 75% but applied on inputs with higher masking ratios. The predictions differ plausibly from the original images, showing that the method can generalize. MAE encoder. Our encoder is a ViT[16] but applied only on visible, unmasked patches. Just as in a standard ViT, our encoder embeds patches by a linear projection with added positional embeddings, and then processes the resulting set via a series of Transformer blocks. However, our encoder only operates on a small subset (e.g. 25%) of the full set. Masked patches are removed; no mask tokens are used. This allows us to train very large encoders with only a fraction of compute and memory. The full set is handled by a lightweight decoder, described next. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 MAE decoder. The input to the MAE decoder is the full set of tokens consisting of (i) encoded visible patches, and (ii) mask tokens. See Figure 1. Each mask token[14] is a shared, learned vector that indicates the presence of (\u8868\u793a..\u7684\u5b58\u5728) a missing patch to be predicted. We add positional embeddings to all tokens in this full set; without this, mask tokens would have no information about their location in the image. The decoder has another series of Transformer blocks. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [14] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT The MAE decoder is only used during pre-training to perform the image reconstruction task (only the encoder is used to produce image representations for recognition). Therefore, the decoder architecture can be flexibly designed in a manner that is independent of the encoder design. We experiment with very samll decoders, narrower and shallowe than the encoder. For example, our default decoder has <10% computation per token vs. the encoder. With this asymmetrical design, the full set of tokens are only processed by the lightweight decoder, which significantly reduces pre-training time. Reconstruction target. Our MAE reconstructs the input by predicting the pixel values for each masked patch. Each element in the decoder's output is a vector of pixel values representing a patch. The last layer of the decoder is a linear projection whose number of output channels equals the number of pixel values in a patch. The decoder's output is reshaped to form a reconstructed image. Our loss function computes the mean squared error (MSE) between the reconstructed and original images in the pixel space. We compute the loss only on masked patches, similar to BERT[14]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [14] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT We also study a variant whose reconstruction target is the normalized pixel values of each masked patch. Specifically, we compute the mean and standard deviation of all pixels in a patch and use them to normalize this patch. Using normalized pixels as the reconstruction target improves representation quality in our experiments. Simple implementation. Our MAE pre-training can be implemented efficiently, and importantly, does not require any specialized sparse operations. First we generate a token for every input patch (by linear projection with an added positional embedding). Next we randomly shuffle the list of tokens and remove the last portion of the list, based on the masking ratio. This process produces a small subset of tokens for the encoder and is equivalent to sampling patches withour replacement. After encoding, we append a list of mask tokens to the list of encoded patches, and unshuffle this full list (inverting the random shuffle operation) to align all tokens with their targets. The decoder is applied to this full list (with positional embeddings added). As noted, no sparse operations are needed. This simple implementation introduces negligible(\u5fae\u4e0d\u8db3\u9053\u7684) overhead(\u5f00\u9500) as the shuffling and unshuffling operations are fast.","title":"\u65b9\u6cd5"},{"location":"VisionMultiModal/MAE/paper_read/#_5","text":"Simple algorithms that scale well are the core of deep learning. In NLP, simple self-supervised learning methods (e.g. [40,14,41,4]) enable benefits from exponentially scaling models. In computer vision, practical pre-training paradigms are dominantly supervised (e.g. [28,44,24,16]) despite progress in self-supervised learning. In this study, we observe on ImageNet and in transfer learning that an autoencoder - a simple self-supervised method similar to techniques in NLP - provides scalable benefits. Self-supervised learning in vision may now be embarking on a similar trajectory as in NLP. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u6807\u9898\u7ffb\u8bd1 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [14]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u6df1\u5ea6\u53cc\u5411Transformers\u7684\u9884\u8bad\u7ec3\u7528\u4e8e\u8bed\u8a00\u7406\u89e3 BERT - [40]Improving Language Understanding by Generative Pre-Training \u4f7f\u7528\u751f\u6210\u5f0f\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u7406\u89e3 GPT - [41]Language Models are Unsupervised Multitask Learners \u8bed\u8a00\u6a21\u578b\u662f\u65e0\u76d1\u7763\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u5668 GPT-2 - [4]Language Models are Few-Shot Learners \u8bed\u8a00\u6a21\u578b\u662f\u5c0f\u6837\u672c\u5b66\u4e60\u5668 GPT-3 2020-05-28 [28]ImageNet Classification with Deep Convolutional Neural Networks \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b AlexNet 2012-12-03 [44]Very Deep Convolutional Networks for Large-Scale Image Recognition - - 2014-09-04 [24]Deep Residual Learning for Image Recognition \u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6765\u505a\u56fe\u7247\u5206\u7c7b ResNet 2015-12-10 [16]An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \u7528Transformers\u505a\u5927\u89c4\u6a21\u56fe\u7247\u8bc6\u522b ViT 2020-10-22 On the other hand, we note that images and languages are signals of a different nature and this difference must be addressed carefully. Images are merely(\u4ec5\u4ec5) recorded light(\u5149) without a semantic decomposition into the visual analogue of words. Instead of attempting to remove objects, we remove random patches that most likely do not form a semantic segment. Likewise, our MAE reconstructs pixels, which are not semantic entities. Nevertheless, we observe (e.g., Figure 4) that our MAE infers complex, holistic(\u6574\u4f53\u7684) reconstructions, suggesting it has learned numerous(\u5f88\u591a\u7684) visual concepts, i.e. semantics. We hypothesize that this behavior occurs by way of a rich hidden representation inside the MAE. We hope this perspective will inspire(\u542f\u53d1) future work.","title":"\u8ba8\u8bba\u548c\u7ed3\u8bba"},{"location":"VisionMultiModal/ViLT/1/","text":"ViLT \u6458\u8981 \u4ee5\u524d\u65b9\u6cd5\u7684\u505a\u6cd5\uff1a\u5305\u542b\u4e00\u4e2a\u56fe\u7247\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff08image feature extraction processes\uff09 \u533a\u57df\u76d1\u7763(region supervision)\uff0c\u5373\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684RPN\u7b49 \u5377\u79ef\u67b6\u6784(convolutional architecture)\uff0c\u5373ResNet\u7b49\u7ed3\u6784 \u4ee5\u524d\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898 \u6548\u7387(efficiency)/\u901f\u5ea6(speed)\u95ee\u9898 \u539f\u56e0\u662f: \u7531\u4e8e\u9700\u8981\u4f7f\u7528backbone\u548cRPN\u7b49\u7ec4\u4ef6\u63d0\u53d6\u56fe\u7247\u7279\u5f81\uff0c\u5fc5\u7136\u9700\u8981\u6d88\u8017\u65f6\u95f4 \u8868\u8fbe\u80fd\u529b(expressive power)\u95ee\u9898 \u539f\u56e0\u662f: \u7531\u4e8e\u53ea\u80fd\u53bb\u8bc6\u522b\u9884\u5b9a\u8bcd\u5e93(predefined visual vocabulary)\u4e2d\u7684\u7c7b\u522b\uff0c\u81ea\u7136\u5c31\u662f\u6709\u4e0a\u9650\u7684\u3002 \u672c\u6587\u65b9\u6cd5 \u5b8c\u5168\u53bb\u9664\u56fe\u7247\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff0c\u4ec5\u4f7f\u7528\u4e00\u4e2a\u8ddf\u5904\u7406\u6587\u672c\u4e00\u6837\u7684Linear Embedding\u6a21\u5757 \u672c\u6587\u65b9\u6cd5\u7684\u4ee3\u7801\u5730\u5740 https://github.com/dandelin/vilt \u7b80\u4ecb VLP\u65b9\u6cd5\u7684\u603b\u4f53\u601d\u60f3 \u5229\u7528\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e24\u8005\u7684\u76f8\u4f3c\u5ea6\uff0c\u6765\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u505a\u5fae\u8c03 \u5229\u7528\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\uff0c\u901a\u8fc7\u63a9\u7801\u7684\u65b9\u5f0f\uff0c\u5bf9\u56fe\u7247\u6216\u8005\u6587\u672c\u505amask\u64cd\u4f5c\uff0c\u7136\u540e\u5bf9\u88abmasked\u7684\u90e8\u5206\u8fdb\u884c\u6062\u590d\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6765\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u505a\u5fae\u8c03","title":"ViLT"},{"location":"VisionMultiModal/ViLT/1/#vilt","text":"","title":"ViLT"},{"location":"VisionMultiModal/ViLT/1/#_1","text":"\u4ee5\u524d\u65b9\u6cd5\u7684\u505a\u6cd5\uff1a\u5305\u542b\u4e00\u4e2a\u56fe\u7247\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff08image feature extraction processes\uff09 \u533a\u57df\u76d1\u7763(region supervision)\uff0c\u5373\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684RPN\u7b49 \u5377\u79ef\u67b6\u6784(convolutional architecture)\uff0c\u5373ResNet\u7b49\u7ed3\u6784 \u4ee5\u524d\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898 \u6548\u7387(efficiency)/\u901f\u5ea6(speed)\u95ee\u9898 \u539f\u56e0\u662f: \u7531\u4e8e\u9700\u8981\u4f7f\u7528backbone\u548cRPN\u7b49\u7ec4\u4ef6\u63d0\u53d6\u56fe\u7247\u7279\u5f81\uff0c\u5fc5\u7136\u9700\u8981\u6d88\u8017\u65f6\u95f4 \u8868\u8fbe\u80fd\u529b(expressive power)\u95ee\u9898 \u539f\u56e0\u662f: \u7531\u4e8e\u53ea\u80fd\u53bb\u8bc6\u522b\u9884\u5b9a\u8bcd\u5e93(predefined visual vocabulary)\u4e2d\u7684\u7c7b\u522b\uff0c\u81ea\u7136\u5c31\u662f\u6709\u4e0a\u9650\u7684\u3002 \u672c\u6587\u65b9\u6cd5 \u5b8c\u5168\u53bb\u9664\u56fe\u7247\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff0c\u4ec5\u4f7f\u7528\u4e00\u4e2a\u8ddf\u5904\u7406\u6587\u672c\u4e00\u6837\u7684Linear Embedding\u6a21\u5757 \u672c\u6587\u65b9\u6cd5\u7684\u4ee3\u7801\u5730\u5740 https://github.com/dandelin/vilt","title":"\u6458\u8981"},{"location":"VisionMultiModal/ViLT/1/#_2","text":"VLP\u65b9\u6cd5\u7684\u603b\u4f53\u601d\u60f3 \u5229\u7528\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e24\u8005\u7684\u76f8\u4f3c\u5ea6\uff0c\u6765\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u505a\u5fae\u8c03 \u5229\u7528\u914d\u5bf9\u7684\u56fe\u7247-\u6587\u672c\uff0c\u901a\u8fc7\u63a9\u7801\u7684\u65b9\u5f0f\uff0c\u5bf9\u56fe\u7247\u6216\u8005\u6587\u672c\u505amask\u64cd\u4f5c\uff0c\u7136\u540e\u5bf9\u88abmasked\u7684\u90e8\u5206\u8fdb\u884c\u6062\u590d\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6765\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u505a\u5fae\u8c03","title":"\u7b80\u4ecb"},{"location":"VisionMultiModal/ViLT/paper_interpretation/","text":"\u6807\u9898\uff1aViLT: Vision-and-Language Transformer Without Convolution or Region Supervision \u672f\u8bed: VLP \uff08Vision-and-Language Pre-training\u7684\u9996\u5b57\u6bcd\uff09\uff0c\u662f\u4e00\u79cd\u6709\u6548\u65b9\u6cd5 \u56fe1 \u7684\u89e3\u91ca\uff1a \u672c\u6587\u4f7f\u7528\u7684\u65b9\u6cd5\u548c\u4e4b\u524d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u591a\u6a21\u6001\u878d\u5408\u7684\u65f6\u95f4\u90fd\u76f8\u7b49\uff0c\u4f46\u662f\u5176\u4ed6\u65f6\u95f4\uff0c\u8fdc\u8fdc\u5c0f\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u800c\u4e14\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u7b2c\u4e00\u4e2a \u5176\u4ed6\u65f6\u95f4\u5c0f\u4e8e\u591a\u6a21\u6001\u878d\u5408\u65f6\u95f4\u7684\u65b9\u6cd5\uff080.4ms < 15ms\uff09 \u6211\u4eec\u81ea\u5df1\u65b9\u6cd5\u7684\u5176\u4ed6\u65f6\u95f4\u662f 0.4ms\uff1b\u591a\u6a21\u6001\u878d\u5408\u65f6\u95f4\u662f15ms\uff1b\u603b\u65f6\u95f4\u662f15.4ms UNITER-Base \u7684\u5176\u4ed6\u65f6\u95f4\u662f 75 + 810 = 885ms\uff1b\u591a\u6a21\u6001\u878d\u5408\u65f6\u95f4\u662f15ms\uff1b\u603b\u65f6\u95f4\u662f900ms Pixel-BERT-R50 \u7684\u5176\u4ed6\u65f6\u95f4\u662f 45ms\uff1b\u591a\u6a21\u6001\u878d\u5408\u65f6\u95f4\u662f15ms\uff1b\u603b\u65f6\u95f4\u662f60ms\uff1b \u6211\u4eec\u7684\u65b9\u6cd5\u4e4b\u6240\u4ee5\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u6240\u7528\u65f6\u95f4\u77ed\uff0c\u662f\u56e0\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u7f16\u7801\u5206\u652f\u6ca1\u6709\u4f7f\u7528 \u8017\u65f6\u7684CNN-Backbone \u6216\u8005 \u66f4\u8017\u65f6\u7684RPN\u76f8\u5173\u7f51\u7edc\uff1b\u4ec5\u4ec5\u4f7f\u7528\u4e86\u8f7b\u91cf\u7684 Linear Embedding\u65b9\u6cd5\u3002 ViLBERT: Pretraining Task-Agnostic Visiolinguistic(\u89c6\u89c9\u8bed\u8a00\u5b66\u7684) Representations for Vision-and-Language Tasks UNITER: Learning Universal Image-TExt Representations","title":"Paper interpretation"},{"location":"VisionMultiModal/X-CLIP/paper_interpretation/","text":"\u6807\u9898: X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval","title":"\u8bba\u6587\u8be6\u89e3"},{"location":"VisionMultiModal/X-CLIP/paper_read/","text":"\u8bba\u6587\u540d\u79f0: X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval readpaper\u5730\u5740: https://readpaper.com/pdf-annotate/note?pdfId=4670865857576960001&noteId=738680763071156224 Abstract Video-text retrieval has been a crucial(\u91cd\u8981\u7684) and fundamental task in multi-modal research. The development of video-text retrieval has been considerably promoted(\u5df2\u7ecf\u5927\u5927\u63d0\u5347) by large-scale multi-modal contrastive pre-training, which primarily focuses on coarse-gained or fine-grained contrast. However, cross-grained contrast, which is the contrast between coarse-grained representations and fine-grained representations, has rarely been explored in prior research.Compared with fine-grained or coarse-grained contrasts, cross-grained contrast calculate the correlation between coarse-grained features and each fine-grained feature,and is able to filter out the unnecessary fine-grained features guided by the coarse-grained feature during similarity calculation, thus improving the accuracy of retrieval.To this end, this paper presents a novel multi-grained contrastive model, namely X-CLIP, for video-text retrieval.However, another challenge lies in the similarity aggregation problem, which aims to aggregate fine-grained and cross-grained similarity matrices to instance-level similarity. To address this challenge, we propose the Attention Over Similarity Matrix (AOSM) module to make the model focus on the contrast between essential frames and words, thus lowering the impact of unnecessary frames and words on retrieval results.With multi-grained contrast and the proposed AOSM module, X-CLIP achieves outstanding performance on five widely-used video-text retrieval datasets, including MSRVTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1 R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous state-of-the-art by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on these benchmarks, demonstrating the superiority of multi-grained contrast and AOSM. Code is available at https://github.com/xuguohai/X-CLIP. 1 Introduction Video-text retrieval (VTR) is a multi-modal task, which aims to find the most relevant video/text based on the text/video query.With the explosive growth of videos on the Internet, VTR has attracted increasing interests and served as an important role in people's daily life.Recent years have witnessed the rapid development of VTR, which is supported by a series of pre-training multi-modal models [4, 30, 44], innovative(\u521b\u65b0\u7684) retrieval methods [3, 5, 13\u201315, 24, 30, 34, 35, 38 , 41, 54 , 58, 61 , 63, 66 ] and video-text benchmarks [ 2, 6, 7, 45 , 56]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [4] Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval - 2021-01-01 [30] Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling CLIPBERT 2021-06-20 [44] Learning Transferable Visual Models From Natural Language Supervision CLIP 2021-02-26 [3] Vivit: A video vision transformer Vivit 2021 [5] Is Space-Time Attention All You Need for Video Understanding? TimeSformer 2021 [13] Dual Dense Encoding for Zero-Example Video Retrieval - 2019 [14] MDMMT: Multidomain Multimodal Transformer for Video Retrieval MDMMT 2021 [15] Multi-modal Transformer for Video Retrieval - - [34] HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval HiT 2021 [35] Use what you have: Video retrieval using representations from collaborative experts - 2019 [38] Clip4clip: An empirical study of clip for end to end video clip retrieval Clip4clip 2021 [41] Learning joint embedding with multimodal cues for cross-modal video-text retrieval - 2018 [54] T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval T2VLAD 2021 [58] Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval - 2020 [61] A joint sequence fusion model for video question answering and retrieval - 2018 [63] Cross-modal and hierarchical modeling of video and text - 2018 [66] Actbert: Learning global-local video-text representations Actbert 2020 [2] Localizing Moments in Video with Natural Language - 2017-08-04 [6] ActivityNet: A large-scale video benchmark for human activity understanding ActivityNet 2015-06-07 [7] Collecting Highly Parallel Data for Paraphrase Evaluation - 2011-06-19 [45] The Long-Short Story of Movie Description - 2015-10-07 [56] MSR-VTT: A Large Video Description Dataset for Bridgin Video and Language MSR-VTT 2016-01-01 Recently, with great success in large-scale contrastive languageimage pre-training, VTR has also achieved great progress. Specifically, with 400M image-text pairs for training, CLIP [ 44] can embed the images and sentences into the shared semantic space for similarity calculation.Furthermore, CLIP4Clip [ 38 ] transfers the imagetext knowledge of CLIP to the VTR task, resulting in significant performance improvements on several video-text retrieval datasets. However, CLIP and CLIP4Clip embed the whole sentence and image/video into textual and visual representations, thus lacking the ability to capture fine-grained interactions.To this end, some previous works [29 , 59 ] propose fine-grained contrastive frameworks, which consider the contrast between each word of the sentence and each frame of the video. Moreover, TACo [ 57 ] introduces tokenlevel and sentence-level loss to consider both fine-grained and coarse-grained contrast. Although they have shown promising advances on the VTR task, cross-modality semantic contrast still needs to be systematically explored. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4","title":"\u8bba\u6587\u901a\u8bfb"},{"location":"VisionMultiModal/X-CLIP/paper_read/#abstract","text":"Video-text retrieval has been a crucial(\u91cd\u8981\u7684) and fundamental task in multi-modal research. The development of video-text retrieval has been considerably promoted(\u5df2\u7ecf\u5927\u5927\u63d0\u5347) by large-scale multi-modal contrastive pre-training, which primarily focuses on coarse-gained or fine-grained contrast. However, cross-grained contrast, which is the contrast between coarse-grained representations and fine-grained representations, has rarely been explored in prior research.Compared with fine-grained or coarse-grained contrasts, cross-grained contrast calculate the correlation between coarse-grained features and each fine-grained feature,and is able to filter out the unnecessary fine-grained features guided by the coarse-grained feature during similarity calculation, thus improving the accuracy of retrieval.To this end, this paper presents a novel multi-grained contrastive model, namely X-CLIP, for video-text retrieval.However, another challenge lies in the similarity aggregation problem, which aims to aggregate fine-grained and cross-grained similarity matrices to instance-level similarity. To address this challenge, we propose the Attention Over Similarity Matrix (AOSM) module to make the model focus on the contrast between essential frames and words, thus lowering the impact of unnecessary frames and words on retrieval results.With multi-grained contrast and the proposed AOSM module, X-CLIP achieves outstanding performance on five widely-used video-text retrieval datasets, including MSRVTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1 R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous state-of-the-art by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on these benchmarks, demonstrating the superiority of multi-grained contrast and AOSM. Code is available at https://github.com/xuguohai/X-CLIP.","title":"Abstract"},{"location":"VisionMultiModal/X-CLIP/paper_read/#1-introduction","text":"Video-text retrieval (VTR) is a multi-modal task, which aims to find the most relevant video/text based on the text/video query.With the explosive growth of videos on the Internet, VTR has attracted increasing interests and served as an important role in people's daily life.Recent years have witnessed the rapid development of VTR, which is supported by a series of pre-training multi-modal models [4, 30, 44], innovative(\u521b\u65b0\u7684) retrieval methods [3, 5, 13\u201315, 24, 30, 34, 35, 38 , 41, 54 , 58, 61 , 63, 66 ] and video-text benchmarks [ 2, 6, 7, 45 , 56]. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [4] Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval - 2021-01-01 [30] Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling CLIPBERT 2021-06-20 [44] Learning Transferable Visual Models From Natural Language Supervision CLIP 2021-02-26 [3] Vivit: A video vision transformer Vivit 2021 [5] Is Space-Time Attention All You Need for Video Understanding? TimeSformer 2021 [13] Dual Dense Encoding for Zero-Example Video Retrieval - 2019 [14] MDMMT: Multidomain Multimodal Transformer for Video Retrieval MDMMT 2021 [15] Multi-modal Transformer for Video Retrieval - - [34] HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval HiT 2021 [35] Use what you have: Video retrieval using representations from collaborative experts - 2019 [38] Clip4clip: An empirical study of clip for end to end video clip retrieval Clip4clip 2021 [41] Learning joint embedding with multimodal cues for cross-modal video-text retrieval - 2018 [54] T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval T2VLAD 2021 [58] Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval - 2020 [61] A joint sequence fusion model for video question answering and retrieval - 2018 [63] Cross-modal and hierarchical modeling of video and text - 2018 [66] Actbert: Learning global-local video-text representations Actbert 2020 [2] Localizing Moments in Video with Natural Language - 2017-08-04 [6] ActivityNet: A large-scale video benchmark for human activity understanding ActivityNet 2015-06-07 [7] Collecting Highly Parallel Data for Paraphrase Evaluation - 2011-06-19 [45] The Long-Short Story of Movie Description - 2015-10-07 [56] MSR-VTT: A Large Video Description Dataset for Bridgin Video and Language MSR-VTT 2016-01-01 Recently, with great success in large-scale contrastive languageimage pre-training, VTR has also achieved great progress. Specifically, with 400M image-text pairs for training, CLIP [ 44] can embed the images and sentences into the shared semantic space for similarity calculation.Furthermore, CLIP4Clip [ 38 ] transfers the imagetext knowledge of CLIP to the VTR task, resulting in significant performance improvements on several video-text retrieval datasets. However, CLIP and CLIP4Clip embed the whole sentence and image/video into textual and visual representations, thus lacking the ability to capture fine-grained interactions.To this end, some previous works [29 , 59 ] propose fine-grained contrastive frameworks, which consider the contrast between each word of the sentence and each frame of the video. Moreover, TACo [ 57 ] introduces tokenlevel and sentence-level loss to consider both fine-grained and coarse-grained contrast. Although they have shown promising advances on the VTR task, cross-modality semantic contrast still needs to be systematically explored. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4","title":"1 Introduction"},{"location":"VisionMultiModal/X-CLIP/paper_code/main/","text":"","title":"\u4ee3\u7801\u9605\u8bfb"},{"location":"deep_learning/conv2d/","text":"","title":"\u624b\u52a8\u5b9e\u73b0\u5377\u79ef"},{"location":"deep_learning/transformer/","text":"","title":"transformer\u4ecb\u7ecd"},{"location":"object_classification/VIT/paper_interpretation/","text":"","title":"Paper interpretation"},{"location":"object_classification/VIT/paper_read/","text":"\u8bba\u6587\u540d\u79f0: An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale readpaper\u5730\u5740: https://readpaper.com/pdf-annotate/note?pdfId=4666805048735449089&noteId=737652127941750784 Abstract While the Transformer architecture has become the de-facto(\u4e8b\u5b9e\u4e0a) standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 Introduction Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP).The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset (Devlin et al., 2019). Thanks to Transformers' computational efficiency and scalability, it has become possible to train models of unprecedented(\u7a7a\u524d\u7684) size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020).With the models and datasets growing, there is still no sign of saturating performance. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention Is All You Need Transformer Vaswani et al., 2017 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT Devlin et al., 2019 Language Models are Few-Shot Learners GPT-3 Brown et al., 2020 GShard: Scaling Glant Models with Conditional Computation and Automatic Sharding GShard Lepikhin et al., 2020 In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns.Therefore, in large-scale image recognition, classic ResNetlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Backpropagation applied to handwritten zip code recognition - LeCun et al., 1989 ImageNet Classification with Deep Convolutional Neural Networks - Krizhevsky et al., 2012 Deep Residual Learning for Image Recognition - He et al., 2016 Non-local Neural Networks - Wang et al., 2018 End-to-End Object Detection with Transformers DETR Carion et al., 2020 Stand-Alone Self-Attention in Vision Models - Ramachandran et al., 2019 Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. - Wang et al., 2020a Exploring the Limits of Weakly Supervised Pretraining - Mahajan et al., 2018 Self-training with noisy student improves imagenet classification - Xie et al., 2020 Big transfer(BiT): General visual representation learning BiT Kolesnikov et al., 2020 Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size.This seemingly discouraging outcome may be expected(\u8fd9\u4e2a\u770b\u8d77\u6765\u4ee4\u4eba\u6cae\u4e27\u7684\u7ed3\u679c\u53ef\u80fd\u4e5f\u662f\u5728\u9884\u6599\u4e4b\u4e2d): Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images).We find that large scale training trumps(\u80dc\u8fc7) inductive bias.Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks. 2 Related Work Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand:BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention Is All You Need Transformer Vaswani et al., 2017 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT Devlin et al., 2019 Improving language understanding with unsupervised learning GPT Radford et al., 2018 Language models are unsupervised multitask learners GPT-2 Radford et al., 2019 Language models are few-shot learners GPT-3 Brown et al., 2020 Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations(\u8fd1\u4f3c\u65b9\u6cd5)have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020).In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images.An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a).Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Image transformer - Parmar et al. (2018) Local relation networks for image recognition - Hu et al., 2019 Stand-alone self-attention in vision models - Ramachandran et al., 2019 Exploring self-attention for image recognition - Zhao et al., 2020 Generating long sequences with sparse transformers Sparse Transformers Child et al., 2019 Scaling autoregressive video models - Weissenborn et al., 2019 Axial attention in multidimensional transformers - Ho et al., 2019 Axial-deeplab: Stand-alone axial-attention for panoptic segmentation - Wang et al., 2020a Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 \u00d7 2from the input image and applies full self-attention on top.This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs.Moreover, Cordonnier et al. (2020) use a small patch size of 2 \u00d7 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classification (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or unified text-vision tasks (Chen et al., 2020c; Lu et al., 2019; Li et al., 2019). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 On the relationship between self-attention and convolutional layers - Cordonnier et al. (2020) Attention augmented convolutional networks - Bello et al., 2019 Relation Networks for Object Detection - Hu et al., 2018 End-to-End Object Detection with Transformer DETR Carion et al., 2020 Non-local Neural Networks - Wang et al., 2018 VideoBERT: A Joint Model for Video and Language Representation Learning VideoBERT Sun et al., 2019 Visual transformers: Token-based image representation and processing for computer vision - Wu et al., 2020 Object-Centric Learning with Slot Attention - Locatello et al., 2020 UNITER: Universal Image-Text Representation Learning UNITER Chen et al., 2020c ViLBERT: Pretraining Task-Agnostic Visiollnguistic(\u89c6\u89c9\u8bed\u8a00\u4e3b\u4e49\u8005) Representations for Vision-and-Language Tasks ViLBERT Lu et al., 2019 VisualBERT: A Simple and Performant Baseline for Vision and Language VisualBERT Li et al., 2019 Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers to image pixels after reducing image resolution and color space.The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Generative pretraining from pixels iGPT Chen et al., 2020a Our work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset. The use of additional data allows to achieve state-of-the-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020). Moreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov et al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining - Mahajan et al., 2018 Fixing the train-test resolution discrepancy - Touvron et al., 2019 Self-training with noisy student improves imagenet classification - Xie et al., 2020 Revisiting Unreasonable(\u4e0d\u5408\u7406\u7684) of Data in Deep Learning Era - Sun et al. (2017) Big transfer(BiT): General visual representation learning BiT Kolesnikov et al. (2020) On robustness and transferability of convolutional neural networks - Djolonga et al. (2020) 3 Method In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and their efficient implementations \u2013 can be used almost out of the box. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 All is All You Need Transformer Vaswani et al., 2017 3.1 Vision Transformer (ViT) An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\Bbb R^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\Bbb {R}^{N \\times (P^2 \\cdot C)}$, where $(H,W)$ is the resolution of the original image, $C$ is the number of channels, $(P,P)$ is the resolution of each image patch, and $N=HW/P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (\u516c\u5f0f1). We refer to the output of this projection as the patch embeddings. \u516c\u5f0f1 : $$ \\mathbf{z} 0 = [\\mathbf{x} {class};\\, \\mathbf{x} {p}^{1} \\mathbf{E}; \\, \\mathbf{x} {p}^{2} \\mathbf{E}; \\, \\cdots, \\, \\mathbf{x} {p}^{N} \\mathbf{E}] + \\mathbf{E} {pos} \\qquad \\qquad \\mathbf{E} \\in \\Bbb{R}^{(P^2 \\cdot C) \\times D}, \\, \\mathbf{E}_{pos} \\in \\Bbb{R}^{(N+1) \\times D} $$ Similar to BERT's [class] token, we prepend\uff08\u9884\u5148\u51c6\u5907\uff09 a learnable embedding to the sequence of embedded patches ($\\mathbf{z_0^0}=\\mathbf{x}_{class}$), whose state at the output of the Transformer encoder $(\\mathbf{z_L^0})$ serves as the image representation $\\mathbf{y}$ (\u89c1\u516c\u5f0f4). Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{y}$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder. The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).The MLP contains two layers with a GELU non-linearity. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention is All You Need Transformer Vaswani et al., 2017 Learning deep transformer models for machine translation - Wang et al., 2019 Adaptive Input Representations for Neural Language Modeling - Baevski & Auli, 2019 $$ \\begin{align} \\mathbf{z} {0} &= [\\mathbf{x} {class};\\, \\mathbf{x} {p}^{1} \\mathbf{E}; \\, \\mathbf{x} {p}^{2} \\mathbf{E}; \\, \\cdots, \\, \\mathbf{x} {p}^{N} \\mathbf{E}] + \\mathbf{E} {pos} \\qquad \\qquad \\mathbf{E} \\in \\Bbb{R}^{(P^2 \\cdot C) \\times D}, \\, \\mathbf{E} {pos} \\in \\Bbb{R}^{(N+1) \\times D} \\ \\mathbf{z}^{'} {\\ell} &= MSA(LN(\\mathbf{z_{\\ell-1}})) + \\mathbf{z} {\\ell-1} \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\ell=1 \\cdots L \\ \\mathbf{z} {\\ell} &= MLP(LN(\\mathbf{z}^{'} {\\ell})) + \\mathbf{z}^{'} {\\ell} \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\ell=1 \\cdots L \\ \\mathbf{y} &= LN(\\mathbf{z}_{L}^0) \\end{align} $$ Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than CNNs.In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model.In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch. Hybrid Architecture As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above. 3.2 Fine-Tuning And Higher Resolution Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks.For this, we remove the pre-trained prediction head and attach a zero-initialized $D \\times$ K$ feedforward layer, where $K$ is the number of downstream classes.It is often beneficial to fine-tune at higher resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length.The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),however, the pre-trained position embeddings may no longer be meaningful.We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Fixing the train-test resolution discrepancy - Touvron et al., 2019 BiT transfer(BiT): General visual representation learning BiT Kolesnikov et al., 2020","title":"Paper read"},{"location":"object_classification/VIT/paper_read/#abstract","text":"While the Transformer architecture has become the de-facto(\u4e8b\u5b9e\u4e0a) standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","title":"Abstract"},{"location":"object_classification/VIT/paper_read/#1-introduction","text":"Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP).The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset (Devlin et al., 2019). Thanks to Transformers' computational efficiency and scalability, it has become possible to train models of unprecedented(\u7a7a\u524d\u7684) size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020).With the models and datasets growing, there is still no sign of saturating performance. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention Is All You Need Transformer Vaswani et al., 2017 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT Devlin et al., 2019 Language Models are Few-Shot Learners GPT-3 Brown et al., 2020 GShard: Scaling Glant Models with Conditional Computation and Automatic Sharding GShard Lepikhin et al., 2020 In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns.Therefore, in large-scale image recognition, classic ResNetlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Backpropagation applied to handwritten zip code recognition - LeCun et al., 1989 ImageNet Classification with Deep Convolutional Neural Networks - Krizhevsky et al., 2012 Deep Residual Learning for Image Recognition - He et al., 2016 Non-local Neural Networks - Wang et al., 2018 End-to-End Object Detection with Transformers DETR Carion et al., 2020 Stand-Alone Self-Attention in Vision Models - Ramachandran et al., 2019 Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. - Wang et al., 2020a Exploring the Limits of Weakly Supervised Pretraining - Mahajan et al., 2018 Self-training with noisy student improves imagenet classification - Xie et al., 2020 Big transfer(BiT): General visual representation learning BiT Kolesnikov et al., 2020 Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size.This seemingly discouraging outcome may be expected(\u8fd9\u4e2a\u770b\u8d77\u6765\u4ee4\u4eba\u6cae\u4e27\u7684\u7ed3\u679c\u53ef\u80fd\u4e5f\u662f\u5728\u9884\u6599\u4e4b\u4e2d): Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images).We find that large scale training trumps(\u80dc\u8fc7) inductive bias.Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.","title":"1 Introduction"},{"location":"object_classification/VIT/paper_read/#2-related-work","text":"Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand:BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention Is All You Need Transformer Vaswani et al., 2017 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT Devlin et al., 2019 Improving language understanding with unsupervised learning GPT Radford et al., 2018 Language models are unsupervised multitask learners GPT-2 Radford et al., 2019 Language models are few-shot learners GPT-3 Brown et al., 2020 Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations(\u8fd1\u4f3c\u65b9\u6cd5)have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020).In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images.An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a).Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Image transformer - Parmar et al. (2018) Local relation networks for image recognition - Hu et al., 2019 Stand-alone self-attention in vision models - Ramachandran et al., 2019 Exploring self-attention for image recognition - Zhao et al., 2020 Generating long sequences with sparse transformers Sparse Transformers Child et al., 2019 Scaling autoregressive video models - Weissenborn et al., 2019 Axial attention in multidimensional transformers - Ho et al., 2019 Axial-deeplab: Stand-alone axial-attention for panoptic segmentation - Wang et al., 2020a Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 \u00d7 2from the input image and applies full self-attention on top.This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs.Moreover, Cordonnier et al. (2020) use a small patch size of 2 \u00d7 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classification (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or unified text-vision tasks (Chen et al., 2020c; Lu et al., 2019; Li et al., 2019). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 On the relationship between self-attention and convolutional layers - Cordonnier et al. (2020) Attention augmented convolutional networks - Bello et al., 2019 Relation Networks for Object Detection - Hu et al., 2018 End-to-End Object Detection with Transformer DETR Carion et al., 2020 Non-local Neural Networks - Wang et al., 2018 VideoBERT: A Joint Model for Video and Language Representation Learning VideoBERT Sun et al., 2019 Visual transformers: Token-based image representation and processing for computer vision - Wu et al., 2020 Object-Centric Learning with Slot Attention - Locatello et al., 2020 UNITER: Universal Image-Text Representation Learning UNITER Chen et al., 2020c ViLBERT: Pretraining Task-Agnostic Visiollnguistic(\u89c6\u89c9\u8bed\u8a00\u4e3b\u4e49\u8005) Representations for Vision-and-Language Tasks ViLBERT Lu et al., 2019 VisualBERT: A Simple and Performant Baseline for Vision and Language VisualBERT Li et al., 2019 Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers to image pixels after reducing image resolution and color space.The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Generative pretraining from pixels iGPT Chen et al., 2020a Our work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset. The use of additional data allows to achieve state-of-the-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020). Moreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov et al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Exploring the Limits of Weakly Supervised Pretraining - Mahajan et al., 2018 Fixing the train-test resolution discrepancy - Touvron et al., 2019 Self-training with noisy student improves imagenet classification - Xie et al., 2020 Revisiting Unreasonable(\u4e0d\u5408\u7406\u7684) of Data in Deep Learning Era - Sun et al. (2017) Big transfer(BiT): General visual representation learning BiT Kolesnikov et al. (2020) On robustness and transferability of convolutional neural networks - Djolonga et al. (2020)","title":"2 Related Work"},{"location":"object_classification/VIT/paper_read/#3-method","text":"In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and their efficient implementations \u2013 can be used almost out of the box. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 All is All You Need Transformer Vaswani et al., 2017","title":"3 Method"},{"location":"object_classification/VIT/paper_read/#31-vision-transformer-vit","text":"An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\Bbb R^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\Bbb {R}^{N \\times (P^2 \\cdot C)}$, where $(H,W)$ is the resolution of the original image, $C$ is the number of channels, $(P,P)$ is the resolution of each image patch, and $N=HW/P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (\u516c\u5f0f1). We refer to the output of this projection as the patch embeddings. \u516c\u5f0f1 : $$ \\mathbf{z} 0 = [\\mathbf{x} {class};\\, \\mathbf{x} {p}^{1} \\mathbf{E}; \\, \\mathbf{x} {p}^{2} \\mathbf{E}; \\, \\cdots, \\, \\mathbf{x} {p}^{N} \\mathbf{E}] + \\mathbf{E} {pos} \\qquad \\qquad \\mathbf{E} \\in \\Bbb{R}^{(P^2 \\cdot C) \\times D}, \\, \\mathbf{E}_{pos} \\in \\Bbb{R}^{(N+1) \\times D} $$ Similar to BERT's [class] token, we prepend\uff08\u9884\u5148\u51c6\u5907\uff09 a learnable embedding to the sequence of embedded patches ($\\mathbf{z_0^0}=\\mathbf{x}_{class}$), whose state at the output of the Transformer encoder $(\\mathbf{z_L^0})$ serves as the image representation $\\mathbf{y}$ (\u89c1\u516c\u5f0f4). Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{y}$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder. The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).The MLP contains two layers with a GELU non-linearity. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Attention is All You Need Transformer Vaswani et al., 2017 Learning deep transformer models for machine translation - Wang et al., 2019 Adaptive Input Representations for Neural Language Modeling - Baevski & Auli, 2019 $$ \\begin{align} \\mathbf{z} {0} &= [\\mathbf{x} {class};\\, \\mathbf{x} {p}^{1} \\mathbf{E}; \\, \\mathbf{x} {p}^{2} \\mathbf{E}; \\, \\cdots, \\, \\mathbf{x} {p}^{N} \\mathbf{E}] + \\mathbf{E} {pos} \\qquad \\qquad \\mathbf{E} \\in \\Bbb{R}^{(P^2 \\cdot C) \\times D}, \\, \\mathbf{E} {pos} \\in \\Bbb{R}^{(N+1) \\times D} \\ \\mathbf{z}^{'} {\\ell} &= MSA(LN(\\mathbf{z_{\\ell-1}})) + \\mathbf{z} {\\ell-1} \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\ell=1 \\cdots L \\ \\mathbf{z} {\\ell} &= MLP(LN(\\mathbf{z}^{'} {\\ell})) + \\mathbf{z}^{'} {\\ell} \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\ell=1 \\cdots L \\ \\mathbf{y} &= LN(\\mathbf{z}_{L}^0) \\end{align} $$ Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than CNNs.In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model.In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch. Hybrid Architecture As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.","title":"3.1 Vision Transformer (ViT)"},{"location":"object_classification/VIT/paper_read/#32-fine-tuning-and-higher-resolution","text":"Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks.For this, we remove the pre-trained prediction head and attach a zero-initialized $D \\times$ K$ feedforward layer, where $K$ is the number of downstream classes.It is often beneficial to fine-tune at higher resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length.The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),however, the pre-trained position embeddings may no longer be meaningful.We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 Fixing the train-test resolution discrepancy - Touvron et al., 2019 BiT transfer(BiT): General visual representation learning BiT Kolesnikov et al., 2020","title":"3.2 Fine-Tuning And Higher Resolution"},{"location":"object_classification/VIT/paper_code/main/","text":"","title":"Main"},{"location":"object_detection/ObjectDetection/","text":"parameters() \u7684\u4f7f\u7528 \u7c7b`Conv``\u6765\u81eayolov5\u4e2d\u5e38\u7528\u7684\u4e00\u4e2a\u7ec4\u4ef6\uff0c\u4f9d\u6b21\u4e3a\u4f8b\uff0c\u6211\u4eec\u611f\u53d7\u4e0bpytorch\u6a21\u5757\u4e2dparameters\u53c2\u6570\u7684\u4f7f\u7528\u3002\u53ef\u4ee5\u53d1\u73b0\uff1a \u7ee7\u627fnn.Module\u7684\u81ea\u5b9a\u4e49\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61\uff0c\u5177\u6709 parameters() \u65b9\u6cd5\uff1b model.parameters() \u8fd4\u56de\u7684\u662f\u751f\u6210\u5668\u5bf9\u8c61 model.parameters() \u5305\u542b\u8fd9\u4e2a\u7c7b\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u5305\u62ec\u5377\u79ef\u6838\u7684\u53c2\u6570\u548cBN\u5c42\u7684\u53c2\u6570 import torch from torch import nn class Conv(nn.Module): def __init__(self, c1=3, c2=64, k=6, s=2, p=2): super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, p, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() def forward(self, x): return self.act(self.bn(self.conv(x))) if __name__ == \"__main__\": model = Conv() print(model.parameters()) # <generator object Module.parameters at 0x7fa68016f850> for idx, para in enumerate(model.parameters()): print(\"============\", idx) print(para) print(\"shape: \", para.shape) print(\"total paras: \", para.numel()) # \u83b7\u53d6para\u7684\u53c2\u6570\u603b\u4e2a\u6570 ============ 0 Parameter containing: tensor([[[[-4.9318e-02, 6.9207e-02, -6.3467e-02, -6.5287e-02, 7.6723e-02, -3.8378e-04], [ 7.2446e-02, 3.1003e-02, 8.9773e-02, -8.0289e-02, 6.2464e-02, -7.7457e-02], [-5.8755e-02, -3.6876e-02, 1.6566e-02, 5.2497e-02, 5.6697e-02, -9.5655e-03], [ 7.1956e-02, 6.3590e-02, 2.7415e-02, -6.0436e-02, 3.3391e-02, 6.7367e-02], [ 2.1894e-02, -5.5162e-02, 8.6821e-03, -7.9369e-02, -7.6835e-02, 8.5203e-03], [ 2.0913e-02, 1.5397e-02, -2.7012e-02, 4.1439e-02, 9.0220e-02, -9.3959e-02]], ..., [[ 8.0491e-02, -7.4360e-02, 5.7617e-03, 7.9103e-03, 9.0219e-02, -4.4207e-03], [ 8.2175e-02, 2.4835e-02, 4.7966e-02, -2.8035e-02, 8.0372e-02, 4.0228e-02], [ 1.4321e-02, 3.6747e-02, -1.5620e-02, 7.2155e-03, -2.5480e-02, -2.4253e-02], [ 2.9245e-02, -6.4244e-02, -5.4636e-03, 2.8508e-02, -7.1834e-02, -7.2294e-02], [ 1.3893e-02, -8.6789e-02, -2.7554e-02, -5.6497e-02, 3.6276e-02, 8.4411e-02], [-1.0794e-02, 8.5295e-03, 7.9786e-02, -1.3315e-02, -9.4185e-02, 7.1432e-02]]]], requires_grad=True) shape: torch.Size([64, 3, 6, 6]) ============ 1 Parameter containing: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True) shape: torch.Size([64]) ============ 2 Parameter containing: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True) shape: torch.Size([64])","title":"yolov5"},{"location":"object_detection/yolov7/","text":"\u8bba\u6587\u540d\u79f0: YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors readpaper\u5730\u5740: https://readpaper.com/pdf-annotate/note?pdfId=4666972415243337729&noteId=729490235758215168 \u5173\u952e\u8bcd: \u68af\u5ea6\u5206\u6790 gradient analysis transition layers Abstract YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100 . YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy.Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7. 1 Introduction Real-time object detection is a very important topic in computer vision, as it is often a necessary component in computer vision systems. For example, multi-object tracking[94,93], autonomous driving [40,18], robotics [35,58], medical image analysis [34,46], etc. The computing devices that execute real-time object detection is usually some mobile CPU or GPU, as well as various neural processing units (NPU) developed by major manufacturers. For example, the Apple neural engine (Apple), the neural compute stick (Intel), Jetson AI edge devices (Nvidia), the edge TPU (Google), the neural processing engine (Qualcomm), the AI processing unit (MediaTek), and the AI SoCs (Kneron), are all NPUs.Some of the above mentioned edge devices focus on speeding up different operations such as vanilla convolution, depth-wise convolution, or MLP operations.In this paper, the real-time object detector we proposed mainly hopes that it can support both mobile GPU and GPU devices from the edge to the cloud. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [94] FairMOT: On the Fainess of Detection and Re-identification in Multiple Object Tracking FairMOT 2020-04-04 [93] ByteTrack: Multi-Object Tracking by Associating Every Detection Box ByteTrack - [40] GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving GS3D 2019-03-26 [18] Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges - 2021-03-01 [35] Object Detection Approach for Robot Grasp(\u6293\u4f4f) Detection - 2019-05-20 [50] Object Detection and Pose Estimation from RGB and Depth Data for Real-Time, Adaptive Robotic Grasping - 2021-01-18 [34] Retina U-Net: Embarrassingly Simple Exploitation(\u7b80\u5355\u5230\u4ee4\u4eba\u5c34\u5c2c\u7684\u5229\u7528) of Segmentation Supervision for Medical Object Detection - 2020-04-30 [46] CLU-CNNs: Object detection for medical images CLU-CNNs 2019-07-20 In recent years, the real-time object detector is still developed for different edge device.For example, the development of MCUNet [49,48] and NanoDet [54] focused on producing low-power single-chip(\u4f4e\u529f\u8017\u5355\u7247\u673a) and improving the inference speed on edge CPU. As for methods such as YOLOX [21] and YOLOR [81], they focus on improving the inference speed of various GPUs.More recently, the development of real-time object detector has focused on the design of efficient architecture.As for real-time object detectors that can be used on CPU[54,88,84,83], their design is mostly based on MobileNet [28,66,27], ShuffleNet [92,55], or GhostNet [25].Another mainstream real-time object detectors are developed for GPU [81,21,97], they mostly use ResNet [26], DarkNet [63], or DLA [87], and then use the CSPNet [80] strategy to optimize the architecture.The development direction of the proposed methods in this paper are different from that of the current mainstream real-time object detectors.In addition to architecture optimization, our proposed methods will focus on the optimization of the training process.Our focus will be on some optimized modules and optimization methods which may strengthen the training cost for improving the accuracy of object detection, but without increasing the inference cost. We call the proposed modules and optimization methods trainable bag-of-freebies. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [49] MCUNet: Tiny Deep Learning on IoT Devices MCUNet 2020-07-20 [48] Memory-efficient Patch-based Inference for Tiny Deep Learning - 2021-12-06 [21] YOLOX: Exceeding(\u8d85\u51fa) YOLO Series in 2021 YOLOX 2021-07-18 [81] You Only Learn One Representaion: Unified Network for Multiple Tasks. YOLOR 2021-05-10 [88] PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices PP-PicoDet - [84] MobileDets: Searching for Object Detection Architectures for Mobile Accelerators MobileDets 2021-06-01 [83] FBNetV5: Neural Architecture Search for Multiple Tasks in One Run FBNetV5 2021-11-19 [28] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications MobileNets 2017-04-17 [66] MobileNetV2: Inverted Residuals and Linear Bottlenecks MobileNetV2 2018-01-13 [27] Searching for MobileNetV3 MobileNetV3 2019-05-06 [92] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices ShuffleNet 2017-07-04 [55] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design ShuffleNet V2 2018-09-08 [25] GhostNet: More Features from Cheap Operations GhostNet 2019-11-27 [97] Objects as Points CenterNet 2019-04-16 [26] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [63] YOLOv3: An Incremental Improvement YOLOv3 2018-04-08 [87] Deep Layer Aggregation DLA 2017-07-20 [80] CSPNet: A New Backbone that can Enhance Learning Capability of CNN CSPNet 2019-11-27 Recently, model re-parameterization [13,12,29] and dynamic label assignment [20,17,42] have become important topics in network training and object detection.Mainly after the above new concepts are proposed, the training of object detector evolves many new issues. In this paper, we will present some of the new issues we have discovered and devise(\u8bbe\u8ba1) effective methods to address them.For model reparameterization, we analyze the model re-parameterization strategies applicable to layers in different networks with the concept of gradient propagation path, and propose planned re-parameterized model.In addition, when we discover that with dynamic label assignment technology, the training of model with multiple output layers will generate new issues. That is: \"How to assign dynamic targets for the outputs of different branches?\" For this problem, we propose a new label assignment method called coarse-to-fine(\u4ece\u7c97\u5230\u7ec6) lead guided label assignment. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [13] RepVGG: Making VGG-style ConvNets Great Again RepVGG 2021-01-11 [12] Diverse Branch Block: Building a Convolution as an Inception-like Unit - 2021-06-01 [29] Online Convolutional Re-parameterization - - [20] OTA: Optimal Transport Assignment for Object Detection OTA 2021-03-26 [17] TOOD: Task-aligned One-stage Object Detection TOOD - [42] A Dual Weighting Label Assignment Scheme for Object Detection - The contributions of this paper are summarized as follows: (1)we design several trainable bag-of-freebies methods, so that real-time object detection can greatly improve the detection accuracy without increasing the inference cost; (2) for the evolution of object detection methods, we found two new issues, namely how re-parameterized module replaces original module, and how dynamic label assignment strategy deals with assignment to different output layers.In addition, we also propose methods to address the difficulties arising from these issues;(3) we propose \"extend\" and \"compound scaling\" methods for the real-time object detector that can effectively utilize parameters and computation; and (4) the method we proposed can effectively reduce about 40% parameters and 50% computation of state-of-the-art real-time object detector, and has faster inference speed and higher detection accuracy. 2 Related work 2.1 Real-time object detectors Currently state-of-the-art real-time object detectors are mainly based on YOLO [61, 62, 63] and FCOS [76, 77], which are [3, 79, 81, 21, 54, 85, 23].Being able to become a state-of-the-art real-time object detector usually requires the following characteristics:(1) a faster and stronger network architecture;(2) a more effective feature integration method [22, 97, 37, 74, 59, 30, 9, 45];(3) a more accurate detection method [76, 77, 69]; (4) a more robust loss function [96, 64, 6, 56, 95, 57]; (5) a more efficient label assignment method [99, 20, 17, 82, 42];and (6) a more efficient training method.In this paper, we do not intend to explore self-supervised learning or knowledge distillation methods that require additional data or large model.Instead, we will design new trainable bag-of-freebies method for the issues derived from the state-of-the-art methods associated with (4), (5), and (6) mentioned above. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [61] You Only Look Once: Unified, Real-Time Object Detection YOLO - [62] YOLO9000: Better, Faster, Stronger YOLO9000 2017-07-21 [63] YOLOv3: An Incremental Improvement YOLOv3 2018-04-08 [76] FCOS: Fully Convolutional One-Stage Object Detection FCOS 2019-04-02 [77] FCOS: A simple and strong anchor-free object detector FCOS 2020-06-14 [3] YOLOv4: Optimal Speed and Accuracy of Object Detection YOLOv4 2020-04-23 [79] Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4 2020-11-16 [81] You Only Learn One Representaion: Unified Network for Multiple Tasks. YOLOR 2021-05-10 [21] YOLOX: Exceeding(\u8d85\u51fa) YOLO Series in 2021 YOLOX 2021-07-18 [85] PP-YOLOE: An evolved version of YOLO PP-YOLOE - [22] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection NAS-FPN 2019-04-16 [97] Objects as Points CenterNet 2019-04-16 [37] Panoptic Feature Pyramid Networks - 2019-01-08 [74] EfficientDet: Scalable and Efficient Object Detection EfficientDet 2019-11-20 [59] DetectoRS: Detecting Objects with Recursive Feature Pyramid and Swithchable Atrous Convolution DetectoRS 2021-06-01 [30] A2-FPN: Attention Aggregation based Feature Pyramid Network for Instance Segmentation A2-FPN 2021-06-01 [9] Dynamic Head: Unifying Object Detection Heads with Attentions - 2021-06-15 [45] Exploring Plain Vision Transformer Backbones for Object Detection - - [69] Sparse R-CNN: End-to-End Object Detection with Learnable Proposals - 2021-06-01 [96] IoU Loss for 2D/3D Object Detection - - [64] Generalized Intersection over Union: A Metric and A Loss fro Bounding Box Regression - 2019-02-25 [6] AP-Loss for Accurate One-Stage Object Detection - 2021-11-01 [56] A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection - 2020-09-28 [95] Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression - 2019-11-19 [57] Rank & Sort Loss for Object Detection and Instance Segmentation - 2021-07-24 [99] AutoAssign: Differentiable Label Assignment for Dense Object Detection AutoAssign 2020-07-07 [20] OTA: Optimal Transport Assignment for Object Detection OTA 2021-03-26 [17] TOOD: Task-aligned One-stage Object Detection TOOD - [42] A Dual(\u53cc\u91cd\u7684) Weighting Label Assignment Scheme for Object Detection - [82] End-to-End Object Detection with Fully Convolutional Network - 2020-12-07 2.2 Model re-parameterization Model re-parametrization techniques [71, 31, 75, 19, 33, 11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple computational modules into one at inference stage.The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble.There are two common practices for model-level reparameterization to obtain the final inference model.One is to train multiple identical models with different training data, and then average the weights of multiple trained models.The other is to perform a weighted average of the weights of models at different iteration number.Modulelevel re-parameterization is a more popular research issue recently. This type of method splits a module into multiple identical or different module branches during training and integrates multiple branched modules into a completely equivalent module during inference.However, not all proposed re-parameterized module can be perfectly applied to different architectures. With this in mind, we have developed new re-parameterization module and designed related application strategies for various architectures. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [71] Rethinking the Inception Architecture for Computer Vision - 2015-12-02 [31] Snapshot Ensembles: Train 1, Get M for Free - 2017-04-01 [75] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results - 2017-01-01 [19] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs - 2018-02-27 [33] Averaging Weights Leads to Wider Optima and Better Generalization - 2018-03-14 [11] ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks ACNet 2019-08-11 [4] Ensemble deep learning in bioinformatics - 2020-08-17 [24] ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks ExpandNets 2018-11-26 [13] RepVGG: Making VGG-style ConvNets Great Again RepVGG 2021-01-11 [12] Diverse Branch Block: Building a Convolution as an Inception-like Unit - 2021-06-01 [10] Re-parameterizing Your Optimizers rather than Architectures - 2022-05-30 [29] Online Convolutional Re-parameterization - - [14] Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs - - [78] An Improved One millisecond Mobile Backbone - - 2.3 Model scaling Model scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way to scale up or down an already designed model and make it fit in different computing devices.The model scaling method usually uses different scaling factors, such as resolution (size of input image), depth (number of layer), width (number of channel), and stage (number of feature pyramid), so as to achieve a good trade-off for the amount of network parameters, computation, inference speed, and accuracy.Network architecture search (NAS) is one of the commonly used model scaling methods. NAS can automatically search for suitable scaling factors from search space without defining too complicated rules.The disadvantage of NAS is that it requires very expensive computation to complete the search for model scaling factors. In [15], the researcher analyzes the relationship between scaling factors and the amount of parameters and operations, trying to directly estimate some rules, and thereby obtain the scaling factors required by model scaling. Checking the literature, we found that almost all model scaling methods analyze individual scaling factor independently, and even the methods in the compound scaling category also optimized scaling factor independently.The reason for this is because most popular NAS architectures deal with scaling factors that are not very correlated.We observed that all concatenationbased models, such as DenseNet [32] or VoVNet [39], will change the input width of some layers when the depth of such models is scaled. Since the proposed architecture is concatenation-based, we have to design a new compound scaling method for this model. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [72] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks EfficientNet 2019-05-24 [60] Designing Network Design Spaces - 2020-03-30 [74] EfficientDet: Scalable and Efficient Object Detection EfficientDet 2019-11-20 [73] EfficientNetV2: Smaller Models and Faster Training EfficientNetV2 2021-04-01 [15] Fast and Accurate Model Scaling - 2021-03-11 [16] Simple Training Strategies and Model Scaling for Object Detection - 2021-06-30 [2] Revisiting ResNets: Improved Training and Scaling Strategies - 2021-12-06 [51] Swin Transformer V2: Scaling up Capacity and Resolution SwinTransformerV2 2021-11-18 [32] Densely Connected Convolutional Networks DenseNet 2016-08-25 [39] An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection VoVNet 2019-04-22 3. Architecture 3.1 Extended efficient layer aggregation networks In most of the literature on designing the efficient architectures, the main considerations are no more than(\u4e0d\u5916\u4e4e\u662f) the number of parameters, the amount of computation, and the computational density.Starting from the characteristics of memory access cost(\u5185\u5b58\u8bbf\u95ee\u6210\u672c), Ma et al. [55] also analyzed the influence of the input/output channel ratio, the number of branches of the architecture, and the element-wise operation on the network inference speed.Doll \u0301ar et al. [15] additionally considered activation when performing model scaling, that is, to put more consideration on the number of elements in the output tensors of convolutional layers.The design of CSPVoVNet [79] in Figure 2 (b) is a variation of VoVNet [39]. In addition to considering the aforementioned(\u524d\u8ff0\u7684) basic designing concerns, the architecture of CSPVoVNet [79] also analyzes the gradient path, in order to enable the weights of different layers to learn more diverse features.The gradient analysis approach described above makes inferences faster and more accurate.ELAN [1] in Figure 2 (c) considers the following design strategy \u2013 \"How to design an efficient network?.\" They came out with a conclusion: By controlling the shortest longest gradient path, a deeper network can learn and converge effectively. In this paper, we propose Extended-ELAN (E-ELAN) based on ELAN and its main architecture is shown in Figure 2 (d). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [55] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design ShuffleNet V2 2018-09-08 [15] Fast and Accurate Model Scaling - 2021-03-11 [79] Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4 2020-11-16 Regardless of(\u65e0\u8bba) the gradient path length and the stacking number of computational blocks in large-scale ELAN, it has reached a stable state.If more computational blocks are stacked unlimitedly, this stable state may be destroyed, and the parameter utilization rate will decrease.The proposed E-ELAN uses expand, shuffle, merge cardinality(\u57fa\u6570) to achieve the ability to continuously enhance the learning ability of the network without destroying the original gradient path.In terms of architecture, E-ELAN only changes the architecture in computational block, while the architecture of transition layer is completely unchanged.Our strategy is to use group convolution to expand the channel and cardinality of computational blocks.We will apply the same group parameter and channel multiplier to all the computational blocks of a computational layer.Then, the feature map calculated by each computational block will be shuffled into g groups according to the set group parameter g, and then concatenate them together. 3.2 Model scaling for concatenation-based models The main purpose of model scaling is to adjust some attributes of the model and generate models of different scales to meet the needs of different inference speeds.For example the scaling model of EfficientNet [72] considers the width, depth, and resolution.As for the scaled-YOLOv4 [79], its scaling model is to adjust the number of stages. In [15], Doll \u0301ar et al. analyzed the influence of vanilla convolution and group convolution on the amount of parameter and computation when performing width and depth scaling, and used this to design the corresponding model scaling method. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [72] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks EfficientNet 2019-05-24 [79] Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4 2020-11-16 [15] Fast and Accurate Model Scaling - 2021-03-11 The above methods are mainly used in architectures such as PlainNet or ResNet.When these architectures are in executing scaling up or scaling down, the in-degree and out-degree of each layer will not change, so we can independently analyze the impact of each scaling factor on the amount of parameters and computation.However, if these methods are applied to the concatenation-based architecture, we will find that when scaling up or scaling down is performed on depth, the in-degree of a translation layer which is immediately after a concatenation-based computational block will decrease or increase, as shown in Figure 3 (a) and (b). It can be inferred from the above phenomenon that(\u4ece\u4ee5\u4e0a\u73b0\u8c61\u4e0d\u96be\u63a8\u7406\u51fa) we cannot analyze different scaling factors separately for a concatenation-based model but must be considered together.Take scaling-up depth as an example, such an action will cause a ratio change between the input channel and output channel of a transition layer, which may lead to a decrease in the hardware usage of the model.Therefore, we must propose the corresponding compound model scaling method for a concatenation-based model.When we scale the depth factor of a computational block, we must also calculate the change of the output channel of that block.Then, we will perform width factor scaling with the same amount of change on the transition layers, and the result is shown in Figure 3 (c).Our proposed compound scaling method can maintain the properties that the model had at the initial design and maintains the optimal structure. 4. Trainable bag-of-freebies 4.1 Planned re-parameterized convolution Although RepConv [13] has achieved excellent performance on the VGG [68], when we directly apply it to ResNet [26] and DenseNet [32] and other architectures, its accuracy will be significantly reduced(\u51cf\u5c11).We use gradient flow propagation paths(\u68af\u5ea6\u6d41\u4f20\u64ad\u8def\u5f84) to analyze how re-parameterized convolution should be combined with different network.We also designed planned re-parameterized convolution accordingly. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [13] RepVGG: Making VGG-style ConvNets Great Again RepVGG 2021-01-11 [68] Very deep convolutional networks for large-scale image recognition VGG 2015-01-01 [26] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [32] Densely Connected Convolutional Networks DenseNet 2016-08-25 RepConv actually combines 3 \u00d7 3 convolution, 1 \u00d7 1convolution, and identity connection in one convolutional layer.RepConv actually combines 3 \u00d7 3 convolution, 1 \u00d7 1convolution, and identity connection in one convolutional layer. After analyzing the combination and corresponding performance of RepConv and different architectures, we find that the identity connection in RepConv destroys the residual in ResNet and the concatenation in DenseNet, which provides more diversity of gradients for different feature maps. For the above reasons, we use RepConv without identity connection (RepConvN) to design the architecture of planned re-parameterized convolution.In our thinking, when a convolutional layer with residual or concatenation is replaced by re-parameterized convolution, there should be no identity connection.Figure 4 shows an example of our designed \"planned re-parameterized convolution\" used in PlainNet and ResNet. As for the complete planned re-parameterized convolution experiment in residual-based model and concatenation-based model, it will be presented in the ablation study session. 4.2 Coarse for auxiliary(\u8f85\u52a9) and fine for lead loss Deep supervision [38] is a technique that is often used in training deep networks. Its main concept is to add extra auxiliary head in the middle layers of the network, and the shallow network weights with assistant loss as the guide.Even for architectures such as ResNet [26] and DenseNet [32] which usually converge well, deep supervision [70, 98, 67, 47, 82, 65, 86, 50] can still significantly improve the performance of the model on many tasks.Figure 5 (a) and (b) show, respectively, the object detector architecture \"without\" and \"with\" deep supervision.In this paper, we call the head responsible for the final output as the lead head, and the head used to assist\uff08\u52a9\u653b\uff09 training is called auxiliary(\u8f85\u52a9) head. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [38] Deeply-Supervised Nets - 2014-09-18 [26] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [32] Densely Connected Convolutional Networks DenseNet 2016-08-25 [70] Going Deeper with Convolutions - - [98] UNet++: A Nested U-Net Architecture for Medical image Segmentation UNet++ 2018-07-18 [67] Object Detection from Scratch with Deep Supervision DSOD 2018-09-25 [47] CBNetV2: A Composite Backbone Network Architecture for Object Detection CBNetV2 2021-07-01 [82] End-to-End Object Detection with Fully Convolutional Network POTO 2020-12-07 [65] Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity Sparse DETR 2021-11-29 [86] 3D-MAN: 3D Multi-frame Attention Network for Object Detection 3D-MAN 2021-06-01 [50] YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection YOLOStereo3D 2021-05-30 Next we want to discuss the issue of label assignment. In the past, in the training of deep network, label assignment usually refers directly to the ground truth and generate hard label according to the given rules. However, in recent years, if we take object detection as an example, researchers often use the quality and distribution of prediction output by the network, and then consider together with the ground truth to use some calculation and optimization methods to generate a reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].For example, YOLO [61] use IoU of prediction of bounding box regression and ground truth as the soft label of objectness.In this paper, we call the mechanism that considers the network prediction results together with the ground truth and then assigns soft labels as \"label assigner.\" \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [61] You Only Look Once: Unified, Real-Time Object Detection YOLO - [8] Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving Gaussian YOLOv3 2019-04-09 [36] Probabilistic Anchor Assignment with IoU Prediction for Object Detection - 2020-07-16 [99] AutoAssign: Differentiable Label Assignment for Dense Object Detection AutoAssign 2020-07-07 [91] Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection - 2019-12-05 [44] Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection - 2020-06-08 [43] Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection - 2020-11-25 [90] VarifocalNet: An IoU-aware Dense Object Detector VarifocalNet 2020-08-31 [20] OTA: Optimal Transport Assignment for Object Detection OTA 2021-03-26 [17] TOOD: Task-aligned One-stage Object Detection TOOD - [42] A Dual(\u53cc\u91cd\u7684) Weighting Label Assignment Scheme for Object Detection - Deep supervision needs to be trained on the target objectives regardless of the circumstances of auxiliary head or lead head.During the development of soft label assigner related techniques, we accidentally discovered a new derivative issue, i.e., \"How to assign soft label to auxiliary head and lead head ?\" To the best of our knowledge, the relevant literature has not explored this issue so far. The results of the most popular method at present is as shown in Figure 5 (c), which is to separate auxiliary head and lead head, and then use their own prediction results and the ground truth to execute label assignment.The method proposed in this paper is a new label assignment method that guides both auxiliary head and lead head by the lead head prediction.In other words, we use lead head prediction as guidance to generate coarse-to-fine hierarchical labels, which are used for auxiliary head and lead head learning, respectively. The two proposed deep supervision label assignment strategies are shown in Figure 5 (d) and (e), respectively. Lead head guided label assigner is mainly calculated based on the prediction result of the lead head and the ground truth, and generate soft label through the optimization process.This set of soft labels will be used as the target training model for both auxiliary head and lead head.The reason to do this is because lead head has a relatively strong learning capability, so the soft label generated from it should be more representative of the distribution and correlation between the source data and the target.Furthermore, we can view such learning as a kind of generalized residual learning. By letting the shallower auxiliary head directly learn the information that lead head has learned, lead head will be more able to focus on learning residual information that has not yet been learned. Coarse-to-fine lead head guided label assigner also used the predicted result of the lead head and the ground truth to generate soft label.However, in the process we generate two different sets of soft label, i.e., coarse label and fine label, where fine label is the same as the soft label generated by lead head guided label assigner, and coarse label is generated by allowing more grids to be treated as positive target by relaxing the constraints of the positive sample assignment process. The reason for this is that the learning ability of an auxiliary head is not as strong as that of a lead head, and in order to avoid losing the information that needs to be learned, we will focus on optimizing the recall of auxiliary head in the object detection task. As for the output of lead head, we can filter the high precision results from the high recall results as the final output.However, we must note that if the additional weight of coarse label is close to that of fine label, it may produce bad prior at final prediction.Therefore, in order to make those extra coarse positive grids have less impact, we put restrictions in the decoder, so that the extra coarse positive grids cannot produce soft label perfectly. The mechanism mentioned above allows the importance of fine label and coarse label to be dynamically adjusted during the learning process, and makes the optimizable upper bound of fine label always higher than coarse label. 4.3 Other trainable bag-of-freebies In this section we will list some trainable bag-offreebies. These freebies are some of the tricks we used in training, but the original concepts were not proposed by us.The training details of these freebies will be elaborated in the Appendix, including (1) Batch normalization in conv-bn-activation topology: This part mainly connects batch normalization layer directly to convolutional layer. The purpose of this is to integrate the mean and variance of batch normalization into the bias and weight of convolutional layer at the inference stage.(2) Implicit knowledge in YOLOR [81] combined with convolution feature map in addition and multiplication manner: Implicit knowledge in YOLOR can be simplified to a vector by pre-computing at the inference stage. This vector can be combined with the bias and weight of the previous or subsequent convolutional layer. (3) EMA model: EMA is a technique used in mean teacher [75], and in our system we use EMA model purely as the final inference model. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [81] You Only Learn One Representaion: Unified Network for Multiple Tasks. YOLOR 2021-05-10 [75] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results - 2017-01-01","title":"Yolov7"},{"location":"object_detection/yolov7/#abstract","text":"YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100 . YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy.Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.","title":"Abstract"},{"location":"object_detection/yolov7/#1-introduction","text":"Real-time object detection is a very important topic in computer vision, as it is often a necessary component in computer vision systems. For example, multi-object tracking[94,93], autonomous driving [40,18], robotics [35,58], medical image analysis [34,46], etc. The computing devices that execute real-time object detection is usually some mobile CPU or GPU, as well as various neural processing units (NPU) developed by major manufacturers. For example, the Apple neural engine (Apple), the neural compute stick (Intel), Jetson AI edge devices (Nvidia), the edge TPU (Google), the neural processing engine (Qualcomm), the AI processing unit (MediaTek), and the AI SoCs (Kneron), are all NPUs.Some of the above mentioned edge devices focus on speeding up different operations such as vanilla convolution, depth-wise convolution, or MLP operations.In this paper, the real-time object detector we proposed mainly hopes that it can support both mobile GPU and GPU devices from the edge to the cloud. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [94] FairMOT: On the Fainess of Detection and Re-identification in Multiple Object Tracking FairMOT 2020-04-04 [93] ByteTrack: Multi-Object Tracking by Associating Every Detection Box ByteTrack - [40] GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving GS3D 2019-03-26 [18] Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges - 2021-03-01 [35] Object Detection Approach for Robot Grasp(\u6293\u4f4f) Detection - 2019-05-20 [50] Object Detection and Pose Estimation from RGB and Depth Data for Real-Time, Adaptive Robotic Grasping - 2021-01-18 [34] Retina U-Net: Embarrassingly Simple Exploitation(\u7b80\u5355\u5230\u4ee4\u4eba\u5c34\u5c2c\u7684\u5229\u7528) of Segmentation Supervision for Medical Object Detection - 2020-04-30 [46] CLU-CNNs: Object detection for medical images CLU-CNNs 2019-07-20 In recent years, the real-time object detector is still developed for different edge device.For example, the development of MCUNet [49,48] and NanoDet [54] focused on producing low-power single-chip(\u4f4e\u529f\u8017\u5355\u7247\u673a) and improving the inference speed on edge CPU. As for methods such as YOLOX [21] and YOLOR [81], they focus on improving the inference speed of various GPUs.More recently, the development of real-time object detector has focused on the design of efficient architecture.As for real-time object detectors that can be used on CPU[54,88,84,83], their design is mostly based on MobileNet [28,66,27], ShuffleNet [92,55], or GhostNet [25].Another mainstream real-time object detectors are developed for GPU [81,21,97], they mostly use ResNet [26], DarkNet [63], or DLA [87], and then use the CSPNet [80] strategy to optimize the architecture.The development direction of the proposed methods in this paper are different from that of the current mainstream real-time object detectors.In addition to architecture optimization, our proposed methods will focus on the optimization of the training process.Our focus will be on some optimized modules and optimization methods which may strengthen the training cost for improving the accuracy of object detection, but without increasing the inference cost. We call the proposed modules and optimization methods trainable bag-of-freebies. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [49] MCUNet: Tiny Deep Learning on IoT Devices MCUNet 2020-07-20 [48] Memory-efficient Patch-based Inference for Tiny Deep Learning - 2021-12-06 [21] YOLOX: Exceeding(\u8d85\u51fa) YOLO Series in 2021 YOLOX 2021-07-18 [81] You Only Learn One Representaion: Unified Network for Multiple Tasks. YOLOR 2021-05-10 [88] PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices PP-PicoDet - [84] MobileDets: Searching for Object Detection Architectures for Mobile Accelerators MobileDets 2021-06-01 [83] FBNetV5: Neural Architecture Search for Multiple Tasks in One Run FBNetV5 2021-11-19 [28] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications MobileNets 2017-04-17 [66] MobileNetV2: Inverted Residuals and Linear Bottlenecks MobileNetV2 2018-01-13 [27] Searching for MobileNetV3 MobileNetV3 2019-05-06 [92] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices ShuffleNet 2017-07-04 [55] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design ShuffleNet V2 2018-09-08 [25] GhostNet: More Features from Cheap Operations GhostNet 2019-11-27 [97] Objects as Points CenterNet 2019-04-16 [26] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [63] YOLOv3: An Incremental Improvement YOLOv3 2018-04-08 [87] Deep Layer Aggregation DLA 2017-07-20 [80] CSPNet: A New Backbone that can Enhance Learning Capability of CNN CSPNet 2019-11-27 Recently, model re-parameterization [13,12,29] and dynamic label assignment [20,17,42] have become important topics in network training and object detection.Mainly after the above new concepts are proposed, the training of object detector evolves many new issues. In this paper, we will present some of the new issues we have discovered and devise(\u8bbe\u8ba1) effective methods to address them.For model reparameterization, we analyze the model re-parameterization strategies applicable to layers in different networks with the concept of gradient propagation path, and propose planned re-parameterized model.In addition, when we discover that with dynamic label assignment technology, the training of model with multiple output layers will generate new issues. That is: \"How to assign dynamic targets for the outputs of different branches?\" For this problem, we propose a new label assignment method called coarse-to-fine(\u4ece\u7c97\u5230\u7ec6) lead guided label assignment. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [13] RepVGG: Making VGG-style ConvNets Great Again RepVGG 2021-01-11 [12] Diverse Branch Block: Building a Convolution as an Inception-like Unit - 2021-06-01 [29] Online Convolutional Re-parameterization - - [20] OTA: Optimal Transport Assignment for Object Detection OTA 2021-03-26 [17] TOOD: Task-aligned One-stage Object Detection TOOD - [42] A Dual Weighting Label Assignment Scheme for Object Detection - The contributions of this paper are summarized as follows: (1)we design several trainable bag-of-freebies methods, so that real-time object detection can greatly improve the detection accuracy without increasing the inference cost; (2) for the evolution of object detection methods, we found two new issues, namely how re-parameterized module replaces original module, and how dynamic label assignment strategy deals with assignment to different output layers.In addition, we also propose methods to address the difficulties arising from these issues;(3) we propose \"extend\" and \"compound scaling\" methods for the real-time object detector that can effectively utilize parameters and computation; and (4) the method we proposed can effectively reduce about 40% parameters and 50% computation of state-of-the-art real-time object detector, and has faster inference speed and higher detection accuracy.","title":"1 Introduction"},{"location":"object_detection/yolov7/#2-related-work","text":"","title":"2 Related work"},{"location":"object_detection/yolov7/#21-real-time-object-detectors","text":"Currently state-of-the-art real-time object detectors are mainly based on YOLO [61, 62, 63] and FCOS [76, 77], which are [3, 79, 81, 21, 54, 85, 23].Being able to become a state-of-the-art real-time object detector usually requires the following characteristics:(1) a faster and stronger network architecture;(2) a more effective feature integration method [22, 97, 37, 74, 59, 30, 9, 45];(3) a more accurate detection method [76, 77, 69]; (4) a more robust loss function [96, 64, 6, 56, 95, 57]; (5) a more efficient label assignment method [99, 20, 17, 82, 42];and (6) a more efficient training method.In this paper, we do not intend to explore self-supervised learning or knowledge distillation methods that require additional data or large model.Instead, we will design new trainable bag-of-freebies method for the issues derived from the state-of-the-art methods associated with (4), (5), and (6) mentioned above. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [61] You Only Look Once: Unified, Real-Time Object Detection YOLO - [62] YOLO9000: Better, Faster, Stronger YOLO9000 2017-07-21 [63] YOLOv3: An Incremental Improvement YOLOv3 2018-04-08 [76] FCOS: Fully Convolutional One-Stage Object Detection FCOS 2019-04-02 [77] FCOS: A simple and strong anchor-free object detector FCOS 2020-06-14 [3] YOLOv4: Optimal Speed and Accuracy of Object Detection YOLOv4 2020-04-23 [79] Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4 2020-11-16 [81] You Only Learn One Representaion: Unified Network for Multiple Tasks. YOLOR 2021-05-10 [21] YOLOX: Exceeding(\u8d85\u51fa) YOLO Series in 2021 YOLOX 2021-07-18 [85] PP-YOLOE: An evolved version of YOLO PP-YOLOE - [22] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection NAS-FPN 2019-04-16 [97] Objects as Points CenterNet 2019-04-16 [37] Panoptic Feature Pyramid Networks - 2019-01-08 [74] EfficientDet: Scalable and Efficient Object Detection EfficientDet 2019-11-20 [59] DetectoRS: Detecting Objects with Recursive Feature Pyramid and Swithchable Atrous Convolution DetectoRS 2021-06-01 [30] A2-FPN: Attention Aggregation based Feature Pyramid Network for Instance Segmentation A2-FPN 2021-06-01 [9] Dynamic Head: Unifying Object Detection Heads with Attentions - 2021-06-15 [45] Exploring Plain Vision Transformer Backbones for Object Detection - - [69] Sparse R-CNN: End-to-End Object Detection with Learnable Proposals - 2021-06-01 [96] IoU Loss for 2D/3D Object Detection - - [64] Generalized Intersection over Union: A Metric and A Loss fro Bounding Box Regression - 2019-02-25 [6] AP-Loss for Accurate One-Stage Object Detection - 2021-11-01 [56] A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection - 2020-09-28 [95] Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression - 2019-11-19 [57] Rank & Sort Loss for Object Detection and Instance Segmentation - 2021-07-24 [99] AutoAssign: Differentiable Label Assignment for Dense Object Detection AutoAssign 2020-07-07 [20] OTA: Optimal Transport Assignment for Object Detection OTA 2021-03-26 [17] TOOD: Task-aligned One-stage Object Detection TOOD - [42] A Dual(\u53cc\u91cd\u7684) Weighting Label Assignment Scheme for Object Detection - [82] End-to-End Object Detection with Fully Convolutional Network - 2020-12-07","title":"2.1 Real-time object detectors"},{"location":"object_detection/yolov7/#22-model-re-parameterization","text":"Model re-parametrization techniques [71, 31, 75, 19, 33, 11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple computational modules into one at inference stage.The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble.There are two common practices for model-level reparameterization to obtain the final inference model.One is to train multiple identical models with different training data, and then average the weights of multiple trained models.The other is to perform a weighted average of the weights of models at different iteration number.Modulelevel re-parameterization is a more popular research issue recently. This type of method splits a module into multiple identical or different module branches during training and integrates multiple branched modules into a completely equivalent module during inference.However, not all proposed re-parameterized module can be perfectly applied to different architectures. With this in mind, we have developed new re-parameterization module and designed related application strategies for various architectures. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [71] Rethinking the Inception Architecture for Computer Vision - 2015-12-02 [31] Snapshot Ensembles: Train 1, Get M for Free - 2017-04-01 [75] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results - 2017-01-01 [19] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs - 2018-02-27 [33] Averaging Weights Leads to Wider Optima and Better Generalization - 2018-03-14 [11] ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks ACNet 2019-08-11 [4] Ensemble deep learning in bioinformatics - 2020-08-17 [24] ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks ExpandNets 2018-11-26 [13] RepVGG: Making VGG-style ConvNets Great Again RepVGG 2021-01-11 [12] Diverse Branch Block: Building a Convolution as an Inception-like Unit - 2021-06-01 [10] Re-parameterizing Your Optimizers rather than Architectures - 2022-05-30 [29] Online Convolutional Re-parameterization - - [14] Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs - - [78] An Improved One millisecond Mobile Backbone - -","title":"2.2 Model re-parameterization"},{"location":"object_detection/yolov7/#23-model-scaling","text":"Model scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way to scale up or down an already designed model and make it fit in different computing devices.The model scaling method usually uses different scaling factors, such as resolution (size of input image), depth (number of layer), width (number of channel), and stage (number of feature pyramid), so as to achieve a good trade-off for the amount of network parameters, computation, inference speed, and accuracy.Network architecture search (NAS) is one of the commonly used model scaling methods. NAS can automatically search for suitable scaling factors from search space without defining too complicated rules.The disadvantage of NAS is that it requires very expensive computation to complete the search for model scaling factors. In [15], the researcher analyzes the relationship between scaling factors and the amount of parameters and operations, trying to directly estimate some rules, and thereby obtain the scaling factors required by model scaling. Checking the literature, we found that almost all model scaling methods analyze individual scaling factor independently, and even the methods in the compound scaling category also optimized scaling factor independently.The reason for this is because most popular NAS architectures deal with scaling factors that are not very correlated.We observed that all concatenationbased models, such as DenseNet [32] or VoVNet [39], will change the input width of some layers when the depth of such models is scaled. Since the proposed architecture is concatenation-based, we have to design a new compound scaling method for this model. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [72] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks EfficientNet 2019-05-24 [60] Designing Network Design Spaces - 2020-03-30 [74] EfficientDet: Scalable and Efficient Object Detection EfficientDet 2019-11-20 [73] EfficientNetV2: Smaller Models and Faster Training EfficientNetV2 2021-04-01 [15] Fast and Accurate Model Scaling - 2021-03-11 [16] Simple Training Strategies and Model Scaling for Object Detection - 2021-06-30 [2] Revisiting ResNets: Improved Training and Scaling Strategies - 2021-12-06 [51] Swin Transformer V2: Scaling up Capacity and Resolution SwinTransformerV2 2021-11-18 [32] Densely Connected Convolutional Networks DenseNet 2016-08-25 [39] An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection VoVNet 2019-04-22","title":"2.3 Model scaling"},{"location":"object_detection/yolov7/#3-architecture","text":"","title":"3. Architecture"},{"location":"object_detection/yolov7/#31-extended-efficient-layer-aggregation-networks","text":"In most of the literature on designing the efficient architectures, the main considerations are no more than(\u4e0d\u5916\u4e4e\u662f) the number of parameters, the amount of computation, and the computational density.Starting from the characteristics of memory access cost(\u5185\u5b58\u8bbf\u95ee\u6210\u672c), Ma et al. [55] also analyzed the influence of the input/output channel ratio, the number of branches of the architecture, and the element-wise operation on the network inference speed.Doll \u0301ar et al. [15] additionally considered activation when performing model scaling, that is, to put more consideration on the number of elements in the output tensors of convolutional layers.The design of CSPVoVNet [79] in Figure 2 (b) is a variation of VoVNet [39]. In addition to considering the aforementioned(\u524d\u8ff0\u7684) basic designing concerns, the architecture of CSPVoVNet [79] also analyzes the gradient path, in order to enable the weights of different layers to learn more diverse features.The gradient analysis approach described above makes inferences faster and more accurate.ELAN [1] in Figure 2 (c) considers the following design strategy \u2013 \"How to design an efficient network?.\" They came out with a conclusion: By controlling the shortest longest gradient path, a deeper network can learn and converge effectively. In this paper, we propose Extended-ELAN (E-ELAN) based on ELAN and its main architecture is shown in Figure 2 (d). \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [55] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design ShuffleNet V2 2018-09-08 [15] Fast and Accurate Model Scaling - 2021-03-11 [79] Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4 2020-11-16 Regardless of(\u65e0\u8bba) the gradient path length and the stacking number of computational blocks in large-scale ELAN, it has reached a stable state.If more computational blocks are stacked unlimitedly, this stable state may be destroyed, and the parameter utilization rate will decrease.The proposed E-ELAN uses expand, shuffle, merge cardinality(\u57fa\u6570) to achieve the ability to continuously enhance the learning ability of the network without destroying the original gradient path.In terms of architecture, E-ELAN only changes the architecture in computational block, while the architecture of transition layer is completely unchanged.Our strategy is to use group convolution to expand the channel and cardinality of computational blocks.We will apply the same group parameter and channel multiplier to all the computational blocks of a computational layer.Then, the feature map calculated by each computational block will be shuffled into g groups according to the set group parameter g, and then concatenate them together.","title":"3.1 Extended efficient layer aggregation networks"},{"location":"object_detection/yolov7/#32-model-scaling-for-concatenation-based-models","text":"The main purpose of model scaling is to adjust some attributes of the model and generate models of different scales to meet the needs of different inference speeds.For example the scaling model of EfficientNet [72] considers the width, depth, and resolution.As for the scaled-YOLOv4 [79], its scaling model is to adjust the number of stages. In [15], Doll \u0301ar et al. analyzed the influence of vanilla convolution and group convolution on the amount of parameter and computation when performing width and depth scaling, and used this to design the corresponding model scaling method. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [72] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks EfficientNet 2019-05-24 [79] Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4 2020-11-16 [15] Fast and Accurate Model Scaling - 2021-03-11 The above methods are mainly used in architectures such as PlainNet or ResNet.When these architectures are in executing scaling up or scaling down, the in-degree and out-degree of each layer will not change, so we can independently analyze the impact of each scaling factor on the amount of parameters and computation.However, if these methods are applied to the concatenation-based architecture, we will find that when scaling up or scaling down is performed on depth, the in-degree of a translation layer which is immediately after a concatenation-based computational block will decrease or increase, as shown in Figure 3 (a) and (b). It can be inferred from the above phenomenon that(\u4ece\u4ee5\u4e0a\u73b0\u8c61\u4e0d\u96be\u63a8\u7406\u51fa) we cannot analyze different scaling factors separately for a concatenation-based model but must be considered together.Take scaling-up depth as an example, such an action will cause a ratio change between the input channel and output channel of a transition layer, which may lead to a decrease in the hardware usage of the model.Therefore, we must propose the corresponding compound model scaling method for a concatenation-based model.When we scale the depth factor of a computational block, we must also calculate the change of the output channel of that block.Then, we will perform width factor scaling with the same amount of change on the transition layers, and the result is shown in Figure 3 (c).Our proposed compound scaling method can maintain the properties that the model had at the initial design and maintains the optimal structure.","title":"3.2 Model scaling for concatenation-based models"},{"location":"object_detection/yolov7/#4-trainable-bag-of-freebies","text":"","title":"4. Trainable bag-of-freebies"},{"location":"object_detection/yolov7/#41-planned-re-parameterized-convolution","text":"Although RepConv [13] has achieved excellent performance on the VGG [68], when we directly apply it to ResNet [26] and DenseNet [32] and other architectures, its accuracy will be significantly reduced(\u51cf\u5c11).We use gradient flow propagation paths(\u68af\u5ea6\u6d41\u4f20\u64ad\u8def\u5f84) to analyze how re-parameterized convolution should be combined with different network.We also designed planned re-parameterized convolution accordingly. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [13] RepVGG: Making VGG-style ConvNets Great Again RepVGG 2021-01-11 [68] Very deep convolutional networks for large-scale image recognition VGG 2015-01-01 [26] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [32] Densely Connected Convolutional Networks DenseNet 2016-08-25 RepConv actually combines 3 \u00d7 3 convolution, 1 \u00d7 1convolution, and identity connection in one convolutional layer.RepConv actually combines 3 \u00d7 3 convolution, 1 \u00d7 1convolution, and identity connection in one convolutional layer. After analyzing the combination and corresponding performance of RepConv and different architectures, we find that the identity connection in RepConv destroys the residual in ResNet and the concatenation in DenseNet, which provides more diversity of gradients for different feature maps. For the above reasons, we use RepConv without identity connection (RepConvN) to design the architecture of planned re-parameterized convolution.In our thinking, when a convolutional layer with residual or concatenation is replaced by re-parameterized convolution, there should be no identity connection.Figure 4 shows an example of our designed \"planned re-parameterized convolution\" used in PlainNet and ResNet. As for the complete planned re-parameterized convolution experiment in residual-based model and concatenation-based model, it will be presented in the ablation study session.","title":"4.1 Planned re-parameterized convolution"},{"location":"object_detection/yolov7/#42-coarse-for-auxiliary-and-fine-for-lead-loss","text":"Deep supervision [38] is a technique that is often used in training deep networks. Its main concept is to add extra auxiliary head in the middle layers of the network, and the shallow network weights with assistant loss as the guide.Even for architectures such as ResNet [26] and DenseNet [32] which usually converge well, deep supervision [70, 98, 67, 47, 82, 65, 86, 50] can still significantly improve the performance of the model on many tasks.Figure 5 (a) and (b) show, respectively, the object detector architecture \"without\" and \"with\" deep supervision.In this paper, we call the head responsible for the final output as the lead head, and the head used to assist\uff08\u52a9\u653b\uff09 training is called auxiliary(\u8f85\u52a9) head. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [38] Deeply-Supervised Nets - 2014-09-18 [26] Deep Residual Learning for Image Recognition ResNet 2015-12-10 [32] Densely Connected Convolutional Networks DenseNet 2016-08-25 [70] Going Deeper with Convolutions - - [98] UNet++: A Nested U-Net Architecture for Medical image Segmentation UNet++ 2018-07-18 [67] Object Detection from Scratch with Deep Supervision DSOD 2018-09-25 [47] CBNetV2: A Composite Backbone Network Architecture for Object Detection CBNetV2 2021-07-01 [82] End-to-End Object Detection with Fully Convolutional Network POTO 2020-12-07 [65] Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity Sparse DETR 2021-11-29 [86] 3D-MAN: 3D Multi-frame Attention Network for Object Detection 3D-MAN 2021-06-01 [50] YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection YOLOStereo3D 2021-05-30 Next we want to discuss the issue of label assignment. In the past, in the training of deep network, label assignment usually refers directly to the ground truth and generate hard label according to the given rules. However, in recent years, if we take object detection as an example, researchers often use the quality and distribution of prediction output by the network, and then consider together with the ground truth to use some calculation and optimization methods to generate a reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].For example, YOLO [61] use IoU of prediction of bounding box regression and ground truth as the soft label of objectness.In this paper, we call the mechanism that considers the network prediction results together with the ground truth and then assigns soft labels as \"label assigner.\" \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [61] You Only Look Once: Unified, Real-Time Object Detection YOLO - [8] Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving Gaussian YOLOv3 2019-04-09 [36] Probabilistic Anchor Assignment with IoU Prediction for Object Detection - 2020-07-16 [99] AutoAssign: Differentiable Label Assignment for Dense Object Detection AutoAssign 2020-07-07 [91] Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection - 2019-12-05 [44] Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection - 2020-06-08 [43] Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection - 2020-11-25 [90] VarifocalNet: An IoU-aware Dense Object Detector VarifocalNet 2020-08-31 [20] OTA: Optimal Transport Assignment for Object Detection OTA 2021-03-26 [17] TOOD: Task-aligned One-stage Object Detection TOOD - [42] A Dual(\u53cc\u91cd\u7684) Weighting Label Assignment Scheme for Object Detection - Deep supervision needs to be trained on the target objectives regardless of the circumstances of auxiliary head or lead head.During the development of soft label assigner related techniques, we accidentally discovered a new derivative issue, i.e., \"How to assign soft label to auxiliary head and lead head ?\" To the best of our knowledge, the relevant literature has not explored this issue so far. The results of the most popular method at present is as shown in Figure 5 (c), which is to separate auxiliary head and lead head, and then use their own prediction results and the ground truth to execute label assignment.The method proposed in this paper is a new label assignment method that guides both auxiliary head and lead head by the lead head prediction.In other words, we use lead head prediction as guidance to generate coarse-to-fine hierarchical labels, which are used for auxiliary head and lead head learning, respectively. The two proposed deep supervision label assignment strategies are shown in Figure 5 (d) and (e), respectively. Lead head guided label assigner is mainly calculated based on the prediction result of the lead head and the ground truth, and generate soft label through the optimization process.This set of soft labels will be used as the target training model for both auxiliary head and lead head.The reason to do this is because lead head has a relatively strong learning capability, so the soft label generated from it should be more representative of the distribution and correlation between the source data and the target.Furthermore, we can view such learning as a kind of generalized residual learning. By letting the shallower auxiliary head directly learn the information that lead head has learned, lead head will be more able to focus on learning residual information that has not yet been learned. Coarse-to-fine lead head guided label assigner also used the predicted result of the lead head and the ground truth to generate soft label.However, in the process we generate two different sets of soft label, i.e., coarse label and fine label, where fine label is the same as the soft label generated by lead head guided label assigner, and coarse label is generated by allowing more grids to be treated as positive target by relaxing the constraints of the positive sample assignment process. The reason for this is that the learning ability of an auxiliary head is not as strong as that of a lead head, and in order to avoid losing the information that needs to be learned, we will focus on optimizing the recall of auxiliary head in the object detection task. As for the output of lead head, we can filter the high precision results from the high recall results as the final output.However, we must note that if the additional weight of coarse label is close to that of fine label, it may produce bad prior at final prediction.Therefore, in order to make those extra coarse positive grids have less impact, we put restrictions in the decoder, so that the extra coarse positive grids cannot produce soft label perfectly. The mechanism mentioned above allows the importance of fine label and coarse label to be dynamically adjusted during the learning process, and makes the optimizable upper bound of fine label always higher than coarse label.","title":"4.2 Coarse for auxiliary(\u8f85\u52a9) and fine for lead loss"},{"location":"object_detection/yolov7/#43-other-trainable-bag-of-freebies","text":"In this section we will list some trainable bag-offreebies. These freebies are some of the tricks we used in training, but the original concepts were not proposed by us.The training details of these freebies will be elaborated in the Appendix, including (1) Batch normalization in conv-bn-activation topology: This part mainly connects batch normalization layer directly to convolutional layer. The purpose of this is to integrate the mean and variance of batch normalization into the bias and weight of convolutional layer at the inference stage.(2) Implicit knowledge in YOLOR [81] combined with convolution feature map in addition and multiplication manner: Implicit knowledge in YOLOR can be simplified to a vector by pre-computing at the inference stage. This vector can be combined with the bias and weight of the previous or subsequent convolutional layer. (3) EMA model: EMA is a technique used in mean teacher [75], and in our system we use EMA model purely as the final inference model. \u8bba\u6587\u540d\u79f0 \u8bba\u6587\u522b\u540d \u8bba\u6587\u65f6\u95f4 [81] You Only Learn One Representaion: Unified Network for Multiple Tasks. YOLOR 2021-05-10 [75] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results - 2017-01-01","title":"4.3 Other trainable bag-of-freebies"},{"location":"object_detection/linsh/GettingStarted/","text":"","title":"GettingStarted"},{"location":"object_detection/linsh/Path/","text":"\u8bf4\u660e Path\u7684\u4f7f\u7528\u8bf4\u660e\uff0c\u53ef\u4ee5\u505a\u5230\u4e0e\u7cfb\u7edf\u65e0\u5173 from pathlib import Path FILE = Path(__file__).resolve() print(FILE) # <PosixPath.parents> print(FILE.parents) # <PosixPath.parents> for x in FILE.parents: print(x) print(FILE.parents[1]) \u5728\u76ee\u5f55 /Users/cuidonglin/Documents/myyolov5/interproject/ \u4e0b\u8fd0\u884c python test.py \u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\uff1a /Users/cuidonglin/Documents/myyolov5/interproject/test.py <PosixPath.parents> /Users/cuidonglin/Documents/myyolov5/interproject [0] /Users/cuidonglin/Documents/myyolov5 [1] /Users/cuidonglin/Documents [2] /Users/cuidonglin [3] /Users [4] / /Users/cuidonglin/Documents/myyolov5 [1] \u5c0f\u547d\u4ee4 \u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728\uff1a file.exists() # \u5176\u4e2d file \u662fPath\u5bf9\u8c61\u5373\u53ef \u6253\u5f00\u6587\u4ef6\uff1a with file.open() as f: \u521b\u5efa\u6587\u4ef6\u5939\uff1a tmp = Path(ROOT/'tmp') tmp.mkdir(parents=True, exist_ok=True) # exist_ok=True\u53ef\u4ee5\u4f7f\u5f97tmp\u5df2\u7ecf\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u4e0d\u62a5\u9519\uff0c\u4e5f\u4e0d\u8986\u76d6\u539f\u6587\u4ef6\u5939 \u6587\u4ef6\u7684\u66f4\u65b0\u65f6\u95f4 from datetime import datetime t = datetime.fromtimestamp(Path(ROOT/'test.py').stat().st_mtime) print(f'{t.year}-{t.month}-{t.day}')","title":"Path"},{"location":"object_detection/linsh/Path/#_1","text":"Path\u7684\u4f7f\u7528\u8bf4\u660e\uff0c\u53ef\u4ee5\u505a\u5230\u4e0e\u7cfb\u7edf\u65e0\u5173 from pathlib import Path FILE = Path(__file__).resolve() print(FILE) # <PosixPath.parents> print(FILE.parents) # <PosixPath.parents> for x in FILE.parents: print(x) print(FILE.parents[1]) \u5728\u76ee\u5f55 /Users/cuidonglin/Documents/myyolov5/interproject/ \u4e0b\u8fd0\u884c python test.py \u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\uff1a /Users/cuidonglin/Documents/myyolov5/interproject/test.py <PosixPath.parents> /Users/cuidonglin/Documents/myyolov5/interproject [0] /Users/cuidonglin/Documents/myyolov5 [1] /Users/cuidonglin/Documents [2] /Users/cuidonglin [3] /Users [4] / /Users/cuidonglin/Documents/myyolov5 [1]","title":"\u8bf4\u660e"},{"location":"object_detection/linsh/Path/#_2","text":"\u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728\uff1a file.exists() # \u5176\u4e2d file \u662fPath\u5bf9\u8c61\u5373\u53ef \u6253\u5f00\u6587\u4ef6\uff1a with file.open() as f: \u521b\u5efa\u6587\u4ef6\u5939\uff1a tmp = Path(ROOT/'tmp') tmp.mkdir(parents=True, exist_ok=True) # exist_ok=True\u53ef\u4ee5\u4f7f\u5f97tmp\u5df2\u7ecf\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u4e0d\u62a5\u9519\uff0c\u4e5f\u4e0d\u8986\u76d6\u539f\u6587\u4ef6\u5939 \u6587\u4ef6\u7684\u66f4\u65b0\u65f6\u95f4 from datetime import datetime t = datetime.fromtimestamp(Path(ROOT/'test.py').stat().st_mtime) print(f'{t.year}-{t.month}-{t.day}')","title":"\u5c0f\u547d\u4ee4"},{"location":"object_detection/linsh/base_func/","text":"\u8bf4\u660e \u672c\u7bc7\u4e3b\u8981\u8bb0\u5f55\u8ddf\u4e0d\u518d\u8c03\u7528\u5176\u4ed6\u51fd\u6570\u7684\u539f\u5b50\u51fd\u6570 1. colorstr \u51fd\u6570 \u51fa\u73b0\u7684\u6587\u4ef6\uff1a def colorstr(*input): # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e. colorstr('blue', 'hello world') *args, string = input if len(input) > 1 else ('blue', 'bold', input[0]) # color arguments, string colors = { 'black': '\\033[30m', # basic colors 'red': '\\033[31m', 'green': '\\033[32m', 'yellow': '\\033[33m', 'blue': '\\033[34m', 'magenta': '\\033[35m', 'cyan': '\\033[36m', 'white': '\\033[37m', 'bright_black': '\\033[90m', # bright colors 'bright_red': '\\033[91m', 'bright_green': '\\033[92m', 'bright_yellow': '\\033[93m', 'bright_blue': '\\033[94m', 'bright_magenta': '\\033[95m', 'bright_cyan': '\\033[96m', 'bright_white': '\\033[97m', 'end': '\\033[0m', # misc 'bold': '\\033[1m', 'underline': '\\033[4m'} return ''.join(colors[x] for x in args) + f'{string}' + colors['end'] if __name__ == \"__main__\": prefix = colorstr('red', 'bold', 'requirements:') print(prefix) \u5728\u7ec8\u7aef\u770b\u5230\u7684 \u5b57\u7b26\u4e32 \"requirements:\"\u662f\u7ea2\u8272\u3001\u7c97\u4f53 2. python\u73af\u5883\u68c0\u67e5 import pkg_resources as pkg import platform print(platform.python_version()) # \u83b7\u53d6\u5f53\u524d\u4f7f\u7528\u7684python\u7248\u672c pkg.parse_version(x) # \u4e0d\u786e\u5b9a\u4ec0\u4e48\u610f\u601d # \u53ef\u4ee5\u83b7\u53d6\u9700\u8981\u5b89\u88c5\u7684\u5305 with file.open() as f: pkg.parse_requirements(f) # f\u662f Path('requirements.txt) python setuptools\u4e4bpkg_resources\u6a21\u5757 3, \u68c0\u67e5\u662f\u5426\u662fkaggle\u73af\u5883 def is_kaggle(): # Is environment a Kaggle Notebook? try: assert os.environ.get('PWD') == '/kaggle/working' assert os.environ.get('KAGGLE_URL_BASE') == 'https://www.kaggle.com' return True except AssertionError: return False \u83b7\u53d6\u5f53\u524d\u8def\u5f84\uff1a os.environ.get('PWD') \u65ad\u8a00\u4e0d\u5b58\u5728\u95ee\u9898\u8fd4\u56deTrue\uff0c\u6709\u95ee\u9898\u8fd4\u56deFalse: try except AssertionError","title":"Base func"},{"location":"object_detection/linsh/base_func/#_1","text":"\u672c\u7bc7\u4e3b\u8981\u8bb0\u5f55\u8ddf\u4e0d\u518d\u8c03\u7528\u5176\u4ed6\u51fd\u6570\u7684\u539f\u5b50\u51fd\u6570","title":"\u8bf4\u660e"},{"location":"object_detection/linsh/base_func/#1-colorstr","text":"\u51fa\u73b0\u7684\u6587\u4ef6\uff1a def colorstr(*input): # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e. colorstr('blue', 'hello world') *args, string = input if len(input) > 1 else ('blue', 'bold', input[0]) # color arguments, string colors = { 'black': '\\033[30m', # basic colors 'red': '\\033[31m', 'green': '\\033[32m', 'yellow': '\\033[33m', 'blue': '\\033[34m', 'magenta': '\\033[35m', 'cyan': '\\033[36m', 'white': '\\033[37m', 'bright_black': '\\033[90m', # bright colors 'bright_red': '\\033[91m', 'bright_green': '\\033[92m', 'bright_yellow': '\\033[93m', 'bright_blue': '\\033[94m', 'bright_magenta': '\\033[95m', 'bright_cyan': '\\033[96m', 'bright_white': '\\033[97m', 'end': '\\033[0m', # misc 'bold': '\\033[1m', 'underline': '\\033[4m'} return ''.join(colors[x] for x in args) + f'{string}' + colors['end'] if __name__ == \"__main__\": prefix = colorstr('red', 'bold', 'requirements:') print(prefix) \u5728\u7ec8\u7aef\u770b\u5230\u7684 \u5b57\u7b26\u4e32 \"requirements:\"\u662f\u7ea2\u8272\u3001\u7c97\u4f53","title":"1. colorstr\u51fd\u6570"},{"location":"object_detection/linsh/base_func/#2-python","text":"import pkg_resources as pkg import platform print(platform.python_version()) # \u83b7\u53d6\u5f53\u524d\u4f7f\u7528\u7684python\u7248\u672c pkg.parse_version(x) # \u4e0d\u786e\u5b9a\u4ec0\u4e48\u610f\u601d # \u53ef\u4ee5\u83b7\u53d6\u9700\u8981\u5b89\u88c5\u7684\u5305 with file.open() as f: pkg.parse_requirements(f) # f\u662f Path('requirements.txt) python setuptools\u4e4bpkg_resources\u6a21\u5757","title":"2. python\u73af\u5883\u68c0\u67e5"},{"location":"object_detection/linsh/base_func/#3-kaggle","text":"def is_kaggle(): # Is environment a Kaggle Notebook? try: assert os.environ.get('PWD') == '/kaggle/working' assert os.environ.get('KAGGLE_URL_BASE') == 'https://www.kaggle.com' return True except AssertionError: return False \u83b7\u53d6\u5f53\u524d\u8def\u5f84\uff1a os.environ.get('PWD') \u65ad\u8a00\u4e0d\u5b58\u5728\u95ee\u9898\u8fd4\u56deTrue\uff0c\u6709\u95ee\u9898\u8fd4\u56deFalse: try except AssertionError","title":"3, \u68c0\u67e5\u662f\u5426\u662fkaggle\u73af\u5883"},{"location":"object_detection/linsh/log/","text":"\u8bf4\u660e \u672c\u7bc7\u4e3b\u8981\u8bb0\u5f55\u8ddf\u65e5\u5fd7\u76f8\u5173\u7684\u5185\u5bb9 \u5c0f\u547d\u4ee4 import logging print(logging.WARNING) # 30 \u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u662f30 print(logging.INFO) # 20 1. logging\u4f7f\u7528 import logging import os print(logging.WARNING) print(logging.INFO) def set_logging(name=None, verbose=True): # Sets level and returns logger rank = int(os.getenv('RANK', -1)) # rank in world for Multi-GPU trainings level = logging.INFO if (verbose and rank in (-1, 0)) else logging.WARNING log = logging.getLogger(name) log.setLevel(level) handler = logging.StreamHandler() handler.setFormatter(logging.Formatter(\"%(message)s\")) handler.setLevel(level) log.addHandler(handler) set_logging() LOGGER = logging.getLogger(\"yolov5\") name = 'python' minimum = '3.7.0' current = '3.7.11' s = f'{name}{minimum} required by YOLOv5, but {name}{current} is currently installed' # string LOGGER.warning(s) \u6682\u65f6\u4e0d\u61c2","title":"Log"},{"location":"object_detection/linsh/log/#_1","text":"\u672c\u7bc7\u4e3b\u8981\u8bb0\u5f55\u8ddf\u65e5\u5fd7\u76f8\u5173\u7684\u5185\u5bb9","title":"\u8bf4\u660e"},{"location":"object_detection/linsh/log/#_2","text":"import logging print(logging.WARNING) # 30 \u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u662f30 print(logging.INFO) # 20","title":"\u5c0f\u547d\u4ee4"},{"location":"object_detection/linsh/log/#1-logging","text":"import logging import os print(logging.WARNING) print(logging.INFO) def set_logging(name=None, verbose=True): # Sets level and returns logger rank = int(os.getenv('RANK', -1)) # rank in world for Multi-GPU trainings level = logging.INFO if (verbose and rank in (-1, 0)) else logging.WARNING log = logging.getLogger(name) log.setLevel(level) handler = logging.StreamHandler() handler.setFormatter(logging.Formatter(\"%(message)s\")) handler.setLevel(level) log.addHandler(handler) set_logging() LOGGER = logging.getLogger(\"yolov5\") name = 'python' minimum = '3.7.0' current = '3.7.11' s = f'{name}{minimum} required by YOLOv5, but {name}{current} is currently installed' # string LOGGER.warning(s) \u6682\u65f6\u4e0d\u61c2","title":"1. logging\u4f7f\u7528"},{"location":"object_detection/linsh/other/","text":"\u8bf4\u660e \u6682\u65f6\u4e0d\u77e5\u9053\u5982\u4f55\u5f52\u7c7b\u7684\u653e\u5230\u6b64\u5904 1. \u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cconv\u5c42\u548cbn\u5c42\u7684\u878d\u5408 def fuse_conv_and_bn(conv, bn): # Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/ fusedconv = nn.Conv2d(conv.in_channels, conv.out_channels, kernel_size=conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=True).requires_grad_(False).to(conv.weight.device) # Prepare filters w_conv = conv.weight.clone().view(conv.out_channels, -1) w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var))) fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape)) # Prepare spatial bias b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps)) fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn) return fusedconv \u6682\u65f6\u8fd8\u6ca1\u770b\u61c2 2. \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 \u7406\u89e3\u540e\u611f\u89c9\u5f88\u597d\u7528 https://blog.csdn.net/zhou_438/article/details/109290596","title":"Other"},{"location":"object_detection/linsh/other/#_1","text":"\u6682\u65f6\u4e0d\u77e5\u9053\u5982\u4f55\u5f52\u7c7b\u7684\u653e\u5230\u6b64\u5904","title":"\u8bf4\u660e"},{"location":"object_detection/linsh/other/#1-convbn","text":"def fuse_conv_and_bn(conv, bn): # Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/ fusedconv = nn.Conv2d(conv.in_channels, conv.out_channels, kernel_size=conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=True).requires_grad_(False).to(conv.weight.device) # Prepare filters w_conv = conv.weight.clone().view(conv.out_channels, -1) w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var))) fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape)) # Prepare spatial bias b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps)) fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn) return fusedconv \u6682\u65f6\u8fd8\u6ca1\u770b\u61c2","title":"1. \u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cconv\u5c42\u548cbn\u5c42\u7684\u878d\u5408"},{"location":"object_detection/linsh/other/#2","text":"\u7406\u89e3\u540e\u611f\u89c9\u5f88\u597d\u7528 https://blog.csdn.net/zhou_438/article/details/109290596","title":"2. \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668"},{"location":"object_detection/linsh/yolov5/","text":"\u8bf4\u660e \u6574\u7406yolov5s\u7684\u6574\u4f53\u7ed3\u6784 1. \u7b2c\u4e00\u5c42 \u8f93\u5165\u60c5\u51b5 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [64, 6, 2, 2]]\" args: [64, 6, 2, 2] gd: 0.33 dw: 0.5 n = 1 m: models.common.Conv c1, c2 = 3, 64 args: [3, 32, 6, 2, 2] # \u7ecf\u8fc7\u66f4\u65b0; 32\u662f64 * 0.5 m_ = m(*args) \u6a21\u5757\u4ee3\u7801 class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 3, 32, 6, 2, 2 \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) \u8f93\u51fa\u60c5\u51b5\uff1a t: models.common.Conv np: 3520 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a(\u5377\u79ef\u90e8\u5206\u53c2\u6570)3*32*6*6+(Bn\u53c2\u6570)32+32= 3520 - \u5305\u62ec\u5377\u79ef\u6838\u7684\u53c2\u6570\u548cBN\u5c42\u7684\u4e24\u4e2a\u53c2\u6570 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv] ch: [32] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 2. \u7b2c\u4e8c\u5c42 \u8f93\u5165\u60c5\u51b5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [128, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [128, 3, 2] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 32 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 128 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 64 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [32, 64, 3, 2] class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 32, 64, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\u8ba1\u7b97\uff1ac1 * c2 * k * k + c2 def autopad(k, p=None): # kernel, padding # Pad to 'same' # k, p = 3, None # p\u81ea\u52a8\u8bbe\u7f6e\u4e3a k\u7684\u4e00\u534a\u5e76\u5411\u4e0b\u53d6\u6574 if p is None: p = k // 2 if isinstance(k, int) else (x // 2 for x in k) # auto-pad return p \u8f93\u51fa\u60c5\u51b5\uff1a t: 'models.common.Conv' np: 18560 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a(\u5377\u79ef\u90e8\u5206\u53c2\u6570)32*64*3*3+(Bn\u53c2\u6570)64+64= 18560 - \u5305\u62ec\u5377\u79ef\u6838\u7684\u53c2\u6570\u548cBN\u5c42\u7684\u4e24\u4e2a\u53c2\u6570 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv] ch: [32, 64] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 3. \u7b2c\u4e09\u5c42 \u8f93\u5165\u60c5\u51b5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 3, 'C3', [128]]\" f, n, m, args = -1, 3, 'C3', [128] m = eval(m) # models.common.C3 n = n_ = 1 # \u8fd9\u91cc\u867d\u7136\u539f\u59cbn\u7b49\u4e8e3\uff0c\u4f46\u7531\u4e8e\u662fs\u7248\u672c\uff0c3 * .33 = 1\uff0c\u5373\u8fd8\u662f1\u4e2aC3\u6a21\u5757 c1 = ch[-1] = 32 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 128 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 64 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [64, 64] # C3\u4e3a\u4ec0\u4e48\u53ea\u6709\u901a\u9053\u6570\uff0c\u6ca1\u6709\u5377\u79ef\u6838\u8fd9\u4e9b\u53c2\u6570 args: [64, 64, 1] # \u6267\u884c\u8fc7args.insert(2, n)\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5\u6700\u540e\u4e00\u9879\u53d8\u4e3a1 class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[64, 64, 1] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(64 * 32 * 1 * 1 + 32*2) + (64 * 32 * 1 * 1 + 32*2) + (64 * 64 * 1 * 1 + 64*2) + B = 8448 + B = 18816 class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[32, 32, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(32 * 32 * 1 * 1 + 32*2) + (32 * 32 * 3 * 3 + 32*2) = 10368 \u8f93\u51fa\u60c5\u51b5\uff1a t: 'models.common.C3' np: 18816 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a(64 * 32 * 1 * 1 + 32*2) + (64 * 32 * 1 * 1 + 32*2) + (64 * 64 * 1 * 1 + 64*2) + (32 * 32 * 1 * 1 + 32*2) + (32 * 32 * 3 * 3 + 32*2) = 18816 - \u540c\u4e0a \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3] ch: [32, 64, 64] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 4. \u7b2c\u56db\u5c42 \u8f93\u5165\u60c5\u51b5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [256, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [256, 3, 2] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 64 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 256 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 128 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [64, 128] # C3\u4e3a\u4ec0\u4e48\u53ea\u6709\u901a\u9053\u6570\uff0c\u6ca1\u6709\u5377\u79ef\u6838\u8fd9\u4e9b\u53c2\u6570 args: [64, 128, 3, 2] class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 64, 128, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) t: 'models.common.Conv' np: 18816 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a64 * 128 * 3 * 3 + 128 * 2 = 73984 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv] ch: [32, 64, 64, 128] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 5. \u7b2c5\u5c42 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 6, 'C3', [256]]\" f, n, m, args = -1, 6, 'C3', [256] m = eval(m) # models.common.C3 n = n_ = 6 * 0.33 = 2 c1 = ch[-1] = 128 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 256 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 128 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [128, 128] # args: [128, 128, 2] class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[128, 128, 2] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(128 * 64 * 1 * 1 + 64*2) * 2 + (128 * 128 * 1 * 1 + 128*2) + B = 33280 + B = 115712 class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[128, 128, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(64 * 64 * 1 * 1 + 64*2) + (64 * 64 * 3 * 3 + 64*2) = 41216 # \u4e24\u4e2aBottleneck : 41216 * 2 = 82432 t: 'models.common.C3' np: 115712 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] 4 -1 2 115712 models.common.C3 [128, 128, 2] # [c1, c2, n] n=2 from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3] ch: [32, 64, 64, 128, 128] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 6. \u7b2c6\u5c42 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [512, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [512, 3, 2] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 128 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 512 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 256 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [512, 3, 2] # args: [128, 256, 3, 2] class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 128, 256, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\uff1a 128 * 256 * 3 * 3 + 256 * 2 = 295424 t: 'models.common.Conv' np: 295424 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] 4 -1 2 115712 models.common.C3 [128, 128, 2] # [c1, c2, n] n=2 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] # [c1, c2, k, s] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv] ch: [32, 64, 64, 128, 128, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 7. \u7b2c7\u5c42 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 9, 'C3', [512]]\" f, n, m, args = -1, 9, 'C3', [512] m = eval(m) # models.common.C3 n = n_ = 9 * 0.33 = 3 c1 = ch[-1] = 256 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 512 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 256 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [256, 256] # args: [256, 256, 3] class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[256, 256, 3] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(256 * 128 * 1 * 1 + 128*2) * 2 + (256 * 256 * 1 * 1 + 256*2) + B = 132096 + B = 625152 class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[128, 128, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(128 * 128 * 1 * 1 + 128*2) + (128 * 128 * 3 * 3 + 128*2) = 164352 # \u4e24\u4e2aBottleneck : 164352 * 3 = 493056 t: 'models.common.C3' np: 625152 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] 4 -1 2 115712 models.common.C3 [128, 128, 2] # [c1, c2, n] n=2 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] # [c1, c2, k, s] 6 -1 3 625152 models.common.C3 [256, 256, 3] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3] ch: [32, 64, 64, 128, 128, 256, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 8. \u7b2c8\u5c42 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [1024, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [1024] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 256 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 1024 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 512 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [256, 512, 3, 2] # class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 256, 512, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\uff1a 256 * 512 * 3 * 3 + 512 * 2 = 1180672 t: 'models.common.Conv' np: 1180672 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv] ch: [32, 64, 64, 128, 128, 256, 256, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 9. \u7b2c9\u5c42 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 3, 'C3', [1024]]\" f, n, m, args = -1, 3, 'C3', [1024]] m = eval(m) # models.common.C3 n = n_ = 3 * 0.33 = 1 c1 = ch[-1] = 512 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 1024 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 512 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [512, 512] # args: [512, 512, 1] class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[512, 512, 1] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(512 * 256 * 1 * 1 + 256*2) * 2 + (512 * 512 * 1 * 1 + 512*2) + B = 526336 + B = class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[256, 256, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(256 * 256 * 1 * 1 + 256*2) + (256 * 256 * 3 * 3 + 256*2) = 656384 t: 'models.common.C3' np: 1182720 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 10. \u7b2c10\u5c42 \u662f\u4e00\u79cd\u65b0\u5c42 SPPF \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'SPPF', [1024, 5]]\" f, n, m, args = -1, 1, 'SPPF', [1024, 5]] m = eval(m) # models.common.SPPF n = n_ = 3 * 0.33 = 1 c1 = ch[-1] = 512 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 1024 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 512 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [512, 512, 5] class SPPF(nn.Module): # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher # [512, 512, 5] def __init__(self, c1, c2, k=5): # equivalent to SPP(k=(5, 9, 13)) super().__init__() c_ = c1 // 2 # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_ * 4, c2, 1, 1) self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2) def forward(self, x): x = self.cv1(x) with warnings.catch_warnings(): warnings.simplefilter('ignore') # suppress torch 1.9.0 max_pool2d() warning y1 = self.m(x) y2 = self.m(y1) return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1)) # \u53c2\u6570\u91cf\uff1a(512 * 256 * 1 * 1 + 256 * 2) + (1024 * 512 * 1 * 1 + 512 * 2) = 656896 t: 'models.common.SPPF' np: 656896 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 11. \u7b2c11\u5c42 \u8f93\u5165\u53c2\u6570\uff1a f, n, m, args = [-1, 1, 'Conv', [512, 1, 1]] i = 10 c1 = ch[f] = 512 c2 = args[0] * 0.5 = 256 args = [512, 256, 1, 1] t = 'models.common.Conv' np = 131584 class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 512, 256, 1, 1, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\uff1a 512 * 256 * 1 * 1 + 256 * 2 = 131584 from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] # [c1, c2, k, s] ``` ```python from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 12. \u7b2c12\u5c42 Upsample\u5c42\u6ca1\u6709\u53c2\u6570\u91cf\uff0c i = 11 f, n, m, args = -1, 1, 'nn.Upsample', ['None', 2, 'nearest'] n = n_ = 1 c1 = 512 # c1\u6ca1\u6709\u53d1\u751f\u53d8\u5316 c2 = ch[f] # 256 t = 'torch.nn.modules.upsampling.Upsample' from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 13. \u7b2c13\u5c42 Concat\u5c42\u6ca1\u6709\u53c2\u6570 i = 12 f, n, m, args = [[-1, 6], 1, 'Concat', [1]] elif m in Concat: c2 = sum(ch[x] for x in f) = sum(ch[-1], ch[6]) = 512 class Concat(nn.Module): def __init__(self, dimension=1): super().__init__() self.d = dimension def forward(self, x): return torch.cat(x, self.d) from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 12 [-1, 6] 1 0 models.common.Concat [1] from: [-1, 6] # \u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u548c\u7d22\u5f15\u7b2c6\u5c42 save: [6] # \u7b2c\u4e00\u4e2a\u4e0d\u7b49\u4e8e-1\u7684\u503c layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c 14. \u7b2c14\u5c42 i = 13 f, n, m, args = -1, 3, 'C3', [512, False] args = [512, 256, False] args = [512, 256, 1, False] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256] 15. \u7b2c15\u5c42 i = 14 f, n, m, args = -1, 1, 'Conv', [256, 1, 1] args = [256, 128, 1, 1] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3, Conv] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256, 128] 16. \u7b2c16\u5c42 i = 15 f, n, m, args = -1, 1, 'nn.Upsamle', ['None', 2, 'nearest'] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3, Upsample] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256, 128, 128] 17. \u7b2c17\u5c42 i = 16 f, n, m, args = [[-1, 4], 1, 'Concat', [1]] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3, Upsample, Concat] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256, 128, 128] \u6700\u540e\u4e00\u5c42 Detect i = 24 f, n, m, args = [17, 20, 23], 1, 'Detect', ['nc', 'anchors'] args = [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]] args = [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [ch[17], ch[20], ch[23]]] class Detect(nn.Module): stride = None # strides computed during build onnx_dynamic = False # ONNX export parameter export = False # export mode def __init__(self, nc=80, anchors=(), ch=(), inplace=True): # detection layer # nc = 80; anchors = [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]] # ch = [128, 256, 512] super().__init__() self.nc = nc # number of classes self.no = nc + 5 # number of outputs per anchor self.nl = len(anchors) # number of detection layers self.na = len(anchors[0]) // 2 # number of anchors self.grid = [torch.zeros(1)] * self.nl # init grid self.anchor_grid = [torch.zeros(1)] * self.nl # init anchor grid self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2)) # shape(nl,na,2) self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch) # output conv self.inplace = inplace # use in-place ops (e.g. slice assignment) self.nc = 80 self.no = nc + 5 = 85 self.nl = len(anchors) = 3 self.na = len(anchors[0]) // 2 = 3 self.grid = [torch.zeros(1)] * self.nl = [tensor([0.]), tensor([0.], tensor([0.]))] self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2)) # \u7b2c\u4e00\u9879\u662f\u5c42\u6570nl\uff1b\u6bcf\u5c42\u53ef\u4ee5\u6709\u591a\u4e2aanchor\uff1b\u4f46\u6bcf\u4e2aanchor\u90fd\u662f\u6709\u4e24\u4e2a\u503c # [[[10, 13], [16, 30], [33, 23]], [[30, 61], [62, 45], [59, 119]], [[116, 90], [156, 198], [373, 326]]] # shape (3, 3, 2) \u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff1a def _forward_once(self, x): y, dt = [], [] for m in self.model: if m.f != -1: x = y[m.f] if isinstance(x, int) x = m(x) y.append(x if m.i in self.save else None) return x \u8f93\u5165\u662f[tensor(1,128,32,32), tensor(1, 256, 16, 16), tensor(1, 512, 8, 8)] class Detect(nn.Module): ... def forward(self, x): # \u8f93\u5165\u662f[tensor(1,128,32,32), tensor(1, 256, 16, 16), tensor(1, 512, 8, 8)] z = [] for i in range(self.nl): # \u51e0\u5c42\u7279\u5f81\u56fe\uff0c3\u5c42 x[i] = self.m[i](x[i]) # self.m[i] \u7531\u4e8eself.m\u662f\u4e00\u4e2ann.ModuleList\uff0cself.m[i]\u662f\u5176\u7b2ci\u9879\uff1b\u5c06\u7b2c\u7279\u5f81\u56fetensor(1,128,32,32)\u4f20\u5165\u5230self.m[0]\u4e2d; shape: [1, 128, 32, 32] -> [1, 255, 32, 32] bs, _, ny, nx = x[i].shape # x[0].shape : [1, 255, 32, 32] # [1, 255, 32, 32] -> [1, 3, 85, 32, 32] -> [1, 3, 32, 32, 85] # 1: \u4e00\u5f20\u56fe\u7247\u7684\u7279\u5f81\u56fe # 3: \u5305\u62ec3\u79cdanchor # 32, 32: 32 x 32\u6bcf\u4e2a\u70b9\u4e0a\u90fd\u6709\u8fd9\u4e483\u79cdanchor # 85: \u6bcf\u4e2aanchor\u670985\u79cd\u8f93\u51fa x[i] = x[i].view(bs, self.na, self.no, self.ny, self.nx).permute(0, 1, 3, 4, 2).contiguous() i = 0 ; [1, 128, 32, 32] -> [1, 255, 32, 32] -> [1, 3, 32, 32, 85] i = 1 ; [1, 256, 16, 16] -> [1, 255, 16, 16] -> [1, 3, 16, 16, 85] i = 2 ; [1, 512, 8, 8] -> [1, 255, 8, 8] -> [1, 3, 8, 8, 85]","title":"Yolov5"},{"location":"object_detection/linsh/yolov5/#_1","text":"\u6574\u7406yolov5s\u7684\u6574\u4f53\u7ed3\u6784","title":"\u8bf4\u660e"},{"location":"object_detection/linsh/yolov5/#1","text":"\u8f93\u5165\u60c5\u51b5 \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [64, 6, 2, 2]]\" args: [64, 6, 2, 2] gd: 0.33 dw: 0.5 n = 1 m: models.common.Conv c1, c2 = 3, 64 args: [3, 32, 6, 2, 2] # \u7ecf\u8fc7\u66f4\u65b0; 32\u662f64 * 0.5 m_ = m(*args) \u6a21\u5757\u4ee3\u7801 class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 3, 32, 6, 2, 2 \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) \u8f93\u51fa\u60c5\u51b5\uff1a t: models.common.Conv np: 3520 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a(\u5377\u79ef\u90e8\u5206\u53c2\u6570)3*32*6*6+(Bn\u53c2\u6570)32+32= 3520 - \u5305\u62ec\u5377\u79ef\u6838\u7684\u53c2\u6570\u548cBN\u5c42\u7684\u4e24\u4e2a\u53c2\u6570 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv] ch: [32] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"1. \u7b2c\u4e00\u5c42"},{"location":"object_detection/linsh/yolov5/#2","text":"\u8f93\u5165\u60c5\u51b5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [128, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [128, 3, 2] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 32 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 128 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 64 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [32, 64, 3, 2] class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 32, 64, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\u8ba1\u7b97\uff1ac1 * c2 * k * k + c2 def autopad(k, p=None): # kernel, padding # Pad to 'same' # k, p = 3, None # p\u81ea\u52a8\u8bbe\u7f6e\u4e3a k\u7684\u4e00\u534a\u5e76\u5411\u4e0b\u53d6\u6574 if p is None: p = k // 2 if isinstance(k, int) else (x // 2 for x in k) # auto-pad return p \u8f93\u51fa\u60c5\u51b5\uff1a t: 'models.common.Conv' np: 18560 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a(\u5377\u79ef\u90e8\u5206\u53c2\u6570)32*64*3*3+(Bn\u53c2\u6570)64+64= 18560 - \u5305\u62ec\u5377\u79ef\u6838\u7684\u53c2\u6570\u548cBN\u5c42\u7684\u4e24\u4e2a\u53c2\u6570 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv] ch: [32, 64] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"2. \u7b2c\u4e8c\u5c42"},{"location":"object_detection/linsh/yolov5/#3","text":"\u8f93\u5165\u60c5\u51b5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 3, 'C3', [128]]\" f, n, m, args = -1, 3, 'C3', [128] m = eval(m) # models.common.C3 n = n_ = 1 # \u8fd9\u91cc\u867d\u7136\u539f\u59cbn\u7b49\u4e8e3\uff0c\u4f46\u7531\u4e8e\u662fs\u7248\u672c\uff0c3 * .33 = 1\uff0c\u5373\u8fd8\u662f1\u4e2aC3\u6a21\u5757 c1 = ch[-1] = 32 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 128 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 64 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [64, 64] # C3\u4e3a\u4ec0\u4e48\u53ea\u6709\u901a\u9053\u6570\uff0c\u6ca1\u6709\u5377\u79ef\u6838\u8fd9\u4e9b\u53c2\u6570 args: [64, 64, 1] # \u6267\u884c\u8fc7args.insert(2, n)\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5\u6700\u540e\u4e00\u9879\u53d8\u4e3a1 class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[64, 64, 1] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(64 * 32 * 1 * 1 + 32*2) + (64 * 32 * 1 * 1 + 32*2) + (64 * 64 * 1 * 1 + 64*2) + B = 8448 + B = 18816 class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[32, 32, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(32 * 32 * 1 * 1 + 32*2) + (32 * 32 * 3 * 3 + 32*2) = 10368 \u8f93\u51fa\u60c5\u51b5\uff1a t: 'models.common.C3' np: 18816 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a(64 * 32 * 1 * 1 + 32*2) + (64 * 32 * 1 * 1 + 32*2) + (64 * 64 * 1 * 1 + 64*2) + (32 * 32 * 1 * 1 + 32*2) + (32 * 32 * 3 * 3 + 32*2) = 18816 - \u540c\u4e0a \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3] ch: [32, 64, 64] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"3. \u7b2c\u4e09\u5c42"},{"location":"object_detection/linsh/yolov5/#4","text":"\u8f93\u5165\u60c5\u51b5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [256, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [256, 3, 2] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 64 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 256 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 128 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [64, 128] # C3\u4e3a\u4ec0\u4e48\u53ea\u6709\u901a\u9053\u6570\uff0c\u6ca1\u6709\u5377\u79ef\u6838\u8fd9\u4e9b\u53c2\u6570 args: [64, 128, 3, 2] class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 64, 128, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) t: 'models.common.Conv' np: 18816 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a64 * 128 * 3 * 3 + 128 * 2 = 73984 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv] ch: [32, 64, 64, 128] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"4. \u7b2c\u56db\u5c42"},{"location":"object_detection/linsh/yolov5/#5-5","text":"\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 6, 'C3', [256]]\" f, n, m, args = -1, 6, 'C3', [256] m = eval(m) # models.common.C3 n = n_ = 6 * 0.33 = 2 c1 = ch[-1] = 128 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 256 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 128 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [128, 128] # args: [128, 128, 2] class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[128, 128, 2] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(128 * 64 * 1 * 1 + 64*2) * 2 + (128 * 128 * 1 * 1 + 128*2) + B = 33280 + B = 115712 class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[128, 128, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(64 * 64 * 1 * 1 + 64*2) + (64 * 64 * 3 * 3 + 64*2) = 41216 # \u4e24\u4e2aBottleneck : 41216 * 2 = 82432 t: 'models.common.C3' np: 115712 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] 4 -1 2 115712 models.common.C3 [128, 128, 2] # [c1, c2, n] n=2 from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3] ch: [32, 64, 64, 128, 128] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"5. \u7b2c5\u5c42"},{"location":"object_detection/linsh/yolov5/#6-6","text":"\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [512, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [512, 3, 2] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 128 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 512 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 256 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [512, 3, 2] # args: [128, 256, 3, 2] class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 128, 256, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\uff1a 128 * 256 * 3 * 3 + 256 * 2 = 295424 t: 'models.common.Conv' np: 295424 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] 4 -1 2 115712 models.common.C3 [128, 128, 2] # [c1, c2, n] n=2 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] # [c1, c2, k, s] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv] ch: [32, 64, 64, 128, 128, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"6. \u7b2c6\u5c42"},{"location":"object_detection/linsh/yolov5/#7-7","text":"\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 9, 'C3', [512]]\" f, n, m, args = -1, 9, 'C3', [512] m = eval(m) # models.common.C3 n = n_ = 9 * 0.33 = 3 c1 = ch[-1] = 256 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 512 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 256 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [256, 256] # args: [256, 256, 3] class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[256, 256, 3] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(256 * 128 * 1 * 1 + 128*2) * 2 + (256 * 256 * 1 * 1 + 256*2) + B = 132096 + B = 625152 class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[128, 128, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(128 * 128 * 1 * 1 + 128*2) + (128 * 128 * 3 * 3 + 128*2) = 164352 # \u4e24\u4e2aBottleneck : 164352 * 3 = 493056 t: 'models.common.C3' np: 625152 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] # [c1, c2, k, s, p] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] # [c1, c2, k, s] 2 -1 1 18816 models.common.C3 [64, 64, 1] # [c1, c2, n] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] # [c1, c2, k, s] 4 -1 2 115712 models.common.C3 [128, 128, 2] # [c1, c2, n] n=2 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] # [c1, c2, k, s] 6 -1 3 625152 models.common.C3 [256, 256, 3] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3] ch: [32, 64, 64, 128, 128, 256, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"7. \u7b2c7\u5c42"},{"location":"object_detection/linsh/yolov5/#8-8","text":"\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'Conv', [1024, 3, 2]]\" f, n, m, args = -1, 1, 'Conv', [1024] m = eval(m) # models.common.Conv n = n_ = 1 c1 = ch[-1] = 256 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 1024 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 512 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [256, 512, 3, 2] # class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 256, 512, 3, 2, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\uff1a 256 * 512 * 3 * 3 + 512 * 2 = 1180672 t: 'models.common.Conv' np: 1180672 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv] ch: [32, 64, 64, 128, 128, 256, 256, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"8. \u7b2c8\u5c42"},{"location":"object_detection/linsh/yolov5/#9-9","text":"\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 3, 'C3', [1024]]\" f, n, m, args = -1, 3, 'C3', [1024]] m = eval(m) # models.common.C3 n = n_ = 3 * 0.33 = 1 c1 = ch[-1] = 512 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 1024 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 512 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [512, 512] # args: [512, 512, 1] class C3(nn.Module): # CSP Bottleneck with 3 convolutions # \u8f93\u5165\u53c2\u6570\uff1a[512, 512, 1] \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u5757\u7684\u5806\u53e0\u4e2a\u6570\uff1b\u5176\u4ed6\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4 # 1.\u4e24\u4e2a1x1\u5377\u79ef\u5206\u6210\u4e24\u90e8\u5206\uff1b2.\u4e00\u90e8\u5206\u7ecf\u8fc7Botteneck\uff0c\u4e00\u90e8\u5206\u4fdd\u6301\u4e0d\u53d8\uff1b3.Cat\u4e24\u90e8\u5206\uff1b4. \u6700\u540e\u7ecf\u8fc7\u4e00\u4e2a1x1\u5377\u79ef def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # \u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\u4e14e=1\uff0c\u8fd9\u79cd\u60c5\u51b5\u80fd\u4e00\u76f4\u5806\u53e0\u3002 # self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) # C3\u7684\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(512 * 256 * 1 * 1 + 256*2) * 2 + (512 * 512 * 1 * 1 + 512*2) + B = 526336 + B = class Bottleneck(nn.Module): # Standard bottleneck # \u4e00\u4e2a\u5305\u542b\u6709 1x1\u5377\u79ef+3x3\u5377\u79ef+shortcut\u7684\u5757 # \u5982\u679c\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u76f8\u7b49\uff0c\u5219\u4f7f\u7528shortcut # 1x1\u5377\u79ef\u7684\u8f93\u51fa\u901a\u9053\u6570*e # \u8f93\u5165\u53c2\u6570\uff1a[256, 256, e=1.0] def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # ch_in, ch_out, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # Bottleneck\u53c2\u6570\u91cf\u8ba1\u7b97\uff1a(256 * 256 * 1 * 1 + 256*2) + (256 * 256 * 3 * 3 + 256*2) = 656384 t: 'models.common.C3' np: 1182720 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"9. \u7b2c9\u5c42"},{"location":"object_detection/linsh/yolov5/#10-10","text":"\u662f\u4e00\u79cd\u65b0\u5c42 SPPF \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\uff1a\"[-1, 1, 'SPPF', [1024, 5]]\" f, n, m, args = -1, 1, 'SPPF', [1024, 5]] m = eval(m) # models.common.SPPF n = n_ = 3 * 0.33 = 1 c1 = ch[-1] = 512 # \u8f93\u5165\u901a\u9053\u6570\uff0c\u4ece\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u83b7\u53d6 c2 = args[0] = 1024 # \u8f93\u51fa\u901a\u9053\u6570\uff0c\u4ece\u5f53\u524d\u5c42\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6, \u5176\u4e2d\u7684\u7b2c\u4e00\u9879 c2 = c2 * 0.5 = 512 # \u56e0\u4e3a\u662fs\u7248\u672c\uff0c\u6240\u4ee5\u8981\u6298\u534a args: [512, 512, 5] class SPPF(nn.Module): # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher # [512, 512, 5] def __init__(self, c1, c2, k=5): # equivalent to SPP(k=(5, 9, 13)) super().__init__() c_ = c1 // 2 # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_ * 4, c2, 1, 1) self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2) def forward(self, x): x = self.cv1(x) with warnings.catch_warnings(): warnings.simplefilter('ignore') # suppress torch 1.9.0 max_pool2d() warning y1 = self.m(x) y2 = self.m(y1) return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1)) # \u53c2\u6570\u91cf\uff1a(512 * 256 * 1 * 1 + 256 * 2) + (1024 * 512 * 1 * 1 + 512 * 2) = 656896 t: 'models.common.SPPF' np: 656896 (\u53c2\u6570\u91cf) - \u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e0a\u9762 \u8f93\u51fa\u6253\u5370\u60c5\u51b5\uff1a from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"10. \u7b2c10\u5c42"},{"location":"object_detection/linsh/yolov5/#11-11","text":"\u8f93\u5165\u53c2\u6570\uff1a f, n, m, args = [-1, 1, 'Conv', [512, 1, 1]] i = 10 c1 = ch[f] = 512 c2 = args[0] * 0.5 = 256 args = [512, 256, 1, 1] t = 'models.common.Conv' np = 131584 class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): \"\"\" c1, c2, k, s, p = 512, 256, 1, 1, None \"\"\" super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) # \u53c2\u6570\u91cf\uff1a 512 * 256 * 1 * 1 + 256 * 2 = 131584 from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] # [c1, c2, k, s] ``` ```python from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"11. \u7b2c11\u5c42"},{"location":"object_detection/linsh/yolov5/#12-12","text":"Upsample\u5c42\u6ca1\u6709\u53c2\u6570\u91cf\uff0c i = 11 f, n, m, args = -1, 1, 'nn.Upsample', ['None', 2, 'nearest'] n = n_ = 1 c1 = 512 # c1\u6ca1\u6709\u53d1\u751f\u53d8\u5316 c2 = ch[f] # 256 t = 'torch.nn.modules.upsampling.Upsample' from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] from: -1 # \u542b\u4e49\u662f\u5f53\u524d\u5c42\u7684\u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa save: [] # \u56e0\u4e3a\u5f53\u524d\u5c42\u4e0d\u4f1a\u8fdb\u884c\u63a8\u7406\uff0c\u6240\u4ee5\u4e0d\u5fc5\u4fdd\u5b58 layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"12. \u7b2c12\u5c42"},{"location":"object_detection/linsh/yolov5/#13-13","text":"Concat\u5c42\u6ca1\u6709\u53c2\u6570 i = 12 f, n, m, args = [[-1, 6], 1, 'Concat', [1]] elif m in Concat: c2 = sum(ch[x] for x in f) = sum(ch[-1], ch[6]) = 512 class Concat(nn.Module): def __init__(self, dimension=1): super().__init__() self.d = dimension def forward(self, x): return torch.cat(x, self.d) from n params module arguments 0 -1 1 3520 models.common.Conv [3, 32, 6, 2, 2] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 18816 models.common.C3 [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 2 115712 models.common.C3 [128, 128, 2] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 3 625152 models.common.C3 [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 1182720 models.common.C3 [512, 512, 1] 9 -1 1 656896 models.common.SPPF [512, 512, 5] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 12 [-1, 6] 1 0 models.common.Concat [1] from: [-1, 6] # \u8f93\u5165\u6765\u81ea\u4e0a\u4e00\u5c42\u548c\u7d22\u5f15\u7b2c6\u5c42 save: [6] # \u7b2c\u4e00\u4e2a\u4e0d\u7b49\u4e8e-1\u7684\u503c layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512] # \u5f53\u524d\u5faa\u73af\u7ed3\u675f\u540e\u7684\u503c","title":"13. \u7b2c13\u5c42"},{"location":"object_detection/linsh/yolov5/#14-14","text":"i = 13 f, n, m, args = -1, 3, 'C3', [512, False] args = [512, 256, False] args = [512, 256, 1, False] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256]","title":"14. \u7b2c14\u5c42"},{"location":"object_detection/linsh/yolov5/#15-15","text":"i = 14 f, n, m, args = -1, 1, 'Conv', [256, 1, 1] args = [256, 128, 1, 1] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3, Conv] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256, 128]","title":"15. \u7b2c15\u5c42"},{"location":"object_detection/linsh/yolov5/#16-16","text":"i = 15 f, n, m, args = -1, 1, 'nn.Upsamle', ['None', 2, 'nearest'] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3, Upsample] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256, 128, 128]","title":"16. \u7b2c16\u5c42"},{"location":"object_detection/linsh/yolov5/#17-17","text":"i = 16 f, n, m, args = [[-1, 4], 1, 'Concat', [1]] layers: [Conv, Conv, C3, Conv, C3, Conv, C3, Conv, C3, SPPF, Conv, Upsample, Concat, C3, Upsample, Concat] ch: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512, 256, 256, 512, 256, 128, 128]","title":"17. \u7b2c17\u5c42"},{"location":"object_detection/linsh/yolov5/#_2","text":"Detect i = 24 f, n, m, args = [17, 20, 23], 1, 'Detect', ['nc', 'anchors'] args = [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]] args = [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [ch[17], ch[20], ch[23]]] class Detect(nn.Module): stride = None # strides computed during build onnx_dynamic = False # ONNX export parameter export = False # export mode def __init__(self, nc=80, anchors=(), ch=(), inplace=True): # detection layer # nc = 80; anchors = [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]] # ch = [128, 256, 512] super().__init__() self.nc = nc # number of classes self.no = nc + 5 # number of outputs per anchor self.nl = len(anchors) # number of detection layers self.na = len(anchors[0]) // 2 # number of anchors self.grid = [torch.zeros(1)] * self.nl # init grid self.anchor_grid = [torch.zeros(1)] * self.nl # init anchor grid self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2)) # shape(nl,na,2) self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch) # output conv self.inplace = inplace # use in-place ops (e.g. slice assignment) self.nc = 80 self.no = nc + 5 = 85 self.nl = len(anchors) = 3 self.na = len(anchors[0]) // 2 = 3 self.grid = [torch.zeros(1)] * self.nl = [tensor([0.]), tensor([0.], tensor([0.]))] self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2)) # \u7b2c\u4e00\u9879\u662f\u5c42\u6570nl\uff1b\u6bcf\u5c42\u53ef\u4ee5\u6709\u591a\u4e2aanchor\uff1b\u4f46\u6bcf\u4e2aanchor\u90fd\u662f\u6709\u4e24\u4e2a\u503c # [[[10, 13], [16, 30], [33, 23]], [[30, 61], [62, 45], [59, 119]], [[116, 90], [156, 198], [373, 326]]] # shape (3, 3, 2) \u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff1a def _forward_once(self, x): y, dt = [], [] for m in self.model: if m.f != -1: x = y[m.f] if isinstance(x, int) x = m(x) y.append(x if m.i in self.save else None) return x \u8f93\u5165\u662f[tensor(1,128,32,32), tensor(1, 256, 16, 16), tensor(1, 512, 8, 8)] class Detect(nn.Module): ... def forward(self, x): # \u8f93\u5165\u662f[tensor(1,128,32,32), tensor(1, 256, 16, 16), tensor(1, 512, 8, 8)] z = [] for i in range(self.nl): # \u51e0\u5c42\u7279\u5f81\u56fe\uff0c3\u5c42 x[i] = self.m[i](x[i]) # self.m[i] \u7531\u4e8eself.m\u662f\u4e00\u4e2ann.ModuleList\uff0cself.m[i]\u662f\u5176\u7b2ci\u9879\uff1b\u5c06\u7b2c\u7279\u5f81\u56fetensor(1,128,32,32)\u4f20\u5165\u5230self.m[0]\u4e2d; shape: [1, 128, 32, 32] -> [1, 255, 32, 32] bs, _, ny, nx = x[i].shape # x[0].shape : [1, 255, 32, 32] # [1, 255, 32, 32] -> [1, 3, 85, 32, 32] -> [1, 3, 32, 32, 85] # 1: \u4e00\u5f20\u56fe\u7247\u7684\u7279\u5f81\u56fe # 3: \u5305\u62ec3\u79cdanchor # 32, 32: 32 x 32\u6bcf\u4e2a\u70b9\u4e0a\u90fd\u6709\u8fd9\u4e483\u79cdanchor # 85: \u6bcf\u4e2aanchor\u670985\u79cd\u8f93\u51fa x[i] = x[i].view(bs, self.na, self.no, self.ny, self.nx).permute(0, 1, 3, 4, 2).contiguous() i = 0 ; [1, 128, 32, 32] -> [1, 255, 32, 32] -> [1, 3, 32, 32, 85] i = 1 ; [1, 256, 16, 16] -> [1, 255, 16, 16] -> [1, 3, 16, 16, 85] i = 2 ; [1, 512, 8, 8] -> [1, 255, 8, 8] -> [1, 3, 8, 8, 85]","title":"\u6700\u540e\u4e00\u5c42"},{"location":"object_detection/linsh/yolov5_img/","text":"","title":"Yolov5 img"},{"location":"system/Introduction/","text":"Vscode\u63d2\u4ef6\u5b89\u88c5 drawio - \u7ed8\u5236\u793a\u610f\u56fe\u63d2\u4ef6 mindmap - \u7ed8\u5236\u8111\u56fe\u63d2\u4ef6 (web\u7aef\u4e0d\u80fd\u4f7f\u7528) https://blog.csdn.net/liuxiao723846/article/details/107414365 markdown \u4f7f\u7528\uff1a https://blog.csdn.net/linshen1213/article/details/115330264 https://www.jianshu.com/p/25f0139637b7 shell\u76f8\u5173 \u7f51\u7edc\u7bc7 \u5728timm\u83b7\u53d6\u6a21\u578b\u4e2d\u78b0\u5230\u4e00\u4e2a\u95ee\u9898\uff0c\u52a0\u5982\u4e0b\u65b9\u5f0f\u53ef\u4ee5\u89e3\u51b3 import ssl ssl._create_default_https_context = ssl._create_unverified_context \u5982\u4f55\u5728vscode\u8c03\u8bd5pytorch\u5e76\u884c\u6a21\u5f0fDDP \u53c2\u8003\u6587\u6863 https://blog.csdn.net/qianbin3200896/article/details/108182504","title":"Introduction"},{"location":"system/Introduction/#vscode","text":"drawio - \u7ed8\u5236\u793a\u610f\u56fe\u63d2\u4ef6 mindmap - \u7ed8\u5236\u8111\u56fe\u63d2\u4ef6 (web\u7aef\u4e0d\u80fd\u4f7f\u7528) https://blog.csdn.net/liuxiao723846/article/details/107414365 markdown \u4f7f\u7528\uff1a https://blog.csdn.net/linshen1213/article/details/115330264 https://www.jianshu.com/p/25f0139637b7","title":"Vscode\u63d2\u4ef6\u5b89\u88c5"},{"location":"system/Introduction/#shell","text":"","title":"shell\u76f8\u5173"},{"location":"system/Introduction/#_1","text":"\u5728timm\u83b7\u53d6\u6a21\u578b\u4e2d\u78b0\u5230\u4e00\u4e2a\u95ee\u9898\uff0c\u52a0\u5982\u4e0b\u65b9\u5f0f\u53ef\u4ee5\u89e3\u51b3 import ssl ssl._create_default_https_context = ssl._create_unverified_context","title":"\u7f51\u7edc\u7bc7"},{"location":"system/Introduction/#vscodepytorchddp","text":"\u53c2\u8003\u6587\u6863 https://blog.csdn.net/qianbin3200896/article/details/108182504","title":"\u5982\u4f55\u5728vscode\u8c03\u8bd5pytorch\u5e76\u884c\u6a21\u5f0fDDP"},{"location":"%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","text":"\u80cc\u666f \u6574\u7406\u4e00\u4e9b\u9700\u8981\u5b66\u4e60\u7684\u8d44\u6599\uff0c\u4ee5\u53ca\u5f85\u5b66\u4e60\u7684\u8d44\u6599\u6216\u8005\u6709\u7591\u95ee\u7684\u5185\u5bb9 \u6982\u5ff5\u5173\u952e\u8bcd\uff1a\u4eba\u8138\u6d3b\u4f53\u3001OCR\u3001\u82af\u7b97\u4e00\u4f53\u3001Mixture-of-Experts (MoE) \u5f85\u5b66\u4e60\uff1a \u81ea\u76d1\u7763\u8868\u5f81\u9884\u8bad\u7ec3CAE\uff08Context Autoencoder\uff09\u65b9\u6cd5, \u5728\u9690\u542b\u8868\u5f81\u7a7a\u95f4\u91cc\uff0c\u5bf9\u63a9\u7801\u533a\u57df\u505a\u9884\u6d4b\uff1b\u53ef\u4ee5\u5b66\u4e60\u5230\u8bed\u4e49\u8868\u5f81\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u5757\u7684\u68c0\u7d22 \u5927\u6a21\u578b\u8bad\u7ec3\u7684\u6311\u6218\u548c\u5e94\u5bf9 \u5171\u6027\uff1a \u6a21\u578b\u5927\uff08\u4f8b\u5982GPT-3\uff09\u3001\u6570\u636e\u91cf\u5927\uff08\u4f8b\u5982\u5341\u51e0\u4ebf\u56fe\u6587\u5bf9\uff09\u3001\u8ba1\u7b97\u91cf\u5927\uff0864\u5361\u8bad\u7ec3\u4e00\u4e2a\u6708\uff09 \u5dee\u5f02\uff1a \u7f51\u7edc\u7ed3\u6784\u6709\u5dee\u522b \u6a21\u578b\u7a00\u758f\u6027\u3001\u7a20\u5bc6\u6027\u6709\u5dee\u522b IO\u7279\u6027 Wide&Deep\u6a21\u578b\uff08\u4e07\u4ebf\u3001\u7a00\u758f\uff09\u3001Transformer\u7c7b\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5343\u4ebf\u3001\u7a20\u5bc6\uff09\u3001Mixture-of-Experts\u7c7b\u591a\u4e13\u5bb6\u6a21\u578b\uff08\u4e07\u4ebf\u3001\u5341\u4e07\u4ebf\u3001\u7a20\u5bc6\uff09 \u96be\uff1a ERNIE3.0 6.2E11 TFLOs / V100 32G 125TFLOPs \u89e3\u51b3\u529e\u6cd5\uff1a \u591a\u8ba1\u7b97\u8282\u70b9\u6269\u5c55\uff08\u4e3b\u52a8\uff09 \u4fdd\u8bc1\u6536\u655b\u6027\uff08\u5e76\u884c\u7b56\u7565\u8003\u8651\u8ba1\u7b97\u7b49\u4ef7\u6027\u3001\u6b63\u786e\u6027\uff09 \u63d0\u5347\u52a0\u901f\u6bd4\uff08\u964d\u4f4e\u901a\u4fe1\u3001\u540c\u6b65\u7b49\u6d88\u8017\uff09 \u53ef\u8bad\u7ec3\uff1a\u89e3\u51b3\u6a21\u578b\u63d0\u53ca\u5e26\u6765\u7684\u6311\u6218 - \u6a21\u578b\u76f8\u5173\u4fe1\u606f\u53ef\u88ab\u5355\u8ba1\u7b97\u8bbe\u5907\u5b58\u50a8\u4e0b\uff1b - \u6a21\u578b\u5207\u5206\u540e\u53ef\u88ab\u6b63\u786e\u3001\u9ad8\u6548\u8bad\u7ec3 \u6570\u636e\u5e76\u884c\u7b56\u7565\uff0c\u53c2\u6570\u3001\u68af\u5ea6\u66f4\u65b0\uff1a\u53c2\u6570\u670d\u52a1\u5668\uff08\u4e2d\u6027\u5316\uff09\u3001all Reduce\uff08\u53bb\u4e2d\u6027\u5316\uff09\u7b49 ====================================== \u5927\u6a21\u578b\u9884NLP\u4ea7\u4e1a\u5e94\u7528 \u65b0\u95fb\u8d44\u8baf\u63a8\u8350 \u667a\u80fd\u5ba2\u670d\u5bf9\u8bdd \u6587\u6863\u8981\u7d20\u62bd\u53d6 \u667a\u80fd\u6587\u672c\u5ba1\u6838 \u667a\u80fd\u4f1a\u8bae\u7eaa\u8981 \u5546\u54c1\u8bc4\u4ef7\u5206\u6790 \u8206\u60c5\u5206\u6790 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316 \u4f01\u4e1a\u7cbe\u51c6\u8425\u9500 \u80cc\u666f\uff1a\u901a\u7528\u6587\u672c\u5206\u7c7b\u5e94\u7528 \u4e0d\u591f\u7528\uff0c\u66f4\u52a0\u573a\u666f\u5316\u3001\u66f4\u6df1\u5165\u3001\u66f4\u9700\u8981\u5b9a\u5236\u5316\u5f00\u53d1\u3002 \u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff1a \u573a\u666f\u5316\u6570\u636e\u91c7\u96c6-\u6807\u6ce8\u56f0\u96be \u6570\u636e\u6807\u51c6\u56f0\u96be \u7814\u53d1\u6210\u672c\u9ad8 \u5e94\u7528\u573a\u666f\u7a84\uff0c\u4ec5\u9650\u4e8e\u5782\u7c7b\uff0c\u8fb9\u9645\u6536\u76ca\u4f4e \u9884\u8bad\u7ec3\u5927\u6a21\u578b\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u65b9\u5411 \u6548\u679c\u597d\uff08NLP\u6548\u679c\u660e\u663e\uff09\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\uff08GPT-3\uff09\u3001\u901a\u7528\u6027\u5f3a\u3002 \u6df1\u5ea6\u5b66\u4e60/\u5927\u6570\u636e/\u5927\u7b97\u529b + \u81ea\u76d1\u7763\u7b97\u6cd5 + \u5c11\u91cf\u6570\u636e\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6765\u5e94\u7528\u4e8e\u4e1a\u52a1\u3002 \u5927\u6a21\u578b\u8ba9AI\u6210\u672c\u5927\u5e45\u5ea6\u964d\u4f4e\uff1a - \u6807\u6ce8\u6570\u636e\u91cf\u5927\u5e45\u964d\u4f4e\uff0c\u964d\u4f4e90% - \u8ba1\u7b97\u8d44\u6e90\u5927\u5e45\u964d\u4f4e - \u7814\u53d1\u5468\u671f\u5927\u5e45\u964d\u4f4e \u5927\u6a21\u578b\u7684\u5de5\u5177\u548c\u5e73\u53f0 NLP\u5f00\u53d1\u5957\u4ef6 \u96be BML-\u6587\u672c\u5f00\u53d1\u5e73\u53f0 \u4e2d EasyDL-\u6587\u672c\u5f00\u53d1\u5e73\u53f0 \u6613 \u667a\u80fd\u6587\u6863\u5206\u6790\uff08TextMind\uff09 \u667a\u80fd\u521b\u4f5c\u51ed\u6761 \u667a\u80fd\u5bf9\u8bdd\u5e73\u53f0\uff08UNIT\uff09 NLP\u5f00\u53d1\u5957\u4ef6 \u63a8\u7406\u548c\u90e8\u7f72\u5f88\u5173\u952e\uff0c\u9700\u8981\u8f7b\u91cf\u5316 ERNIE-Gen \u3010\u6587\u672c\u751f\u6210\u4efb\u52a1\u3011\u5bf9\u6bd4\u540c\u5c3a\u5bf8\u901a\u7528\u6a21\u578bBLEU4\u63d0\u53472-5% ERNIE-IE \u3010\u6587\u672c\u62bd\u53d6\u4efb\u52a1\u3011\uff0c\u9002\u5408\u96f6\u6837\u672c\u5c11\u6837\u672c\u60c5\u51b5\uff0c\u5355\u6807\u7b7e5\u6761\u6837\u672c ERNIE-Rank \u3010\u68c0\u7d22\u6392\u5e8f\u4efb\u52a1\u3011\u63d0\u53472% ERNIE-Senta \u3010\u60c5\u611f\u5206\u6790\u4efb\u52a1\u3011\uff0c\u5bf9\u6bd4\u540c\u5c3a\u5bf8\u901a\u7528\u6a21\u578b\u5e73\u5747\u63d0\u53470.6% ERNIE-Sim \u3010\u6587\u672c\u5339\u914d\u4efb\u52a1\u3011\u5bf9\u6bd4\u540c\u5c3a\u5bf8\u901a\u7528\u6a21\u578b\u63d0\u534710% ERNIE-Doc \u3010\u957f\u6587\u672c\u5206\u7c7b\u5339\u914d\u62bd\u53d6\u4efb\u52a1\u3011\u7b49 ERNIE-M \u3010\u591a\u8bed\u8a00\u5206\u7c7b\u5339\u914d\u62bd\u53d6\u4efb\u52a1\u3011\u7b49 \u5e38\u89c1NLP\u573a\u666f\u4efb\u52a1\uff1a - \u6587\u672c\u5206\u7c7b-\u5355\u6807\u7b7e - \u6587\u672c\u5206\u7c7b-\u591a\u6807\u7b7e - \u60c5\u611f\u5206\u6790 - \u6587\u672c\u5339\u914d - \u5e8f\u5217\u6807\u6ce8 - \u547d\u540d\u5b9e\u4f53\u8bc6\u522b - \u673a\u5668\u9605\u8bfb\u7406\u89e3 - \u4e2d\u6587\u95ee\u7b54 - \u4fe1\u606f\u62bd\u53d6 - \u6458\u8981\u751f\u6210 - \u5b9e\u4f53\u8bc6\u522b - \u5173\u7cfb\u62bd\u53d6 - \u6587\u672c\u751f\u6210 - \u56fe\u6587\u68c0\u7d22 - \u5bcc\u5a92\u4f53\u62bd\u53d6 \u5b8c\u5584\u7684\u6570\u636e\u5904\u7406\u80fd\u529b - \u6570\u636e\u589e\u5f3a \u6807\u6ce8\u5f88\u5c11\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c31\u884c - \u968f\u673a\u66ff\u6362 - \u968f\u673a\u5220\u9664 - \u540c\u4e49\u8bcd\u66ff\u6362 - \u8bcd\u5411\u91cf\u8fd1\u4e49\u8bcd\u66ff\u6362 - MLM\u8fd1\u4e49\u8bcd\u66ff\u6362 \u4e3b\u52a8\u5b66\u4e60\u5f0f\u7684\u6807\u6ce8\u5de5\u5177: \u6a21\u578b\u9884\u6d4b+\u6807\u6ce8 \u7aef\u5230\u7aef\u5927\u6a21\u578b\u8f7b\u91cf\u5316\u65b9\u6848\uff1a\u663e\u8457\u964d\u4f4e\u5f00\u53d1\u8005\u90e8\u7f72\u5927\u6a21\u578b\u6210\u672c","title":"\u80cc\u666f"},{"location":"%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E5%A4%A7%E6%A8%A1%E5%9E%8B/#_1","text":"\u6574\u7406\u4e00\u4e9b\u9700\u8981\u5b66\u4e60\u7684\u8d44\u6599\uff0c\u4ee5\u53ca\u5f85\u5b66\u4e60\u7684\u8d44\u6599\u6216\u8005\u6709\u7591\u95ee\u7684\u5185\u5bb9 \u6982\u5ff5\u5173\u952e\u8bcd\uff1a\u4eba\u8138\u6d3b\u4f53\u3001OCR\u3001\u82af\u7b97\u4e00\u4f53\u3001Mixture-of-Experts (MoE) \u5f85\u5b66\u4e60\uff1a \u81ea\u76d1\u7763\u8868\u5f81\u9884\u8bad\u7ec3CAE\uff08Context Autoencoder\uff09\u65b9\u6cd5, \u5728\u9690\u542b\u8868\u5f81\u7a7a\u95f4\u91cc\uff0c\u5bf9\u63a9\u7801\u533a\u57df\u505a\u9884\u6d4b\uff1b\u53ef\u4ee5\u5b66\u4e60\u5230\u8bed\u4e49\u8868\u5f81\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u5757\u7684\u68c0\u7d22 \u5927\u6a21\u578b\u8bad\u7ec3\u7684\u6311\u6218\u548c\u5e94\u5bf9 \u5171\u6027\uff1a \u6a21\u578b\u5927\uff08\u4f8b\u5982GPT-3\uff09\u3001\u6570\u636e\u91cf\u5927\uff08\u4f8b\u5982\u5341\u51e0\u4ebf\u56fe\u6587\u5bf9\uff09\u3001\u8ba1\u7b97\u91cf\u5927\uff0864\u5361\u8bad\u7ec3\u4e00\u4e2a\u6708\uff09 \u5dee\u5f02\uff1a \u7f51\u7edc\u7ed3\u6784\u6709\u5dee\u522b \u6a21\u578b\u7a00\u758f\u6027\u3001\u7a20\u5bc6\u6027\u6709\u5dee\u522b IO\u7279\u6027 Wide&Deep\u6a21\u578b\uff08\u4e07\u4ebf\u3001\u7a00\u758f\uff09\u3001Transformer\u7c7b\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5343\u4ebf\u3001\u7a20\u5bc6\uff09\u3001Mixture-of-Experts\u7c7b\u591a\u4e13\u5bb6\u6a21\u578b\uff08\u4e07\u4ebf\u3001\u5341\u4e07\u4ebf\u3001\u7a20\u5bc6\uff09 \u96be\uff1a ERNIE3.0 6.2E11 TFLOs / V100 32G 125TFLOPs \u89e3\u51b3\u529e\u6cd5\uff1a \u591a\u8ba1\u7b97\u8282\u70b9\u6269\u5c55\uff08\u4e3b\u52a8\uff09 \u4fdd\u8bc1\u6536\u655b\u6027\uff08\u5e76\u884c\u7b56\u7565\u8003\u8651\u8ba1\u7b97\u7b49\u4ef7\u6027\u3001\u6b63\u786e\u6027\uff09 \u63d0\u5347\u52a0\u901f\u6bd4\uff08\u964d\u4f4e\u901a\u4fe1\u3001\u540c\u6b65\u7b49\u6d88\u8017\uff09 \u53ef\u8bad\u7ec3\uff1a\u89e3\u51b3\u6a21\u578b\u63d0\u53ca\u5e26\u6765\u7684\u6311\u6218 - \u6a21\u578b\u76f8\u5173\u4fe1\u606f\u53ef\u88ab\u5355\u8ba1\u7b97\u8bbe\u5907\u5b58\u50a8\u4e0b\uff1b - \u6a21\u578b\u5207\u5206\u540e\u53ef\u88ab\u6b63\u786e\u3001\u9ad8\u6548\u8bad\u7ec3 \u6570\u636e\u5e76\u884c\u7b56\u7565\uff0c\u53c2\u6570\u3001\u68af\u5ea6\u66f4\u65b0\uff1a\u53c2\u6570\u670d\u52a1\u5668\uff08\u4e2d\u6027\u5316\uff09\u3001all Reduce\uff08\u53bb\u4e2d\u6027\u5316\uff09\u7b49 ====================================== \u5927\u6a21\u578b\u9884NLP\u4ea7\u4e1a\u5e94\u7528 \u65b0\u95fb\u8d44\u8baf\u63a8\u8350 \u667a\u80fd\u5ba2\u670d\u5bf9\u8bdd \u6587\u6863\u8981\u7d20\u62bd\u53d6 \u667a\u80fd\u6587\u672c\u5ba1\u6838 \u667a\u80fd\u4f1a\u8bae\u7eaa\u8981 \u5546\u54c1\u8bc4\u4ef7\u5206\u6790 \u8206\u60c5\u5206\u6790 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316 \u4f01\u4e1a\u7cbe\u51c6\u8425\u9500 \u80cc\u666f\uff1a\u901a\u7528\u6587\u672c\u5206\u7c7b\u5e94\u7528 \u4e0d\u591f\u7528\uff0c\u66f4\u52a0\u573a\u666f\u5316\u3001\u66f4\u6df1\u5165\u3001\u66f4\u9700\u8981\u5b9a\u5236\u5316\u5f00\u53d1\u3002 \u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff1a \u573a\u666f\u5316\u6570\u636e\u91c7\u96c6-\u6807\u6ce8\u56f0\u96be \u6570\u636e\u6807\u51c6\u56f0\u96be \u7814\u53d1\u6210\u672c\u9ad8 \u5e94\u7528\u573a\u666f\u7a84\uff0c\u4ec5\u9650\u4e8e\u5782\u7c7b\uff0c\u8fb9\u9645\u6536\u76ca\u4f4e \u9884\u8bad\u7ec3\u5927\u6a21\u578b\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u65b9\u5411 \u6548\u679c\u597d\uff08NLP\u6548\u679c\u660e\u663e\uff09\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\uff08GPT-3\uff09\u3001\u901a\u7528\u6027\u5f3a\u3002 \u6df1\u5ea6\u5b66\u4e60/\u5927\u6570\u636e/\u5927\u7b97\u529b + \u81ea\u76d1\u7763\u7b97\u6cd5 + \u5c11\u91cf\u6570\u636e\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6765\u5e94\u7528\u4e8e\u4e1a\u52a1\u3002 \u5927\u6a21\u578b\u8ba9AI\u6210\u672c\u5927\u5e45\u5ea6\u964d\u4f4e\uff1a - \u6807\u6ce8\u6570\u636e\u91cf\u5927\u5e45\u964d\u4f4e\uff0c\u964d\u4f4e90% - \u8ba1\u7b97\u8d44\u6e90\u5927\u5e45\u964d\u4f4e - \u7814\u53d1\u5468\u671f\u5927\u5e45\u964d\u4f4e \u5927\u6a21\u578b\u7684\u5de5\u5177\u548c\u5e73\u53f0 NLP\u5f00\u53d1\u5957\u4ef6 \u96be BML-\u6587\u672c\u5f00\u53d1\u5e73\u53f0 \u4e2d EasyDL-\u6587\u672c\u5f00\u53d1\u5e73\u53f0 \u6613 \u667a\u80fd\u6587\u6863\u5206\u6790\uff08TextMind\uff09 \u667a\u80fd\u521b\u4f5c\u51ed\u6761 \u667a\u80fd\u5bf9\u8bdd\u5e73\u53f0\uff08UNIT\uff09 NLP\u5f00\u53d1\u5957\u4ef6 \u63a8\u7406\u548c\u90e8\u7f72\u5f88\u5173\u952e\uff0c\u9700\u8981\u8f7b\u91cf\u5316 ERNIE-Gen \u3010\u6587\u672c\u751f\u6210\u4efb\u52a1\u3011\u5bf9\u6bd4\u540c\u5c3a\u5bf8\u901a\u7528\u6a21\u578bBLEU4\u63d0\u53472-5% ERNIE-IE \u3010\u6587\u672c\u62bd\u53d6\u4efb\u52a1\u3011\uff0c\u9002\u5408\u96f6\u6837\u672c\u5c11\u6837\u672c\u60c5\u51b5\uff0c\u5355\u6807\u7b7e5\u6761\u6837\u672c ERNIE-Rank \u3010\u68c0\u7d22\u6392\u5e8f\u4efb\u52a1\u3011\u63d0\u53472% ERNIE-Senta \u3010\u60c5\u611f\u5206\u6790\u4efb\u52a1\u3011\uff0c\u5bf9\u6bd4\u540c\u5c3a\u5bf8\u901a\u7528\u6a21\u578b\u5e73\u5747\u63d0\u53470.6% ERNIE-Sim \u3010\u6587\u672c\u5339\u914d\u4efb\u52a1\u3011\u5bf9\u6bd4\u540c\u5c3a\u5bf8\u901a\u7528\u6a21\u578b\u63d0\u534710% ERNIE-Doc \u3010\u957f\u6587\u672c\u5206\u7c7b\u5339\u914d\u62bd\u53d6\u4efb\u52a1\u3011\u7b49 ERNIE-M \u3010\u591a\u8bed\u8a00\u5206\u7c7b\u5339\u914d\u62bd\u53d6\u4efb\u52a1\u3011\u7b49 \u5e38\u89c1NLP\u573a\u666f\u4efb\u52a1\uff1a - \u6587\u672c\u5206\u7c7b-\u5355\u6807\u7b7e - \u6587\u672c\u5206\u7c7b-\u591a\u6807\u7b7e - \u60c5\u611f\u5206\u6790 - \u6587\u672c\u5339\u914d - \u5e8f\u5217\u6807\u6ce8 - \u547d\u540d\u5b9e\u4f53\u8bc6\u522b - \u673a\u5668\u9605\u8bfb\u7406\u89e3 - \u4e2d\u6587\u95ee\u7b54 - \u4fe1\u606f\u62bd\u53d6 - \u6458\u8981\u751f\u6210 - \u5b9e\u4f53\u8bc6\u522b - \u5173\u7cfb\u62bd\u53d6 - \u6587\u672c\u751f\u6210 - \u56fe\u6587\u68c0\u7d22 - \u5bcc\u5a92\u4f53\u62bd\u53d6 \u5b8c\u5584\u7684\u6570\u636e\u5904\u7406\u80fd\u529b - \u6570\u636e\u589e\u5f3a \u6807\u6ce8\u5f88\u5c11\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c31\u884c - \u968f\u673a\u66ff\u6362 - \u968f\u673a\u5220\u9664 - \u540c\u4e49\u8bcd\u66ff\u6362 - \u8bcd\u5411\u91cf\u8fd1\u4e49\u8bcd\u66ff\u6362 - MLM\u8fd1\u4e49\u8bcd\u66ff\u6362 \u4e3b\u52a8\u5b66\u4e60\u5f0f\u7684\u6807\u6ce8\u5de5\u5177: \u6a21\u578b\u9884\u6d4b+\u6807\u6ce8 \u7aef\u5230\u7aef\u5927\u6a21\u578b\u8f7b\u91cf\u5316\u65b9\u6848\uff1a\u663e\u8457\u964d\u4f4e\u5f00\u53d1\u8005\u90e8\u7f72\u5927\u6a21\u578b\u6210\u672c","title":"\u80cc\u666f"},{"location":"%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E5%BE%85%E5%AD%A6%E4%B9%A0%E5%88%97%E8%A1%A8/","text":"\u53c2\u6570\u670d\u52a1\u5668(Parameter Server) Ring All Reduce How to Train Really Large Models on Many GPUs https://lilianweng.github.io/posts/2021-09-25-train-large/","title":"\u5f85\u5b66\u4e60\u5217\u8868"},{"location":"%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96/","text":"","title":"\u6a21\u578b\u8f7b\u91cf\u5316"},{"location":"%E6%96%B9%E6%B3%95%E8%AE%BA/main/","text":"\u5982\u4f55\u505a\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b \u5904\u7406\u6570\u636e\u96c6\u7684\u4e00\u7cfb\u5217\u5de5\u5177 \u5bf9\u67d0\u4e2a\u6570\u636e\u5168\u8c8c\u7684\u523b\u753b\uff0c\u4f8b\u5982\u7c7b\u522b\u5206\u5e03\u3001hw\u5206\u5e03\u3001\u5927\u5c0f\u76ee\u6807\u5206\u5e03\u7b49 \u5bf9\u4e8e\u5206\u7c7b\u6570\u636e\u96c6: \u7c7b\u522b\u540d\u79f0\u3001\u7c7b\u522b\u540d\u79f0\u5230\u7d22\u5f15\u7684\u6620\u5c04","title":"Main"},{"location":"%E6%96%B9%E6%B3%95%E8%AE%BA/main/#_1","text":"\u5904\u7406\u6570\u636e\u96c6\u7684\u4e00\u7cfb\u5217\u5de5\u5177 \u5bf9\u67d0\u4e2a\u6570\u636e\u5168\u8c8c\u7684\u523b\u753b\uff0c\u4f8b\u5982\u7c7b\u522b\u5206\u5e03\u3001hw\u5206\u5e03\u3001\u5927\u5c0f\u76ee\u6807\u5206\u5e03\u7b49 \u5bf9\u4e8e\u5206\u7c7b\u6570\u636e\u96c6: \u7c7b\u522b\u540d\u79f0\u3001\u7c7b\u522b\u540d\u79f0\u5230\u7d22\u5f15\u7684\u6620\u5c04","title":"\u5982\u4f55\u505a\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/1.%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/","text":"\u9898\u76ee \u7ed9\u5b9a\u4e00\u4e2a\u5217\u8868\uff0c\u5217\u8868\u4e2d\u6bcf\u4e00\u9879\u662f\u4e00\u4e2a\u6574\u6570\uff1b\u7ed9\u5b9a\u4e00\u4e2a\u76ee\u6807\u503ctarget\uff0c\u8bf7\u95ee\u5217\u8868\u4e2d\u662f\u5426\u5b58\u5728\u4e24\u4e2a\u6570\uff0c\u5176\u548c\u7b49\u4e8etarget. \u793a\u4f8b1: \u8f93\u5165: nums = [2,7,11,15], target=9 \u8f93\u51fa: [0,1] \u56e0\u4e3anums\u7684\u7d22\u5f150\u5904\u7684\u503cnums[0] \u548c nums\u7684\u7d22\u5f151\u5904\u7684\u503cnums[1] \u4e4b\u548c nums[0] + nums[1] = target \u793a\u4f8b2: \u8f93\u5165: [3,2,4], target=6 \u8f93\u51fa: [1,2] \u56e0\u4e3anums\u7684\u7d22\u5f150\u5904\u7684\u503cnums[1] \u548c nums\u7684\u7d22\u5f151\u5904\u7684\u503cnums[2] \u4e4b\u548c nums[1] + nums[2] = target \u793a\u4f8b3: \u8f93\u5165: [3,3], target=6 \u8f93\u51fa: [0,1] \u56e0\u4e3anums\u7684\u7d22\u5f150\u5904\u7684\u503cnums[0] \u548c nums\u7684\u7d22\u5f151\u5904\u7684\u503cnums[1] \u4e4b\u548c nums[0] + nums[1] = target \u65b9\u6cd5\u4e00: \u66b4\u529b\u679a\u4e3e\u6cd5 \u601d\u8def\u53ca\u65b9\u6cd5 \u6700\u5bb9\u6613\u60f3\u5230\u7684\u65b9\u6cd5\u662f\u679a\u4e3e\u6570\u7ec4\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6570$x$\uff0c\u5bfb\u627e\u6570\u7ec4\u4e2d\u662f\u5426\u5b58\u5728$target-x$\u3002 \u5f53\u6211\u4eec\u4f7f\u7528\u904d\u5386\u6574\u4e2a\u6570\u7ec4\u7684\u65b9\u5f0f\u5bfb\u627e $target-x$ \u65f6\uff0c\u9700\u8981\u6ce8\u610f\u5230\u6bcf\u4e00\u4e2a\u4f4d\u4e8e$x$ \u4e4b\u524d\u7684\u5143\u7d20\u90fd\u5df2\u7ecf\u548c$x$\u5339\u914d\u8fc7\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u518d\u8fdb\u884c\u5339\u914d\u3002\u800c\u6bcf\u4e2a\u5143\u7d20\u4e0d\u80fd\u88ab\u4f7f\u7528\u4e24\u6b21\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u9700\u8981\u5728$x$\u540e\u9762\u7684\u5143\u7d20\u4e2d\u5bfb\u627e$target-x$\u3002 class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]: n = len(nums) for i in range(n): # \u53d6\u503c0,1,2,...,n-1 for j in range(i + 1, n): # \u53d6\u503ci+1, i+2, ..., n-1 if nums[i] + nums[j] == target: # nums[0] + nums[1]/nums[2]/.../nums[n-1]; nums[1] + nums[2]/nums[3]/.../nums[n-1]; ... return [i, j] # \u5b58\u5728\u4e00\u4e2a\u5c31\u8fd4\u56de return [] class Solution { public: vector<int> twoSum(vector<int>& nums, int target) { int n = nums.size(); for (int i = 0; i < n; ++i) { for (int j = i + 1; j < n; ++j) { if (nums[i] + nums[j] == target) { return {i, j}; } } } return {}; } }; int* twoSum(int* nums, int numsSize, int target, int* returnSize) { for (int i = 0; i < numsSize; ++i ) { for (int j = i + 1; j < numsSize; ++j) { if (nums[i] + nums[j] == target) { int* ret = malloc(sizeof(int) * 2); ret[0] = i, ret[1] = j; *returnSize = 2; return ret; } } } *returnSize = 0; return NULL; } \u65f6\u95f4\u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6: $O(N^2)$\uff0c\u5176\u4e2d$N$\u662f\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u3002\u6700\u574f\u60c5\u51b5\u4e0b\u6570\u7ec4\u4e2d\u4efb\u610f\u4e24\u4e2a\u6570\u90fd\u8981\u88ab\u5339\u914d\u4e00\u6b21\u3002 \u7a7a\u95f4\u590d\u6742\u5ea6: $O(1)$\u3002 \u65b9\u6cd5\u4e8c\uff1a\u54c8\u5e0c\u8868 \u601d\u8def\u53ca\u7b97\u6cd5 \u6ce8\u610f\u5230\u65b9\u6cd5\u4e00\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8f83\u9ad8\u7684\u539f\u56e0\u662f\u5bfb\u627e$target-x$\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8fc7\u9ad8\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u79c0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5feb\u901f\u5bfb\u627e\u6570\u7ec4\u4e2d\u662f\u5426\u5b58\u5728\u76ee\u6807\u5143\u7d20\u3002\u5982\u679c\u5b58\u5728\uff0c\u6211\u4eec\u9700\u8981\u627e\u51fa\u5b83\u7684\u7d22\u5f15\u3002 \u4f7f\u7528\u54c8\u5e0c\u8868\uff0c\u53ef\u4ee5\u5c06\u5bfb\u627e$target-x$\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4ece$O(N)$\u964d\u4f4e\u5230$O(1)$\u3002 \u8fd9\u6837\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u54c8\u5e0c\u8868\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a$x$\uff0c\u6211\u4eec\u9996\u5148\u67e5\u8be2\u54c8\u5e0c\u8868\u4e2d\u662f\u5426\u5b58\u5728$target-x$\uff0c\u7136\u540e\u5c06$x$\u63d2\u5165\u5230\u54c8\u5e0c\u8868\u4e2d\uff0c\u5373\u53ef\u4fdd\u8bc1\u4e0d\u4f1a\u8ba9$x$\u548c\u81ea\u5df1\u5339\u914d\u3002 class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i # hashtable\u7684key\u662fnums\u7684\u503c\uff0cnums\u503c\u7684\u7d22\u5f15\u662fhashtable\u7684value return [] class Solution { public: vector<int> twoSum(vector<int>& nums, int target) { unordered_map<int, int> hashtable; for (int i = 0; i < nums.size(); ++i){ auto it = hashtable.find(target - nums[i]); if (it != hashtable.end()) { return {it->second, i}; } hashtable[nums[i]] = i; } return {}; } }; struct hashTable { int key; int val; UT_hash_handle hh; // \u4ec0\u4e48\u542b\u4e49\uff1f }; struct hashTable* hashtable; struct hashTable* find(int ikey) { struct hashTable* tmp; HASH_FIND_INT(hashtable, &ikey, tmp); return tmp; } void insert(int ikey, int ival) { struct hashTable* it = find(ikey); if (it == NULL) { struct hashTable* tmp = malloc(sizeof(struct hashTable)); tmp->key = ikey, tmp->val = ival; HASH_ADD_INT(hashtable, key, tmp); } else { it->val = ival; } } int* twoSum(int* nums, int numsSize, int target, int* returnSize) { hashtable = NULL; for (int i = 0; i < numsSize; i++){ struct hashTable* it = find(target - nums[i]); if (it != NULL) { int* ret = malloc(sizeof(int) * 2); ret[0] = it->val, ret[1] = i; *returnSize = 2; return ret; } insert(nums[i], i); } *returnSize = 0; return NULL; } \u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6: $O(N)$, \u5176\u4e2d$N$\u662f\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u3002\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5143\u7d20$x$\uff0c\u6211\u4eec\u53ef\u4ee5$O(1)$\u5730\u5bfb\u627e$target-x$ \u7a7a\u95f4\u590d\u6742\u5ea6: $O(N)$, \u5176\u4e2d$N$\u662f\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u3002\u4e3b\u8981\u4e3a\u54c8\u5e0c\u8868\u7684\u5f00\u9500\u3002","title":"1.\u4e24\u6570\u4e4b\u548c"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/1.%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/#_1","text":"\u7ed9\u5b9a\u4e00\u4e2a\u5217\u8868\uff0c\u5217\u8868\u4e2d\u6bcf\u4e00\u9879\u662f\u4e00\u4e2a\u6574\u6570\uff1b\u7ed9\u5b9a\u4e00\u4e2a\u76ee\u6807\u503ctarget\uff0c\u8bf7\u95ee\u5217\u8868\u4e2d\u662f\u5426\u5b58\u5728\u4e24\u4e2a\u6570\uff0c\u5176\u548c\u7b49\u4e8etarget. \u793a\u4f8b1: \u8f93\u5165: nums = [2,7,11,15], target=9 \u8f93\u51fa: [0,1] \u56e0\u4e3anums\u7684\u7d22\u5f150\u5904\u7684\u503cnums[0] \u548c nums\u7684\u7d22\u5f151\u5904\u7684\u503cnums[1] \u4e4b\u548c nums[0] + nums[1] = target \u793a\u4f8b2: \u8f93\u5165: [3,2,4], target=6 \u8f93\u51fa: [1,2] \u56e0\u4e3anums\u7684\u7d22\u5f150\u5904\u7684\u503cnums[1] \u548c nums\u7684\u7d22\u5f151\u5904\u7684\u503cnums[2] \u4e4b\u548c nums[1] + nums[2] = target \u793a\u4f8b3: \u8f93\u5165: [3,3], target=6 \u8f93\u51fa: [0,1] \u56e0\u4e3anums\u7684\u7d22\u5f150\u5904\u7684\u503cnums[0] \u548c nums\u7684\u7d22\u5f151\u5904\u7684\u503cnums[1] \u4e4b\u548c nums[0] + nums[1] = target","title":"\u9898\u76ee"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/1.%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/#_2","text":"\u601d\u8def\u53ca\u65b9\u6cd5 \u6700\u5bb9\u6613\u60f3\u5230\u7684\u65b9\u6cd5\u662f\u679a\u4e3e\u6570\u7ec4\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6570$x$\uff0c\u5bfb\u627e\u6570\u7ec4\u4e2d\u662f\u5426\u5b58\u5728$target-x$\u3002 \u5f53\u6211\u4eec\u4f7f\u7528\u904d\u5386\u6574\u4e2a\u6570\u7ec4\u7684\u65b9\u5f0f\u5bfb\u627e $target-x$ \u65f6\uff0c\u9700\u8981\u6ce8\u610f\u5230\u6bcf\u4e00\u4e2a\u4f4d\u4e8e$x$ \u4e4b\u524d\u7684\u5143\u7d20\u90fd\u5df2\u7ecf\u548c$x$\u5339\u914d\u8fc7\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u518d\u8fdb\u884c\u5339\u914d\u3002\u800c\u6bcf\u4e2a\u5143\u7d20\u4e0d\u80fd\u88ab\u4f7f\u7528\u4e24\u6b21\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u9700\u8981\u5728$x$\u540e\u9762\u7684\u5143\u7d20\u4e2d\u5bfb\u627e$target-x$\u3002 class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]: n = len(nums) for i in range(n): # \u53d6\u503c0,1,2,...,n-1 for j in range(i + 1, n): # \u53d6\u503ci+1, i+2, ..., n-1 if nums[i] + nums[j] == target: # nums[0] + nums[1]/nums[2]/.../nums[n-1]; nums[1] + nums[2]/nums[3]/.../nums[n-1]; ... return [i, j] # \u5b58\u5728\u4e00\u4e2a\u5c31\u8fd4\u56de return [] class Solution { public: vector<int> twoSum(vector<int>& nums, int target) { int n = nums.size(); for (int i = 0; i < n; ++i) { for (int j = i + 1; j < n; ++j) { if (nums[i] + nums[j] == target) { return {i, j}; } } } return {}; } }; int* twoSum(int* nums, int numsSize, int target, int* returnSize) { for (int i = 0; i < numsSize; ++i ) { for (int j = i + 1; j < numsSize; ++j) { if (nums[i] + nums[j] == target) { int* ret = malloc(sizeof(int) * 2); ret[0] = i, ret[1] = j; *returnSize = 2; return ret; } } } *returnSize = 0; return NULL; } \u65f6\u95f4\u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6: $O(N^2)$\uff0c\u5176\u4e2d$N$\u662f\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u3002\u6700\u574f\u60c5\u51b5\u4e0b\u6570\u7ec4\u4e2d\u4efb\u610f\u4e24\u4e2a\u6570\u90fd\u8981\u88ab\u5339\u914d\u4e00\u6b21\u3002 \u7a7a\u95f4\u590d\u6742\u5ea6: $O(1)$\u3002","title":"\u65b9\u6cd5\u4e00: \u66b4\u529b\u679a\u4e3e\u6cd5"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/1.%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/#_3","text":"\u601d\u8def\u53ca\u7b97\u6cd5 \u6ce8\u610f\u5230\u65b9\u6cd5\u4e00\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8f83\u9ad8\u7684\u539f\u56e0\u662f\u5bfb\u627e$target-x$\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8fc7\u9ad8\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u79c0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5feb\u901f\u5bfb\u627e\u6570\u7ec4\u4e2d\u662f\u5426\u5b58\u5728\u76ee\u6807\u5143\u7d20\u3002\u5982\u679c\u5b58\u5728\uff0c\u6211\u4eec\u9700\u8981\u627e\u51fa\u5b83\u7684\u7d22\u5f15\u3002 \u4f7f\u7528\u54c8\u5e0c\u8868\uff0c\u53ef\u4ee5\u5c06\u5bfb\u627e$target-x$\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4ece$O(N)$\u964d\u4f4e\u5230$O(1)$\u3002 \u8fd9\u6837\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u54c8\u5e0c\u8868\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a$x$\uff0c\u6211\u4eec\u9996\u5148\u67e5\u8be2\u54c8\u5e0c\u8868\u4e2d\u662f\u5426\u5b58\u5728$target-x$\uff0c\u7136\u540e\u5c06$x$\u63d2\u5165\u5230\u54c8\u5e0c\u8868\u4e2d\uff0c\u5373\u53ef\u4fdd\u8bc1\u4e0d\u4f1a\u8ba9$x$\u548c\u81ea\u5df1\u5339\u914d\u3002 class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i # hashtable\u7684key\u662fnums\u7684\u503c\uff0cnums\u503c\u7684\u7d22\u5f15\u662fhashtable\u7684value return [] class Solution { public: vector<int> twoSum(vector<int>& nums, int target) { unordered_map<int, int> hashtable; for (int i = 0; i < nums.size(); ++i){ auto it = hashtable.find(target - nums[i]); if (it != hashtable.end()) { return {it->second, i}; } hashtable[nums[i]] = i; } return {}; } }; struct hashTable { int key; int val; UT_hash_handle hh; // \u4ec0\u4e48\u542b\u4e49\uff1f }; struct hashTable* hashtable; struct hashTable* find(int ikey) { struct hashTable* tmp; HASH_FIND_INT(hashtable, &ikey, tmp); return tmp; } void insert(int ikey, int ival) { struct hashTable* it = find(ikey); if (it == NULL) { struct hashTable* tmp = malloc(sizeof(struct hashTable)); tmp->key = ikey, tmp->val = ival; HASH_ADD_INT(hashtable, key, tmp); } else { it->val = ival; } } int* twoSum(int* nums, int numsSize, int target, int* returnSize) { hashtable = NULL; for (int i = 0; i < numsSize; i++){ struct hashTable* it = find(target - nums[i]); if (it != NULL) { int* ret = malloc(sizeof(int) * 2); ret[0] = it->val, ret[1] = i; *returnSize = 2; return ret; } insert(nums[i], i); } *returnSize = 0; return NULL; } \u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6: $O(N)$, \u5176\u4e2d$N$\u662f\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u3002\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5143\u7d20$x$\uff0c\u6211\u4eec\u53ef\u4ee5$O(1)$\u5730\u5bfb\u627e$target-x$ \u7a7a\u95f4\u590d\u6742\u5ea6: $O(N)$, \u5176\u4e2d$N$\u662f\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u3002\u4e3b\u8981\u4e3a\u54c8\u5e0c\u8868\u7684\u5f00\u9500\u3002","title":"\u65b9\u6cd5\u4e8c\uff1a\u54c8\u5e0c\u8868"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/159.%E8%87%B3%E5%A4%9A%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/","text":"1. \u9898\u76ee \u7ed9\u4f60\u4e00\u4e2a\u5b57\u7b26\u4e32 $s$\uff0c\u8bf7\u4f60\u627e\u51fa \u81f3\u591a \u5305\u542b \u4e24\u4e2a\u4e0d\u540c\u5b57\u7b26 \u7684\u6700\u957f\u5b50\u4e32\uff0c\u5e76\u8fd4\u56de\u8be5\u5b50\u4e32\u7684\u957f\u5ea6\u3002 \u793a\u4f8b1\uff1a \u8f93\u5165\uff1as = \"eceba\" \u8f93\u51fa\uff1a3 \u89e3\u91ca\uff1a\u6ee1\u8db3\u9898\u76ee\u8981\u6c42\u7684\u5b50\u4e32\u662f \"ece\"\uff0c\u957f\u5ea6\u4e3a3 \u793a\u4f8b2\uff1a \u8f93\u5165\uff1as = \"ccaabbb\" \u8f93\u51fa\uff1a5 \u89e3\u91ca\uff1a\u6ee1\u8db3\u9898\u76ee\u8981\u6c42\u7684\u5b50\u4e32\u662f \"aabbb\"\uff0c\u957f\u5ea6\u4e3a5 \u63d0\u793a\uff1a $1 <= s.length <= 10^5$ $s$\u7531\u82f1\u6587\u5b57\u6bcd\u7ec4\u6210 2. \u601d\u8def \u65b9\u6cd51\uff1a\u6ed1\u52a8\u7a97\u53e3 \u4e3a\u4e86\u904d\u5386\u4e00\u904d\u5c31\u80fd\u5f97\u5230\u7b54\u6848\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5de6\u6307\u9488\u548c\u4e00\u4e2a\u53f3\u6307\u9488\u8868\u793a\u6ed1\u52a8\u7a97\u53e3\u7684\u8fb9\u754c\u3002 \u4e00\u5f00\u59cb\uff0c\u8ba9\u4e24\u4e2a\u6307\u9488\u90fd\u6307\u54110\uff0c\u5f53\u7a97\u53e3\u5305\u542b\u7684\u5b57\u7b26\u4e0d\u8d85\u8fc72\u4e2a\u4e0d\u540c\u7684\u5b57\u7b26\u65f6\uff0c\u5c31\u4e0d\u65ad\u5c06\u53f3\u6307\u9488\u5f80\u53f3\u8fb9\u79fb\u52a8\u3002\u5982\u679c\u5728\u67d0\u4e00\u4e2a\u4f4d\u7f6e\u67093\u4e2a\u4e0d\u540c\u7684\u5b57\u7b26\uff0c\u5219\u5f00\u59cb\u79fb\u52a8\u5de6\u6307\u9488\uff0c\u76f4\u5230\u7a97\u53e3\u5185\u5305\u542b\u4e0d\u8d85\u8fc72\u4e2a\u4e0d\u540c\u5b57\u7b26\u3002 \u8fd9\u5c31\u662f\u57fa\u672c\u7684\u601d\u60f3\uff1a\u6cbf\u7740\u5b57\u7b26\u4e32\u79fb\u52a8\u6ed1\u52a8\u7a97\u53e3\uff0c\u5e76\u4fdd\u6301\u7a97\u53e3\u5185\u53ea\u6709\u4e0d\u8d85\u8fc72\u4e2a\u4e0d\u540c\u5b57\u7b26\uff0c\u540c\u65f6\u6bcf\u4e00\u6b65\u90fd\u66f4\u65b0\u6700\u957f\u5b50\u4e32\u7684\u957f\u5ea6\u3002 \u53ea\u6709\u4e00\u4e2a\u95ee\u9898\u8fd8\u6ca1\u6709\u89e3\u51b3 - \u5982\u4f55\u79fb\u52a8\u5de6\u6307\u9488\u786e\u4fdd\u6bcf\u4e00\u6b65\u90fd\u66f4\u65b0\u6700\u957f\u5b50\u4e32\u7684\u957f\u5ea6\u3002 \u6211\u4eec\u4f7f\u7528\u4e00\u4e2ahashmap\uff0c\u628a\u5b57\u7b26\u4e32\u7684\u5b57\u7b26\u90fd\u5f53\u4f5c\u952e\uff0c\u5728\u7a97\u53e3\u4e2d\u7684\u6700\u53f3\u8fb9\u7684\u5b57\u7b26\u4f4d\u7f6e\u4f5c\u4e3a\u503c\u3002\u6bcf\u4e00\u4e2a\u65f6\u523b\uff0c\u8fd9\u4e2ahashmap\u5305\u62ec\u4e0d\u8d85\u8fc73\u4e2a\u5143\u7d20\u3002 \u6bd4\u65b9\u8bf4\uff0c\u901a\u8fc7\u8fd9\u4e2ahashmap\uff0c\u4f60\u53ef\u4ee5\u77e5\u9053\u7a97\u53e3 \"eeeeeeeef\"\u4e2d\u5b57\u7b26\u7684\"e\"\u6700\u53f3\u8fb9\u7684\u4f4d\u7f6e\u662f8\uff0c\u6240\u4ee5\u5fc5\u987b\u8981\u81f3\u5c11\u5c06\u5de6\u6307\u9488\u79fb\u52a8\u52308+1=9\u7684\u4f4d\u7f6e\u6765\u5c06\"e\"\u4ece\u6ed1\u52a8\u7a97\u53e3\u4e2d\u79fb\u9664\u3002 \u6211\u4eec\u7684\u65b9\u6cd5\u65f6\u95f4\u590d\u6742\u5ea6\u662f\u5426\u662f\u6700\u4f18\u7684\u5462\uff1f\u7b54\u6848\u662f\u662f\u7684\u3002\u6211\u4eec\u53ea\u5c06\u5b57\u7b26\u4e32\u7684N\u4e2a\u5b57\u7b26\u904d\u5386\u4e86\u4e00\u6b21\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u662f $O(N)$\u3002 \u7b97\u6cd5\uff1a \u5982\u679cN\u7684\u957f\u5ea6\u5c0f\u4e8e3\uff0c\u8fd4\u56deN \u5c06\u5de6\u53f3\u6307\u9488\u90fd\u521d\u59cb\u5316\u6210\u5b57\u7b26\u4e32\u7684\u5de6\u7aef\u70b9\uff0cleft=0\u548cright=0\uff0c\u4e14\u521d\u59cb\u5316\u6700\u5927\u5b57\u7b26\u4e32\u4e3amax_len=2 \u5f53\u53f3\u6307\u9488\u5c0f\u4e8eN\u65f6\uff1a \u5982\u679chashmap\u5305\u542b\u5c0f\u4e8e3\u4e2a\u4e0d\u540c\u5b57\u7b26\uff0c\u90a3\u4e48\u5c06\u5f53\u524d\u5b57\u7b26s[right]\u653e\u5230hashmap\u4e2d\u5e76\u5c06\u53f3\u6307\u9488\u5f80\u53f3\u79fb\u52a8\u4e00\u6b21\u3002 \u5982\u679chashmap\u5305\u542b3\u4e2a\u4e0d\u540c\u5b57\u7b26\uff0c\u5c06\u6700\u5de6\u8fb9\u7684\u5b57\u7b26\u4ece\u54c8\u5e0c\u8868\u4e2d\u5220\u53bb\uff0c\u5e76\u79fb\u52a8\u5de6\u6307\u9488\uff0c\u4ee5\u4fbf\u6ed1\u52a8\u7a97\u53e3\u53ea\u5305\u542b2\u4e2a\u4e0d\u540c\u5b57\u7b26\u3002 \u66f4\u65b0max_len 3\u3001\u4ee3\u7801 from collections import defaultdict class Solution: def lengthOfLongestSubstringTwoDistinct(self, s: 'str') -> 'int': n = len(s) if n < 3: return n # sliding window left and right pointers left, right = 0, 0 # hashmap character -> its rightmost position in sliding window hashmap = defaultdict() max_len = 2 while right < n: # slidewindow contains less than 3 characters if len(hashmap) < 3: hashmap[s[right]] = right right += 1 # slidewindow contains 3 characters if len(hashmap) == 3: # delete the leftmost character del_idx = min(hashmap.values()) del hashmap[s[del_idx]] # move left pointer of the slidewindow left = del_idx + 1 max_len = max(max_len, right - left) # \u4e0d\u662fright-left+1\u4e48 return max_len \u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6\uff1a$O(N)$ \u5176\u4e2dN\u662f\u8f93\u5165\u4e32\u7684\u5b57\u7b26\u6570\u76ee \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a$O(1)$ \u8fd9\u662f\u56e0\u4e3a\u989d\u5916\u7684\u7a7a\u95f4\u53ea\u6709hashmap\uff0c\u4e14\u5b83\u4e0d\u8d85\u8fc73\u4e2a\u5143\u7d20","title":"159.\u81f3\u591a\u5305\u542b\u4e24\u4e2a\u4e0d\u540c\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/159.%E8%87%B3%E5%A4%9A%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#1","text":"\u7ed9\u4f60\u4e00\u4e2a\u5b57\u7b26\u4e32 $s$\uff0c\u8bf7\u4f60\u627e\u51fa \u81f3\u591a \u5305\u542b \u4e24\u4e2a\u4e0d\u540c\u5b57\u7b26 \u7684\u6700\u957f\u5b50\u4e32\uff0c\u5e76\u8fd4\u56de\u8be5\u5b50\u4e32\u7684\u957f\u5ea6\u3002 \u793a\u4f8b1\uff1a \u8f93\u5165\uff1as = \"eceba\" \u8f93\u51fa\uff1a3 \u89e3\u91ca\uff1a\u6ee1\u8db3\u9898\u76ee\u8981\u6c42\u7684\u5b50\u4e32\u662f \"ece\"\uff0c\u957f\u5ea6\u4e3a3 \u793a\u4f8b2\uff1a \u8f93\u5165\uff1as = \"ccaabbb\" \u8f93\u51fa\uff1a5 \u89e3\u91ca\uff1a\u6ee1\u8db3\u9898\u76ee\u8981\u6c42\u7684\u5b50\u4e32\u662f \"aabbb\"\uff0c\u957f\u5ea6\u4e3a5 \u63d0\u793a\uff1a $1 <= s.length <= 10^5$ $s$\u7531\u82f1\u6587\u5b57\u6bcd\u7ec4\u6210","title":"1. \u9898\u76ee"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/159.%E8%87%B3%E5%A4%9A%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#2","text":"\u65b9\u6cd51\uff1a\u6ed1\u52a8\u7a97\u53e3 \u4e3a\u4e86\u904d\u5386\u4e00\u904d\u5c31\u80fd\u5f97\u5230\u7b54\u6848\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5de6\u6307\u9488\u548c\u4e00\u4e2a\u53f3\u6307\u9488\u8868\u793a\u6ed1\u52a8\u7a97\u53e3\u7684\u8fb9\u754c\u3002 \u4e00\u5f00\u59cb\uff0c\u8ba9\u4e24\u4e2a\u6307\u9488\u90fd\u6307\u54110\uff0c\u5f53\u7a97\u53e3\u5305\u542b\u7684\u5b57\u7b26\u4e0d\u8d85\u8fc72\u4e2a\u4e0d\u540c\u7684\u5b57\u7b26\u65f6\uff0c\u5c31\u4e0d\u65ad\u5c06\u53f3\u6307\u9488\u5f80\u53f3\u8fb9\u79fb\u52a8\u3002\u5982\u679c\u5728\u67d0\u4e00\u4e2a\u4f4d\u7f6e\u67093\u4e2a\u4e0d\u540c\u7684\u5b57\u7b26\uff0c\u5219\u5f00\u59cb\u79fb\u52a8\u5de6\u6307\u9488\uff0c\u76f4\u5230\u7a97\u53e3\u5185\u5305\u542b\u4e0d\u8d85\u8fc72\u4e2a\u4e0d\u540c\u5b57\u7b26\u3002 \u8fd9\u5c31\u662f\u57fa\u672c\u7684\u601d\u60f3\uff1a\u6cbf\u7740\u5b57\u7b26\u4e32\u79fb\u52a8\u6ed1\u52a8\u7a97\u53e3\uff0c\u5e76\u4fdd\u6301\u7a97\u53e3\u5185\u53ea\u6709\u4e0d\u8d85\u8fc72\u4e2a\u4e0d\u540c\u5b57\u7b26\uff0c\u540c\u65f6\u6bcf\u4e00\u6b65\u90fd\u66f4\u65b0\u6700\u957f\u5b50\u4e32\u7684\u957f\u5ea6\u3002 \u53ea\u6709\u4e00\u4e2a\u95ee\u9898\u8fd8\u6ca1\u6709\u89e3\u51b3 - \u5982\u4f55\u79fb\u52a8\u5de6\u6307\u9488\u786e\u4fdd\u6bcf\u4e00\u6b65\u90fd\u66f4\u65b0\u6700\u957f\u5b50\u4e32\u7684\u957f\u5ea6\u3002 \u6211\u4eec\u4f7f\u7528\u4e00\u4e2ahashmap\uff0c\u628a\u5b57\u7b26\u4e32\u7684\u5b57\u7b26\u90fd\u5f53\u4f5c\u952e\uff0c\u5728\u7a97\u53e3\u4e2d\u7684\u6700\u53f3\u8fb9\u7684\u5b57\u7b26\u4f4d\u7f6e\u4f5c\u4e3a\u503c\u3002\u6bcf\u4e00\u4e2a\u65f6\u523b\uff0c\u8fd9\u4e2ahashmap\u5305\u62ec\u4e0d\u8d85\u8fc73\u4e2a\u5143\u7d20\u3002 \u6bd4\u65b9\u8bf4\uff0c\u901a\u8fc7\u8fd9\u4e2ahashmap\uff0c\u4f60\u53ef\u4ee5\u77e5\u9053\u7a97\u53e3 \"eeeeeeeef\"\u4e2d\u5b57\u7b26\u7684\"e\"\u6700\u53f3\u8fb9\u7684\u4f4d\u7f6e\u662f8\uff0c\u6240\u4ee5\u5fc5\u987b\u8981\u81f3\u5c11\u5c06\u5de6\u6307\u9488\u79fb\u52a8\u52308+1=9\u7684\u4f4d\u7f6e\u6765\u5c06\"e\"\u4ece\u6ed1\u52a8\u7a97\u53e3\u4e2d\u79fb\u9664\u3002 \u6211\u4eec\u7684\u65b9\u6cd5\u65f6\u95f4\u590d\u6742\u5ea6\u662f\u5426\u662f\u6700\u4f18\u7684\u5462\uff1f\u7b54\u6848\u662f\u662f\u7684\u3002\u6211\u4eec\u53ea\u5c06\u5b57\u7b26\u4e32\u7684N\u4e2a\u5b57\u7b26\u904d\u5386\u4e86\u4e00\u6b21\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u662f $O(N)$\u3002 \u7b97\u6cd5\uff1a \u5982\u679cN\u7684\u957f\u5ea6\u5c0f\u4e8e3\uff0c\u8fd4\u56deN \u5c06\u5de6\u53f3\u6307\u9488\u90fd\u521d\u59cb\u5316\u6210\u5b57\u7b26\u4e32\u7684\u5de6\u7aef\u70b9\uff0cleft=0\u548cright=0\uff0c\u4e14\u521d\u59cb\u5316\u6700\u5927\u5b57\u7b26\u4e32\u4e3amax_len=2 \u5f53\u53f3\u6307\u9488\u5c0f\u4e8eN\u65f6\uff1a \u5982\u679chashmap\u5305\u542b\u5c0f\u4e8e3\u4e2a\u4e0d\u540c\u5b57\u7b26\uff0c\u90a3\u4e48\u5c06\u5f53\u524d\u5b57\u7b26s[right]\u653e\u5230hashmap\u4e2d\u5e76\u5c06\u53f3\u6307\u9488\u5f80\u53f3\u79fb\u52a8\u4e00\u6b21\u3002 \u5982\u679chashmap\u5305\u542b3\u4e2a\u4e0d\u540c\u5b57\u7b26\uff0c\u5c06\u6700\u5de6\u8fb9\u7684\u5b57\u7b26\u4ece\u54c8\u5e0c\u8868\u4e2d\u5220\u53bb\uff0c\u5e76\u79fb\u52a8\u5de6\u6307\u9488\uff0c\u4ee5\u4fbf\u6ed1\u52a8\u7a97\u53e3\u53ea\u5305\u542b2\u4e2a\u4e0d\u540c\u5b57\u7b26\u3002 \u66f4\u65b0max_len","title":"2. \u601d\u8def"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/159.%E8%87%B3%E5%A4%9A%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#3","text":"from collections import defaultdict class Solution: def lengthOfLongestSubstringTwoDistinct(self, s: 'str') -> 'int': n = len(s) if n < 3: return n # sliding window left and right pointers left, right = 0, 0 # hashmap character -> its rightmost position in sliding window hashmap = defaultdict() max_len = 2 while right < n: # slidewindow contains less than 3 characters if len(hashmap) < 3: hashmap[s[right]] = right right += 1 # slidewindow contains 3 characters if len(hashmap) == 3: # delete the leftmost character del_idx = min(hashmap.values()) del hashmap[s[del_idx]] # move left pointer of the slidewindow left = del_idx + 1 max_len = max(max_len, right - left) # \u4e0d\u662fright-left+1\u4e48 return max_len \u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6\uff1a$O(N)$ \u5176\u4e2dN\u662f\u8f93\u5165\u4e32\u7684\u5b57\u7b26\u6570\u76ee \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a$O(1)$ \u8fd9\u662f\u56e0\u4e3a\u989d\u5916\u7684\u7a7a\u95f4\u53ea\u6709hashmap\uff0c\u4e14\u5b83\u4e0d\u8d85\u8fc73\u4e2a\u5143\u7d20","title":"3\u3001\u4ee3\u7801"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/","text":"1\u3001\u9898\u76ee \u7ed9\u4f60\u4e24\u4e2a\u975e\u7a7a\u7684\u94fe\u8868\uff0c\u8868\u793a\u4e24\u4e2a\u975e\u8d1f\u7684\u6574\u6570\u3002\u5b83\u4eec\u6bcf\u4f4d\u6570\u5b57\u90fd\u662f\u6309\u7167\u9006\u5e8f\u7684\u65b9\u5f0f\u5b58\u50a8\u7684\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8282\u70b9\u53ea\u80fd\u5b58\u50a8\u4e00\u4f4d\u6570\u5b57\u3002 \u8bf7\u4f60\u5c06\u4e24\u4e2a\u6570\u76f8\u52a0\uff0c\u5e76\u4ee5\u76f8\u540c\u5f62\u5f0f\u8fd4\u56de\u4e00\u4e2a\u8868\u793a\u548c\u7684\u94fe\u8868\u3002 2\u3001\u793a\u4f8b \u4f60\u53ef\u4ee5\u5047\u8bbe\u9664\u4e86\u6570\u5b570\u4e4b\u5916\uff0c\u8fd9\u4e24\u4e2a\u6570\u90fd\u4e0d\u4f1a\u4ee50\u5f00\u5934\u3002 \u793a\u4f8b1\uff1a \u8f93\u5165\uff1al1 = [2,4,3], l2 = [5,6,4] \u8f93\u51fa\uff1a[7,0,8] \u89e3\u91ca\uff1a342 + 465 = 807 \u793a\u4f8b2\uff1a \u8f93\u5165\uff1al1 = [0], l2 = [0] \u8f93\u51fa\uff1a[0] \u89e3\u91ca\uff1a0 + 0 = 0 \u793a\u4f8b3\uff1a \u8f93\u5165\uff1al1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] \u8f93\u51fa\uff1a[8,9,9,9,0,0,0,1] \u89e3\u91ca\uff1a9999999 + 9999 = 10009998 \u63d0\u793a\uff1a \u6bcf\u4e2a\u94fe\u8868\u4e2d\u7684\u8282\u70b9\u6570\u5728\u8303\u56f4[1,100]\u5185 $0<=Node.val<=9$ \u9898\u76ee\u6570\u636e\u4fdd\u8bc1\u5217\u8868\u8868\u793a\u7684\u6570\u5b57\u4e0d\u542b\u524d\u5bfc\u96f6 3\u3001\u601d\u8def\u4e0e\u7b97\u6cd5 \u7531\u4e8e\u8f93\u5165\u7684\u4e24\u4e2a\u94fe\u8868\u90fd\u662f \u9006\u5e8f \u5b58\u50a8\u6570\u5b57\u7684\u4f4d\u6570\u7684\uff0c\u56e0\u6b64\u4e24\u4e2a\u94fe\u8868\u4e2d\u540c\u4e00\u4f4d\u7f6e\u7684\u6570\u5b57\u53ef\u4ee5\u76f4\u63a5\u76f8\u52a0\u3002 \u6211\u4eec\u540c\u65f6\u904d\u5386\u4e24\u4e2a\u94fe\u8868\uff0c\u9010\u4f4d\u8ba1\u7b97\u5b83\u4eec\u7684\u548c\uff0c\u5e76\u4e0e\u5f53\u524d\u4f4d\u7f6e\u7684\u8fdb\u4f4d\u503c\u76f8\u52a0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5982\u679c\u5f53\u524d\u4e24\u4e2a\u94fe\u8868\u5904\u76f8\u5e94\u4f4d\u7f6e\u7684\u6570\u5b57\u4e3a$n1$,$n2$\uff0c\u8fdb\u4f4d\u503c\u4e3a$carry$\uff0c\u5219\u4ed6\u4eec\u7684\u548c\u4e3a$n1+n2+carry$;\u5176\u4e2d\uff0c\u7b54\u6848\u94fe\u8868\u5904\u76f8\u5e94\u4f4d\u7f6e\u7684\u6570\u5b57\u4e3a$(n1+n2+carry) \\%10$, \u800c\u8fdb\u4f4d\u503c\u4e3a$(n1+n2+carry) //10$\u3002 \u5982\u679c\u4e24\u4e2a\u94fe\u8868\u7684\u957f\u5ea6\u4e0d\u540c\uff0c\u5219\u53ef\u4ee5\u8ba4\u4e3a\u957f\u5ea6\u77ed\u7684\u94fe\u8868\u7684\u540e\u9762\u6709\u82e5\u5e72\u4e2a0\u3002 \u6b64\u5916\uff0c\u5982\u679c\u94fe\u8868\u904d\u5386\u7ed3\u675f\u540e\uff0c\u6709$carry>0$\uff0c\u8fd8\u9700\u8981\u5728\u7b54\u6848\u94fe\u8868\u7684\u540e\u9762\u9644\u52a0\u4e00\u4e2a\u8282\u70b9\uff0c\u8282\u70b9\u7684\u503c\u4e3a$carry$\u3002 4\u3001\u4ee3\u7801\u5b9e\u73b0 4.1 python\u4ee3\u7801 class Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -> ListNode: tmp_node = ListNode(0) head = tmp_node # carry = 0 while l1 or l2 or carry != 0: total = carry if l1: total += l1.val l1 = l1.next if l2: total += l2.val l2 = l2.next # set value if total <= 9: tmp_node.val = total carry = 0 else: tmp_node.val = total % 10 carry = total // 10 # create new node if l1 or l2 or carry != 0: tmp_node.next = ListNode(0) tmp_node = tmp_node.next return head 4.2 c++\u4ee3\u7801 class Solution { public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode *head = nullptr, *tail = nullptr; int carry = 0; while (l1 || l2) { int n1 = l1 ? l1->val: 0; int n2 = l2 ? l2->val: 0; int sum = n1 + n2 + carry; if (!head) { head = tail = new ListNode(sum % 10); } else { tail->next = new ListNode(sum % 10); tail = tail->next; } carry = sum / 10; if (l1) { l1 = l1->next; } if (l2) { l2 = l2->next; } } if (carry > 0) { tail->next = new ListNode(carry); } return head; } };","title":"2.\u4e24\u6570\u76f8\u52a0"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/#1","text":"\u7ed9\u4f60\u4e24\u4e2a\u975e\u7a7a\u7684\u94fe\u8868\uff0c\u8868\u793a\u4e24\u4e2a\u975e\u8d1f\u7684\u6574\u6570\u3002\u5b83\u4eec\u6bcf\u4f4d\u6570\u5b57\u90fd\u662f\u6309\u7167\u9006\u5e8f\u7684\u65b9\u5f0f\u5b58\u50a8\u7684\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8282\u70b9\u53ea\u80fd\u5b58\u50a8\u4e00\u4f4d\u6570\u5b57\u3002 \u8bf7\u4f60\u5c06\u4e24\u4e2a\u6570\u76f8\u52a0\uff0c\u5e76\u4ee5\u76f8\u540c\u5f62\u5f0f\u8fd4\u56de\u4e00\u4e2a\u8868\u793a\u548c\u7684\u94fe\u8868\u3002","title":"1\u3001\u9898\u76ee"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/#2","text":"\u4f60\u53ef\u4ee5\u5047\u8bbe\u9664\u4e86\u6570\u5b570\u4e4b\u5916\uff0c\u8fd9\u4e24\u4e2a\u6570\u90fd\u4e0d\u4f1a\u4ee50\u5f00\u5934\u3002 \u793a\u4f8b1\uff1a \u8f93\u5165\uff1al1 = [2,4,3], l2 = [5,6,4] \u8f93\u51fa\uff1a[7,0,8] \u89e3\u91ca\uff1a342 + 465 = 807 \u793a\u4f8b2\uff1a \u8f93\u5165\uff1al1 = [0], l2 = [0] \u8f93\u51fa\uff1a[0] \u89e3\u91ca\uff1a0 + 0 = 0 \u793a\u4f8b3\uff1a \u8f93\u5165\uff1al1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] \u8f93\u51fa\uff1a[8,9,9,9,0,0,0,1] \u89e3\u91ca\uff1a9999999 + 9999 = 10009998 \u63d0\u793a\uff1a \u6bcf\u4e2a\u94fe\u8868\u4e2d\u7684\u8282\u70b9\u6570\u5728\u8303\u56f4[1,100]\u5185 $0<=Node.val<=9$ \u9898\u76ee\u6570\u636e\u4fdd\u8bc1\u5217\u8868\u8868\u793a\u7684\u6570\u5b57\u4e0d\u542b\u524d\u5bfc\u96f6","title":"2\u3001\u793a\u4f8b"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/#3","text":"\u7531\u4e8e\u8f93\u5165\u7684\u4e24\u4e2a\u94fe\u8868\u90fd\u662f \u9006\u5e8f \u5b58\u50a8\u6570\u5b57\u7684\u4f4d\u6570\u7684\uff0c\u56e0\u6b64\u4e24\u4e2a\u94fe\u8868\u4e2d\u540c\u4e00\u4f4d\u7f6e\u7684\u6570\u5b57\u53ef\u4ee5\u76f4\u63a5\u76f8\u52a0\u3002 \u6211\u4eec\u540c\u65f6\u904d\u5386\u4e24\u4e2a\u94fe\u8868\uff0c\u9010\u4f4d\u8ba1\u7b97\u5b83\u4eec\u7684\u548c\uff0c\u5e76\u4e0e\u5f53\u524d\u4f4d\u7f6e\u7684\u8fdb\u4f4d\u503c\u76f8\u52a0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5982\u679c\u5f53\u524d\u4e24\u4e2a\u94fe\u8868\u5904\u76f8\u5e94\u4f4d\u7f6e\u7684\u6570\u5b57\u4e3a$n1$,$n2$\uff0c\u8fdb\u4f4d\u503c\u4e3a$carry$\uff0c\u5219\u4ed6\u4eec\u7684\u548c\u4e3a$n1+n2+carry$;\u5176\u4e2d\uff0c\u7b54\u6848\u94fe\u8868\u5904\u76f8\u5e94\u4f4d\u7f6e\u7684\u6570\u5b57\u4e3a$(n1+n2+carry) \\%10$, \u800c\u8fdb\u4f4d\u503c\u4e3a$(n1+n2+carry) //10$\u3002 \u5982\u679c\u4e24\u4e2a\u94fe\u8868\u7684\u957f\u5ea6\u4e0d\u540c\uff0c\u5219\u53ef\u4ee5\u8ba4\u4e3a\u957f\u5ea6\u77ed\u7684\u94fe\u8868\u7684\u540e\u9762\u6709\u82e5\u5e72\u4e2a0\u3002 \u6b64\u5916\uff0c\u5982\u679c\u94fe\u8868\u904d\u5386\u7ed3\u675f\u540e\uff0c\u6709$carry>0$\uff0c\u8fd8\u9700\u8981\u5728\u7b54\u6848\u94fe\u8868\u7684\u540e\u9762\u9644\u52a0\u4e00\u4e2a\u8282\u70b9\uff0c\u8282\u70b9\u7684\u503c\u4e3a$carry$\u3002","title":"3\u3001\u601d\u8def\u4e0e\u7b97\u6cd5"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/#4","text":"","title":"4\u3001\u4ee3\u7801\u5b9e\u73b0"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/#41-python","text":"class Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -> ListNode: tmp_node = ListNode(0) head = tmp_node # carry = 0 while l1 or l2 or carry != 0: total = carry if l1: total += l1.val l1 = l1.next if l2: total += l2.val l2 = l2.next # set value if total <= 9: tmp_node.val = total carry = 0 else: tmp_node.val = total % 10 carry = total // 10 # create new node if l1 or l2 or carry != 0: tmp_node.next = ListNode(0) tmp_node = tmp_node.next return head","title":"4.1 python\u4ee3\u7801"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/2.%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/#42-c","text":"class Solution { public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode *head = nullptr, *tail = nullptr; int carry = 0; while (l1 || l2) { int n1 = l1 ? l1->val: 0; int n2 = l2 ? l2->val: 0; int sum = n1 + n2 + carry; if (!head) { head = tail = new ListNode(sum % 10); } else { tail->next = new ListNode(sum % 10); tail = tail->next; } carry = sum / 10; if (l1) { l1 = l1->next; } if (l2) { l2 = l2->next; } } if (carry > 0) { tail->next = new ListNode(carry); } return head; } };","title":"4.2 c++\u4ee3\u7801"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/","text":"1\u3001\u9898\u76ee \u7ed9\u5b9a\u4e00\u4e2a\u5b57\u7b26\u4e32$s$\uff0c\u8bf7\u4f60\u627e\u51fa\u5176\u4e2d\u4e0d\u5305\u542b\u6709\u91cd\u590d\u5b57\u7b26\u7684 \u6700\u957f\u5b50\u4e32 \u7684\u957f\u5ea6\u3002 2\u3001\u793a\u4f8b \u793a\u4f8b1\uff1a \u8f93\u5165\uff1as = \"abcabcbb\" \u8f93\u51fa\uff1a3 \u89e3\u91ca\uff1a\u56e0\u4e3a\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32\u662f\"abc\"\uff0c\u6240\u4ee5\u5176\u957f\u5ea6\u4e3a3. \u793a\u4f8b2\uff1a \u8f93\u5165\uff1as = \"bbbbb\" \u8f93\u51fa\uff1a1 \u89e3\u91ca\uff1a\u56e0\u4e3a\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32\u662f\"b\"\uff0c\u6240\u4ee5\u5176\u957f\u5ea6\u4e3a1. \u793a\u4f8b3\uff1a \u8f93\u5165\uff1as = \"pwwkew\" \u8f93\u51fa\uff1a3 \u89e3\u91ca\uff1a\u56e0\u4e3a\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32\u662f\"wke\"\uff0c\u6240\u4ee5\u5176\u957f\u5ea6\u4e3a3.\u8bf7\u6ce8\u610f\uff0c\u4f60\u7684\u7b54\u6848\u5fc5\u987b\u662f **\u5b50\u4e32** \u7684\u957f\u5ea6\uff0c\"pwke\" \u662f\u4e00\u4e2a\u5b50\u5e8f\u5217\uff0c\u4e0d\u662f\u5b50\u4e32\u3002 \u63d0\u793a\uff1a $0 <= s.length <= 5 * 10^4$ s\u7531\u82f1\u6587\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7b26\u53f7\u548c\u7a7a\u683c\u7ec4\u6210 3\u3001\u601d\u8def \u8fd9\u9053\u9898\u4e3b\u8981\u7528\u5230\u601d\u8def\u662f\uff1a\u6ed1\u52a8\u7a97\u53e3 \u4ec0\u4e48\u662f\u6ed1\u52a8\u7a97\u53e3\uff1f \u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u961f\u5217\uff0c\u6bd4\u5982\u4f8b\u9898\u4e2d\u7684 $abcabcbb$\uff0c \u8fdb\u5165\u8fd9\u4e2a\u961f\u5217\uff08\u7a97\u53e3\uff09\u4e3a $abc$ \u6ee1\u8db3\u9898\u76ee\u8981\u6c42\uff0c\u5f53\u518d\u8fdb\u5165$a$\uff0c\u961f\u5217\u53d8\u6210 $abca$\uff0c\u8fd9\u65f6\u5019\u4e0d\u6ee1\u8db3\u8981\u6c42\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u8981\u79fb\u52a8\u8fd9\u4e2a\u961f\u5217\uff01 \u5982\u4f55\u79fb\u52a8\uff1f \u6211\u4eec\u53ea\u8981\u628a\u961f\u5217\u7684\u5de6\u8fb9\u7684\u5143\u7d20\u79fb\u51fa\u5c31\u884c\u4e86\uff0c\u76f4\u5230\u6ee1\u8db3\u9898\u76ee\u8981\u6c42\uff01 \u4e00\u76f4\u7ef4\u6301\u8fd9\u6837\u7684\u961f\u5217\uff0c\u627e\u51fa\u961f\u5217\u51fa\u73b0\u6700\u957f\u7684\u957f\u5ea6\u65f6\u5019\uff0c\u6c42\u51fa\u89e3\uff01 \u65f6\u95f4\u590d\u6742\u5ea6\uff1a$O(N)$ 4\u3001\u4ee3\u7801 4.1 python\u4ee3\u7801 class Solution: def lengthOfLongestSubstring(self, s: str) -> int: if not s: return 0 left = 0 lookup = set() n = len(s) max_len = 0 cur_len = 0 for i in range(n): cur_len += 1 while s[i] in lookup: lookup.remove(s[left]) left += 1 cur_len -= 1 if cur_len > max_len: max_len = cur_len lookup.add(s[i]) return max_len \u6267\u884c\u7528\u65f6: 48ms \u5185\u5b58\u6d88\u8017: 15.2MB 4.2 c++\u4ee3\u7801 class Solution{ public: int lengthOfLongestSubstring(string s) { if (s.size() == 0) return 0; unordered_set<char> lookup; int maxStr = 0; int left = 0; for(int i = 0; i < s.size(); i++){ while (lookup.find(s[i]) != lookup.end()) { lookup.erase(s[left]); left ++; } maxStr = max(maxStr, i - left + 1); lookup.insert(s[i]); } return maxStr; } }; \u6267\u884c\u7528\u65f6: 28ms \u5185\u5b58\u9650\u53f7: 10.6MB","title":"3.\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#1","text":"\u7ed9\u5b9a\u4e00\u4e2a\u5b57\u7b26\u4e32$s$\uff0c\u8bf7\u4f60\u627e\u51fa\u5176\u4e2d\u4e0d\u5305\u542b\u6709\u91cd\u590d\u5b57\u7b26\u7684 \u6700\u957f\u5b50\u4e32 \u7684\u957f\u5ea6\u3002","title":"1\u3001\u9898\u76ee"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#2","text":"\u793a\u4f8b1\uff1a \u8f93\u5165\uff1as = \"abcabcbb\" \u8f93\u51fa\uff1a3 \u89e3\u91ca\uff1a\u56e0\u4e3a\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32\u662f\"abc\"\uff0c\u6240\u4ee5\u5176\u957f\u5ea6\u4e3a3. \u793a\u4f8b2\uff1a \u8f93\u5165\uff1as = \"bbbbb\" \u8f93\u51fa\uff1a1 \u89e3\u91ca\uff1a\u56e0\u4e3a\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32\u662f\"b\"\uff0c\u6240\u4ee5\u5176\u957f\u5ea6\u4e3a1. \u793a\u4f8b3\uff1a \u8f93\u5165\uff1as = \"pwwkew\" \u8f93\u51fa\uff1a3 \u89e3\u91ca\uff1a\u56e0\u4e3a\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32\u662f\"wke\"\uff0c\u6240\u4ee5\u5176\u957f\u5ea6\u4e3a3.\u8bf7\u6ce8\u610f\uff0c\u4f60\u7684\u7b54\u6848\u5fc5\u987b\u662f **\u5b50\u4e32** \u7684\u957f\u5ea6\uff0c\"pwke\" \u662f\u4e00\u4e2a\u5b50\u5e8f\u5217\uff0c\u4e0d\u662f\u5b50\u4e32\u3002 \u63d0\u793a\uff1a $0 <= s.length <= 5 * 10^4$ s\u7531\u82f1\u6587\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7b26\u53f7\u548c\u7a7a\u683c\u7ec4\u6210","title":"2\u3001\u793a\u4f8b"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#3","text":"\u8fd9\u9053\u9898\u4e3b\u8981\u7528\u5230\u601d\u8def\u662f\uff1a\u6ed1\u52a8\u7a97\u53e3 \u4ec0\u4e48\u662f\u6ed1\u52a8\u7a97\u53e3\uff1f \u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u961f\u5217\uff0c\u6bd4\u5982\u4f8b\u9898\u4e2d\u7684 $abcabcbb$\uff0c \u8fdb\u5165\u8fd9\u4e2a\u961f\u5217\uff08\u7a97\u53e3\uff09\u4e3a $abc$ \u6ee1\u8db3\u9898\u76ee\u8981\u6c42\uff0c\u5f53\u518d\u8fdb\u5165$a$\uff0c\u961f\u5217\u53d8\u6210 $abca$\uff0c\u8fd9\u65f6\u5019\u4e0d\u6ee1\u8db3\u8981\u6c42\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u8981\u79fb\u52a8\u8fd9\u4e2a\u961f\u5217\uff01 \u5982\u4f55\u79fb\u52a8\uff1f \u6211\u4eec\u53ea\u8981\u628a\u961f\u5217\u7684\u5de6\u8fb9\u7684\u5143\u7d20\u79fb\u51fa\u5c31\u884c\u4e86\uff0c\u76f4\u5230\u6ee1\u8db3\u9898\u76ee\u8981\u6c42\uff01 \u4e00\u76f4\u7ef4\u6301\u8fd9\u6837\u7684\u961f\u5217\uff0c\u627e\u51fa\u961f\u5217\u51fa\u73b0\u6700\u957f\u7684\u957f\u5ea6\u65f6\u5019\uff0c\u6c42\u51fa\u89e3\uff01 \u65f6\u95f4\u590d\u6742\u5ea6\uff1a$O(N)$","title":"3\u3001\u601d\u8def"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#4","text":"","title":"4\u3001\u4ee3\u7801"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#41-python","text":"class Solution: def lengthOfLongestSubstring(self, s: str) -> int: if not s: return 0 left = 0 lookup = set() n = len(s) max_len = 0 cur_len = 0 for i in range(n): cur_len += 1 while s[i] in lookup: lookup.remove(s[left]) left += 1 cur_len -= 1 if cur_len > max_len: max_len = cur_len lookup.add(s[i]) return max_len \u6267\u884c\u7528\u65f6: 48ms \u5185\u5b58\u6d88\u8017: 15.2MB","title":"4.1 python\u4ee3\u7801"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/3.%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/#42-c","text":"class Solution{ public: int lengthOfLongestSubstring(string s) { if (s.size() == 0) return 0; unordered_set<char> lookup; int maxStr = 0; int left = 0; for(int i = 0; i < s.size(); i++){ while (lookup.find(s[i]) != lookup.end()) { lookup.erase(s[left]); left ++; } maxStr = max(maxStr, i - left + 1); lookup.insert(s[i]); } return maxStr; } }; \u6267\u884c\u7528\u65f6: 28ms \u5185\u5b58\u9650\u53f7: 10.6MB","title":"4.2 c++\u4ee3\u7801"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/340.%E8%87%B3%E5%A4%9A%E5%8C%85%E5%90%ABK%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/","text":"","title":"340.\u81f3\u591a\u5305\u542bK\u4e2a\u4e0d\u540c\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/76.%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/","text":"1\u3001\u9898\u76ee \u7ed9\u4f60\u4e00\u4e2a\u5b57\u7b26\u4e32 $s$\u3001\u4e00\u4e2a\u5b57\u7b26\u4e32 $t$\u3002\u8fd4\u56de $s$ \u4e2d\u6db5\u76d6 $t$ \u6240\u6709\u5b57\u7b26\u7684\u6700\u5c0f\u5b50\u4e32\u3002\u5982\u679c $s$ \u4e2d\u4e0d\u5b58\u5728\u6db5\u76d6 $t$ \u6240\u6709\u5b57\u7b26\u7684\u5b50\u4e32\uff0c\u5219\u8fd4\u56de\u7a7a\u5b57\u7b26\u4e32 \"\"\u3002 \u6ce8\u610f\uff1a \u5bf9\u4e8e $t$ \u4e2d\u91cd\u590d\u5b57\u7b26\uff0c\u6211\u4eec\u5bfb\u627e\u7684\u5b50\u5b57\u7b26\u4e32\u4e2d\u8be5\u5b57\u7b26\u6570\u91cf\u5fc5\u987b\u4e0d\u5c11\u4e8e $t$ \u4e2d\u8be5\u5b57\u7b26\u7684\u6570\u91cf\u3002 \u5982\u679c $s$ \u4e2d\u5b58\u5728\u8fd9\u6837\u7684\u5b50\u4e32\uff0c\u6211\u4eec\u4fdd\u8bc1\u5b83\u662f\u552f\u4e00\u7684\u7b54\u6848\u3002 2\u3001\u793a\u4f8b \u793a\u4f8b1\uff1a \u8f93\u5165\uff1as = \"ADOBECODEBANC\", t = \"ABC\" \u8f93\u51fa\uff1a\"BANC\" \u793a\u4f8b2\uff1a \u8f93\u5165\uff1as = \"a\", t = \"a\" \u8f93\u51fa\uff1a\"a\" \u793a\u4f8b3\uff1a \u8f93\u5165\uff1as = \"a\", t = \"aa\" \u8f93\u51fa\uff1a\"\" \u89e3\u91ca\uff1at\u4e2d\u4e24\u4e2a\u5b57\u7b26 'a' \u5747\u5e94\u5305\u542b\u5728 s \u7684\u5b50\u4e32\u4e2d\uff0c\u56e0\u6b64\u6ca1\u6709\u7b26\u5408\u6761\u4ef6\u7684\u5b50\u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7a7a\u5b57\u7b26\u4e32\u3002 3\u3001\u601d\u8def \u672c\u95ee\u9898\u8981\u6c42\u6211\u4eec\u8fd4\u56de\u5b57\u7b26\u4e32 $s$ \u4e2d\u5305\u542b\u5b57\u7b26\u4e32 $t$ \u7684\u5168\u90e8\u5b57\u7b26\u7684\u6700\u5c0f\u7a97\u53e3\u3002\u6211\u4eec\u79f0\u5305\u542b $t$ \u7684\u5168\u90e8\u5b57\u6bcd\u7684\u7a97\u53e3\u4e3a\u300c\u53ef\u884c\u300d\u7a97\u53e3\u3002 \u6211\u4eec\u53ef\u4ee5\u7528\u6ed1\u52a8\u7a97\u53e3\u7684\u601d\u60f3\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u6ed1\u52a8\u7a97\u53e3\u7c7b\u578b\u7684\u95ee\u9898\u4e2d\u90fd\u4f1a\u6709\u4e24\u4e2a\u6307\u9488\uff0c\u4e00\u4e2a\u7528\u4e8e\u300c\u5ef6\u4f38\u300d\u73b0\u6709\u7a97\u53e3 $r$ \u6307\u9488\uff0c\u548c\u4e00\u4e2a\u7528\u4e8e\u300c\u6536\u7f29\u300d\u7a97\u53e3\u7684 $l$ \u6307\u9488\u3002\u5728\u4efb\u610f\u65f6\u523b\uff0c\u53ea\u6709\u4e00\u4e2a\u6307\u9488\u8fd0\u52a8\uff0c\u800c\u53e6\u4e00\u4e2a\u6307\u9488\u4fdd\u6301\u9759\u6b62\u3002\u6211\u4eec\u5728 $s$ \u4e0a\u6ed1\u52a8\u7a97\u53e3\uff0c\u901a\u8fc7\u79fb\u52a8 $r$ \u6307\u9488\u4e0d\u65ad\u6269\u5f20\u7a97\u53e3\u3002\u5f53\u7a97\u53e3\u5305\u542b $t$ \u5168\u90e8\u6240\u9700\u7684\u5b57\u7b26\u540e\uff0c\u5982\u679c\u80fd\u6536\u7f29\uff0c\u6211\u4eec\u5c31\u6536\u7f29\u7a97\u53e3\u76f4\u5230\u5f97\u5230\u6700\u5c0f\u7a97\u53e3\u3002 \u5982\u4f55\u5224\u65ad\u5f53\u524d\u7684\u7a97\u53e3\u5305\u542b\u6240\u6709 $t$ \u6240\u9700\u7684\u5b57\u7b26\u5462\uff1f\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u54c8\u5e0c\u8868\u8868\u793a $t$ \u4e2d\u6240\u6709\u7684\u5b57\u7b26\u4ee5\u53ca\u5b83\u4eec\u7684\u4e2a\u6570\uff0c\u7528\u4e00\u4e2a\u54c8\u5e0c\u8868\u52a8\u6001\u7ef4\u62a4\u7a97\u53e3\u4e2d\u6240\u6709\u7684\u5b57\u7b26\u4ee5\u53ca\u5b83\u4eec\u7684\u4e2a\u6570\uff0c\u5982\u679c\u8fd9\u4e2a\u52a8\u6001\u8868\u4e2d\u5305\u542b $t$ \u7684\u54c8\u5e0c\u8868\u4e2d\u7684\u6240\u6709\u5b57\u7b26\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u4e2a\u6570\u90fd\u4e0d\u5c0f\u4e8e $t$ \u7684\u54c8\u5e0c\u8868\u4e2d\u5404\u4e2a\u5b57\u7b26\u7684\u4e2a\u6570\uff0c\u90a3\u4e48\u5f53\u524d\u7a97\u53e3\u662f\u300c\u53ef\u884c\u300d\u7684\u3002 *\u6ce8\u610f\uff1a\u8fd9\u91cc$t$\u4e2d\u51fa\u73b0\u91cd\u590d\u7684\u5b57\u7b26\uff0c\u6240\u4ee5\u6211\u4eec\u8981\u8bb0\u5f55\u5b57\u7b26\u7684\u4e2a\u6570\u3002 \u8003\u8651\u5982\u4f55\u4f18\u5316\uff1f\u5982\u679c$s=XX...XABCXXXX,t=ABC$\uff0c\u90a3\u4e48\u663e\u7136$[XX..XXXABC]$\u662f\u7b2c\u4e00\u4e2a\u5f97\u5230\u7684\u300c\u53ef\u884c\u300d\u53d6\u4ef6\uff0c\u5f97\u5230\u8fd9\u4e2a\u53ef\u884c\u533a\u95f4\u540e\uff0c\u6211\u4eec\u6309\u7167\u300c\u6536\u7f29\u300d\u7a97\u53e3\u7684\u539f\u5219\u66f4\u65b0\u5de6\u8fb9\u754c\uff0c\u5f97\u5230\u6700\u5c0f\u533a\u95f4\u3002\u6211\u4eec\u5176\u5b9e\u505a\u4e86\u4e00\u4e9b\u65e0\u7528\u64cd\u4f5c\uff0c\u5c31\u662f\u66f4\u65b0\u53f3\u8fb9\u754c\u7684\u65f6\u5019\u300c\u5ef6\u4f38\u300d\u8fdb\u4e86\u5f88\u591a\u65e0\u7528\u7684$X$\uff0c\u66f4\u65b0\u5de6\u8fb9\u754c\u7684\u65f6\u5019\u300c\u6536\u7f29\u300d\u6254\u6389\u4e86\u8fd9\u4e9b\u65e0\u7528\u7684$X$\uff0c\u505a\u4e86\u8fd9\u4e48\u591a\u65e0\u7528\u7684\u64cd\u4f5c\uff0c\u53ea\u662f\u4e3a\u4e86\u5f97\u5230\u77ed\u77ed\u7684$ABC$\u3002\u6ca1\u9519\uff0c\u5176\u5b9e\u5728$s$\u4e2d\uff0c\u6709\u7684\u5b57\u7b26\u6211\u4eec\u662f\u4e0d\u5173\u5fc3\u7684\uff0c\u6211\u4eec\u53ea\u5173\u5fc3$t$\u51fa\u73b0\u7684\u5b57\u7b26\uff0c\u6211\u4eec\u53ef\u4e0d\u53ef\u4ee5\u5148\u9884\u5904\u7406$s$\uff0c\u6254\u6389\u90a3\u4e9b$t$\u4e2d\u6ca1\u6709\u51fa\u73b0\u7684\u5b57\u7b26\uff0c\u7136\u540e\u518d\u505a\u6ed1\u52a8\u7a97\u53e3\u5462\uff1f\u4e5f\u8bb8\u4f60\u4f1a\u8bf4\uff0c\u8fd9\u6837\u53ef\u80fd\u51fa\u73b0$XXABXXC$\u7684\u60c5\u51b5\uff0c\u5728\u7edf\u8ba1\u957f\u5ea6\u7684\u65f6\u5019\u53ef\u4ee5\u6254\u6389\u524d\u4e24\u4e2a$X$\uff0c\u4f46\u662f\u4ecd\u4e0d\u6389\u4e2d\u95f4\u7684$X$\uff0c\u600e\u6837\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5462\uff1f\u4f18\u5316\u540e\u7684\u65f6\u7a7a\u590d\u6742\u5ea6\u53c8\u662f\u591a\u5c11\u5462\uff1f\u8fd9\u91cc\u4ee3\u7801\u7ed9\u51fa\u6ca1\u6709\u4f18\u5316\u7684\u7248\u672c\uff0c\u4ee5\u4e0a\u7684\u4e09\u4e2a\u95ee\u9898\u7559\u7ed9\u8bfb\u7740\u601d\u8003\u3002 4\u3001\u4ee3\u7801 class Solution: def minWindow(self, s: 'str', t: 'str') -> 'str': from collections import defaultdict lookup = defaultdict(int) for c in t: lookup[c] += 1 start = 0 end = 0 min_len = float(\"inf\") counter = len(t) res = \"\" while end < len(s): if lookup[s[end]] > 0: counter -= 1 lookup[s[end]] -= 1 end += 1 while counter == 0: if min_len > end - start: min_len = end - start res = s[start: end] if lookup[s[start]] == 0: counter += 1 lookup[s[start]] += 1 start += 1 return res \u6267\u884c\u7528\u65f6\uff1a64 ms \u5185\u5b58\u6d88\u8017\uff1a15.5 MB class Solution { public: unordered_map <char, int> ori, cnt; bool check() { for (const auto &p: ori) { if (cnt[p.first] < p.second) { return false; } } return true; } string minWindow(string s, string t) { for (const auto &c: t) { ++ori[c]; } int l = 0, r = -1; int len = INT_MAX, ansL = -1, ansR = -1; while (r < int(s.size())) { if (ori.find(s[++r]) != ori.end()) { ++cnt[s[r]]; } while (check() && l <= r) { if (r - l + 1 < len) { len = r - l + 1; ansL = l; } if (ori.find(s[l]) != ori.end()) { --cnt[s[l]]; } ++l; } } return ansL == -1 ? string() : s.substr(ansL, len); } }; \u6267\u884c\u7528\u65f6\uff1a100 ms \u5185\u5b58\u6d88\u8017\uff1a7.6 MB \u901a\u8fc7\u6d4b\u8bd5\u7528\u4f8b\uff1a 266 / 266 \u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6\uff1a\u6700\u574f\u60c5\u51b5\u4e0b\u5de6\u53f3\u6307\u9488\u5bf9$s$\u7684\u6bcf\u4e2a\u5143\u7d20\u5404\u904d\u5386\u4e00\u904d\uff0c\u54c8\u5e0c\u8868\u4e2d\u5bf9$s$\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u5404\u63d2\u5165\u3001\u5220\u9664\u4e00\u6b21\uff0c\u5bf9$t$\u4e2d\u7684\u5143\u7d20\u5404\u63d2\u5165\u4e00\u6b21\u3002\u6bcf\u6b21\u68c0\u67e5\u662f\u5426\u53ef\u884c\u4f1a\u904d\u5386\u6574\u4e2a$t$\u7684\u54c8\u5e0c\u8868\uff0c\u54c8\u5e0c\u8868\u7684\u5927\u5c0f\u4e0e\u5b57\u7b26\u96c6\u7684\u5927\u5c0f\u6709\u5173\uff0c\u8bbe\u5b57\u7b26\u96c6\u5927\u5c0f\u4e3a$C$\uff0c\u5219\u6e10\u8fdb\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$O(C \\cdot |s| + |t|)$ \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a\u8fd9\u91cc\u7528\u4e86\u4e24\u5f20\u54c8\u5e0c\u8868\u4f5c\u4e3a\u8f85\u52a9\u7a7a\u95f4\uff0c\u6bcf\u5f20\u54c8\u5e0c\u8868\u6700\u591a\u4e0d\u4f1a\u5b58\u653e\u8d85\u8fc7\u5b57\u7b26\u96c6\u5927\u5c0f\u7684\u952e\u503c\u5bf9\uff0c\u6211\u4eec\u8bbe\u5b57\u7b26\u96c6\u5927\u5c0f\u4e3a$C$\uff0c\u5219\u6e10\u8fdb\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a$O(C)$.","title":"76.\u6700\u5c0f\u8986\u76d6\u5b50\u4e32"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/76.%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/#1","text":"\u7ed9\u4f60\u4e00\u4e2a\u5b57\u7b26\u4e32 $s$\u3001\u4e00\u4e2a\u5b57\u7b26\u4e32 $t$\u3002\u8fd4\u56de $s$ \u4e2d\u6db5\u76d6 $t$ \u6240\u6709\u5b57\u7b26\u7684\u6700\u5c0f\u5b50\u4e32\u3002\u5982\u679c $s$ \u4e2d\u4e0d\u5b58\u5728\u6db5\u76d6 $t$ \u6240\u6709\u5b57\u7b26\u7684\u5b50\u4e32\uff0c\u5219\u8fd4\u56de\u7a7a\u5b57\u7b26\u4e32 \"\"\u3002 \u6ce8\u610f\uff1a \u5bf9\u4e8e $t$ \u4e2d\u91cd\u590d\u5b57\u7b26\uff0c\u6211\u4eec\u5bfb\u627e\u7684\u5b50\u5b57\u7b26\u4e32\u4e2d\u8be5\u5b57\u7b26\u6570\u91cf\u5fc5\u987b\u4e0d\u5c11\u4e8e $t$ \u4e2d\u8be5\u5b57\u7b26\u7684\u6570\u91cf\u3002 \u5982\u679c $s$ \u4e2d\u5b58\u5728\u8fd9\u6837\u7684\u5b50\u4e32\uff0c\u6211\u4eec\u4fdd\u8bc1\u5b83\u662f\u552f\u4e00\u7684\u7b54\u6848\u3002","title":"1\u3001\u9898\u76ee"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/76.%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/#2","text":"\u793a\u4f8b1\uff1a \u8f93\u5165\uff1as = \"ADOBECODEBANC\", t = \"ABC\" \u8f93\u51fa\uff1a\"BANC\" \u793a\u4f8b2\uff1a \u8f93\u5165\uff1as = \"a\", t = \"a\" \u8f93\u51fa\uff1a\"a\" \u793a\u4f8b3\uff1a \u8f93\u5165\uff1as = \"a\", t = \"aa\" \u8f93\u51fa\uff1a\"\" \u89e3\u91ca\uff1at\u4e2d\u4e24\u4e2a\u5b57\u7b26 'a' \u5747\u5e94\u5305\u542b\u5728 s \u7684\u5b50\u4e32\u4e2d\uff0c\u56e0\u6b64\u6ca1\u6709\u7b26\u5408\u6761\u4ef6\u7684\u5b50\u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7a7a\u5b57\u7b26\u4e32\u3002","title":"2\u3001\u793a\u4f8b"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/76.%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/#3","text":"\u672c\u95ee\u9898\u8981\u6c42\u6211\u4eec\u8fd4\u56de\u5b57\u7b26\u4e32 $s$ \u4e2d\u5305\u542b\u5b57\u7b26\u4e32 $t$ \u7684\u5168\u90e8\u5b57\u7b26\u7684\u6700\u5c0f\u7a97\u53e3\u3002\u6211\u4eec\u79f0\u5305\u542b $t$ \u7684\u5168\u90e8\u5b57\u6bcd\u7684\u7a97\u53e3\u4e3a\u300c\u53ef\u884c\u300d\u7a97\u53e3\u3002 \u6211\u4eec\u53ef\u4ee5\u7528\u6ed1\u52a8\u7a97\u53e3\u7684\u601d\u60f3\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u6ed1\u52a8\u7a97\u53e3\u7c7b\u578b\u7684\u95ee\u9898\u4e2d\u90fd\u4f1a\u6709\u4e24\u4e2a\u6307\u9488\uff0c\u4e00\u4e2a\u7528\u4e8e\u300c\u5ef6\u4f38\u300d\u73b0\u6709\u7a97\u53e3 $r$ \u6307\u9488\uff0c\u548c\u4e00\u4e2a\u7528\u4e8e\u300c\u6536\u7f29\u300d\u7a97\u53e3\u7684 $l$ \u6307\u9488\u3002\u5728\u4efb\u610f\u65f6\u523b\uff0c\u53ea\u6709\u4e00\u4e2a\u6307\u9488\u8fd0\u52a8\uff0c\u800c\u53e6\u4e00\u4e2a\u6307\u9488\u4fdd\u6301\u9759\u6b62\u3002\u6211\u4eec\u5728 $s$ \u4e0a\u6ed1\u52a8\u7a97\u53e3\uff0c\u901a\u8fc7\u79fb\u52a8 $r$ \u6307\u9488\u4e0d\u65ad\u6269\u5f20\u7a97\u53e3\u3002\u5f53\u7a97\u53e3\u5305\u542b $t$ \u5168\u90e8\u6240\u9700\u7684\u5b57\u7b26\u540e\uff0c\u5982\u679c\u80fd\u6536\u7f29\uff0c\u6211\u4eec\u5c31\u6536\u7f29\u7a97\u53e3\u76f4\u5230\u5f97\u5230\u6700\u5c0f\u7a97\u53e3\u3002 \u5982\u4f55\u5224\u65ad\u5f53\u524d\u7684\u7a97\u53e3\u5305\u542b\u6240\u6709 $t$ \u6240\u9700\u7684\u5b57\u7b26\u5462\uff1f\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u54c8\u5e0c\u8868\u8868\u793a $t$ \u4e2d\u6240\u6709\u7684\u5b57\u7b26\u4ee5\u53ca\u5b83\u4eec\u7684\u4e2a\u6570\uff0c\u7528\u4e00\u4e2a\u54c8\u5e0c\u8868\u52a8\u6001\u7ef4\u62a4\u7a97\u53e3\u4e2d\u6240\u6709\u7684\u5b57\u7b26\u4ee5\u53ca\u5b83\u4eec\u7684\u4e2a\u6570\uff0c\u5982\u679c\u8fd9\u4e2a\u52a8\u6001\u8868\u4e2d\u5305\u542b $t$ \u7684\u54c8\u5e0c\u8868\u4e2d\u7684\u6240\u6709\u5b57\u7b26\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u4e2a\u6570\u90fd\u4e0d\u5c0f\u4e8e $t$ \u7684\u54c8\u5e0c\u8868\u4e2d\u5404\u4e2a\u5b57\u7b26\u7684\u4e2a\u6570\uff0c\u90a3\u4e48\u5f53\u524d\u7a97\u53e3\u662f\u300c\u53ef\u884c\u300d\u7684\u3002 *\u6ce8\u610f\uff1a\u8fd9\u91cc$t$\u4e2d\u51fa\u73b0\u91cd\u590d\u7684\u5b57\u7b26\uff0c\u6240\u4ee5\u6211\u4eec\u8981\u8bb0\u5f55\u5b57\u7b26\u7684\u4e2a\u6570\u3002 \u8003\u8651\u5982\u4f55\u4f18\u5316\uff1f\u5982\u679c$s=XX...XABCXXXX,t=ABC$\uff0c\u90a3\u4e48\u663e\u7136$[XX..XXXABC]$\u662f\u7b2c\u4e00\u4e2a\u5f97\u5230\u7684\u300c\u53ef\u884c\u300d\u53d6\u4ef6\uff0c\u5f97\u5230\u8fd9\u4e2a\u53ef\u884c\u533a\u95f4\u540e\uff0c\u6211\u4eec\u6309\u7167\u300c\u6536\u7f29\u300d\u7a97\u53e3\u7684\u539f\u5219\u66f4\u65b0\u5de6\u8fb9\u754c\uff0c\u5f97\u5230\u6700\u5c0f\u533a\u95f4\u3002\u6211\u4eec\u5176\u5b9e\u505a\u4e86\u4e00\u4e9b\u65e0\u7528\u64cd\u4f5c\uff0c\u5c31\u662f\u66f4\u65b0\u53f3\u8fb9\u754c\u7684\u65f6\u5019\u300c\u5ef6\u4f38\u300d\u8fdb\u4e86\u5f88\u591a\u65e0\u7528\u7684$X$\uff0c\u66f4\u65b0\u5de6\u8fb9\u754c\u7684\u65f6\u5019\u300c\u6536\u7f29\u300d\u6254\u6389\u4e86\u8fd9\u4e9b\u65e0\u7528\u7684$X$\uff0c\u505a\u4e86\u8fd9\u4e48\u591a\u65e0\u7528\u7684\u64cd\u4f5c\uff0c\u53ea\u662f\u4e3a\u4e86\u5f97\u5230\u77ed\u77ed\u7684$ABC$\u3002\u6ca1\u9519\uff0c\u5176\u5b9e\u5728$s$\u4e2d\uff0c\u6709\u7684\u5b57\u7b26\u6211\u4eec\u662f\u4e0d\u5173\u5fc3\u7684\uff0c\u6211\u4eec\u53ea\u5173\u5fc3$t$\u51fa\u73b0\u7684\u5b57\u7b26\uff0c\u6211\u4eec\u53ef\u4e0d\u53ef\u4ee5\u5148\u9884\u5904\u7406$s$\uff0c\u6254\u6389\u90a3\u4e9b$t$\u4e2d\u6ca1\u6709\u51fa\u73b0\u7684\u5b57\u7b26\uff0c\u7136\u540e\u518d\u505a\u6ed1\u52a8\u7a97\u53e3\u5462\uff1f\u4e5f\u8bb8\u4f60\u4f1a\u8bf4\uff0c\u8fd9\u6837\u53ef\u80fd\u51fa\u73b0$XXABXXC$\u7684\u60c5\u51b5\uff0c\u5728\u7edf\u8ba1\u957f\u5ea6\u7684\u65f6\u5019\u53ef\u4ee5\u6254\u6389\u524d\u4e24\u4e2a$X$\uff0c\u4f46\u662f\u4ecd\u4e0d\u6389\u4e2d\u95f4\u7684$X$\uff0c\u600e\u6837\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5462\uff1f\u4f18\u5316\u540e\u7684\u65f6\u7a7a\u590d\u6742\u5ea6\u53c8\u662f\u591a\u5c11\u5462\uff1f\u8fd9\u91cc\u4ee3\u7801\u7ed9\u51fa\u6ca1\u6709\u4f18\u5316\u7684\u7248\u672c\uff0c\u4ee5\u4e0a\u7684\u4e09\u4e2a\u95ee\u9898\u7559\u7ed9\u8bfb\u7740\u601d\u8003\u3002","title":"3\u3001\u601d\u8def"},{"location":"%E7%AE%97%E6%B3%95%E9%A2%98/76.%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/#4","text":"class Solution: def minWindow(self, s: 'str', t: 'str') -> 'str': from collections import defaultdict lookup = defaultdict(int) for c in t: lookup[c] += 1 start = 0 end = 0 min_len = float(\"inf\") counter = len(t) res = \"\" while end < len(s): if lookup[s[end]] > 0: counter -= 1 lookup[s[end]] -= 1 end += 1 while counter == 0: if min_len > end - start: min_len = end - start res = s[start: end] if lookup[s[start]] == 0: counter += 1 lookup[s[start]] += 1 start += 1 return res \u6267\u884c\u7528\u65f6\uff1a64 ms \u5185\u5b58\u6d88\u8017\uff1a15.5 MB class Solution { public: unordered_map <char, int> ori, cnt; bool check() { for (const auto &p: ori) { if (cnt[p.first] < p.second) { return false; } } return true; } string minWindow(string s, string t) { for (const auto &c: t) { ++ori[c]; } int l = 0, r = -1; int len = INT_MAX, ansL = -1, ansR = -1; while (r < int(s.size())) { if (ori.find(s[++r]) != ori.end()) { ++cnt[s[r]]; } while (check() && l <= r) { if (r - l + 1 < len) { len = r - l + 1; ansL = l; } if (ori.find(s[l]) != ori.end()) { --cnt[s[l]]; } ++l; } } return ansL == -1 ? string() : s.substr(ansL, len); } }; \u6267\u884c\u7528\u65f6\uff1a100 ms \u5185\u5b58\u6d88\u8017\uff1a7.6 MB \u901a\u8fc7\u6d4b\u8bd5\u7528\u4f8b\uff1a 266 / 266 \u590d\u6742\u5ea6\u5206\u6790 \u65f6\u95f4\u590d\u6742\u5ea6\uff1a\u6700\u574f\u60c5\u51b5\u4e0b\u5de6\u53f3\u6307\u9488\u5bf9$s$\u7684\u6bcf\u4e2a\u5143\u7d20\u5404\u904d\u5386\u4e00\u904d\uff0c\u54c8\u5e0c\u8868\u4e2d\u5bf9$s$\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u5404\u63d2\u5165\u3001\u5220\u9664\u4e00\u6b21\uff0c\u5bf9$t$\u4e2d\u7684\u5143\u7d20\u5404\u63d2\u5165\u4e00\u6b21\u3002\u6bcf\u6b21\u68c0\u67e5\u662f\u5426\u53ef\u884c\u4f1a\u904d\u5386\u6574\u4e2a$t$\u7684\u54c8\u5e0c\u8868\uff0c\u54c8\u5e0c\u8868\u7684\u5927\u5c0f\u4e0e\u5b57\u7b26\u96c6\u7684\u5927\u5c0f\u6709\u5173\uff0c\u8bbe\u5b57\u7b26\u96c6\u5927\u5c0f\u4e3a$C$\uff0c\u5219\u6e10\u8fdb\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$O(C \\cdot |s| + |t|)$ \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a\u8fd9\u91cc\u7528\u4e86\u4e24\u5f20\u54c8\u5e0c\u8868\u4f5c\u4e3a\u8f85\u52a9\u7a7a\u95f4\uff0c\u6bcf\u5f20\u54c8\u5e0c\u8868\u6700\u591a\u4e0d\u4f1a\u5b58\u653e\u8d85\u8fc7\u5b57\u7b26\u96c6\u5927\u5c0f\u7684\u952e\u503c\u5bf9\uff0c\u6211\u4eec\u8bbe\u5b57\u7b26\u96c6\u5927\u5c0f\u4e3a$C$\uff0c\u5219\u6e10\u8fdb\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a$O(C)$.","title":"4\u3001\u4ee3\u7801"}]}