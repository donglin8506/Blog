# 背景

整理一些需要学习的资料，以及待学习的资料或者有疑问的内容

概念关键词：人脸活体、OCR、芯算一体、Mixture-of-Experts (MoE)

待学习：

1. 自监督表征预训练CAE（Context Autoencoder）方法, 在隐含表征空间里，对掩码区域做预测；可以学习到语义表征，可用于图像块的检索

大模型训练的挑战和应对

共性：

模型大（例如GPT-3）、数据量大（例如十几亿图文对）、计算量大（64卡训练一个月）

差异：

- 网络结构有差别
- 模型稀疏性、稠密性有差别
- IO特性

Wide&Deep模型（万亿、稀疏）、Transformer类型预训练模型（千亿、稠密）、Mixture-of-Experts类多专家模型（万亿、十万亿、稠密）


难： ERNIE3.0 6.2E11 TFLOs / V100 32G 125TFLOPs

解决办法：


**多计算节点扩展（主动）**

- 保证收敛性（并行策略考虑计算等价性、正确性）
- 提升加速比（降低通信、同步等消耗）

**可训练：解决模型提及带来的挑战**
- 模型相关信息可被单计算设备存储下；
- 模型切分后可被正确、高效训练


数据并行策略，参数、梯度更新：参数服务器（中性化）、all Reduce（去中性化）等

飞桨自适应
